{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ® Phase 4: Atari í™˜ê²½ ì‹¤ìŠµ\n\n> **ëª©í‘œ**: Atari ê²Œì„ í™˜ê²½ì—ì„œ Decision Transformerê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì´í•´í•˜ê³ ,  \n> ì´ë¯¸ì§€ ê¸°ë°˜ ê°•í™”í•™ìŠµì˜ íŠ¹ìˆ˜ì„±ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n\n---\n\n## ğŸ—ºï¸ ì´ Phaseì—ì„œ ë°°ìš¸ ë‚´ìš©\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      Phase 4: Atari í™˜ê²½ ì‹¤ìŠµ ê°œìš”                          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                            â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\nâ”‚   â”‚   DQN        â”‚    â”‚   CNN        â”‚    â”‚   Atari      â”‚                â”‚\nâ”‚   â”‚  Replay      â”‚ â”€â”€â–¶â”‚  Encoder     â”‚ â”€â”€â–¶â”‚   GPT        â”‚                â”‚\nâ”‚   â”‚  ë°ì´í„°ì…‹    â”‚    â”‚  (ì´ë¯¸ì§€â†’ë²¡í„°)â”‚    â”‚  ëª¨ë¸        â”‚                â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\nâ”‚         â”‚                   â”‚                    â”‚                        â”‚\nâ”‚         â–¼                   â–¼                    â–¼                        â”‚\nâ”‚   â€¢ 84Ã—84 í”½ì…€ ì´ë¯¸ì§€      â€¢ Conv ë ˆì´ì–´       â€¢ Reward-Conditioned      â”‚\nâ”‚   â€¢ 4 í”„ë ˆì„ ìŠ¤íƒ          â€¢ íŠ¹ì§• ì¶”ì¶œ         â€¢ Naive (BC)               â”‚\nâ”‚   â€¢ 50ê°œ ë²„í¼              â€¢ 3136â†’128 ì••ì¶•    â€¢ Cross-Entropy Loss       â”‚\nâ”‚                                                                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## ğŸ“‹ ëª©ì°¨\n\n| ì„¹ì…˜ | ì£¼ì œ | í•™ìŠµ ë‚´ìš© |\n|:---:|:---|:---|\n| **1** | [í™˜ê²½ ì„¤ì •](#1-í™˜ê²½-ì„¤ì •) | ê²½ë¡œ ì„¤ì • ë° ë°ì´í„° í™•ì¸ |\n| **2** | [ë°ì´í„°ì…‹ ìƒì„± ë¶„ì„](#2-ë°ì´í„°ì…‹-ìƒì„±-ë¶„ì„) | DQN Replay Bufferì™€ RTG ê³„ì‚° |\n| **3** | [CNN Encoder ì´í•´](#3-cnn-encoder-ì´í•´) | ì´ë¯¸ì§€ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì • |\n| **4** | [ëª¨ë¸ êµ¬ì¡° ë¶„ì„](#4-ëª¨ë¸-êµ¬ì¡°-ë¶„ì„) | Atariìš© GPT ëª¨ë¸ êµ¬ì¡° |\n| **5** | [Reward-Conditioned vs Naive](#5-reward-conditioned-vs-naive-ë¹„êµ) | ë‘ ëª¨ë“œì˜ ì°¨ì´ì  |\n\n---\n\n## ğŸ”— Phase ì—°ê²°\n\n```\nPhase 1 (ë°°ê²½ì§€ì‹)     Phase 2 (í•µì‹¬ê°œë…)     Phase 3 (Gym)      Phase 4 (Atari)\n      â”‚                     â”‚                    â”‚                   â”‚\n      â–¼                     â–¼                    â–¼                   â–¼\n  RL/Transformer       RTG/ì‹œí€€ìŠ¤           ì—°ì† ìƒíƒœ â”€â”€â–¶     ì´ë¯¸ì§€ ìƒíƒœ\n    ê¸°ì´ˆ ê°œë…          êµ¬ì„± ì›ë¦¬            (11ì°¨ì› ë²¡í„°)    (84Ã—84Ã—4 í”½ì…€)\n```\n\n## ğŸ¯ Gym vs Atari ë¹„êµ\n\n| í•­ëª© | Gym (Phase 3) | Atari (Phase 4) |\n|:---|:---|:---|\n| **State** | 11ì°¨ì› ë²¡í„° | 84Ã—84Ã—4 ì´ë¯¸ì§€ |\n| **Action** | ì—°ì†ê°’ (3ì°¨ì›) | ì´ì‚°ê°’ (4~18ê°œ) |\n| **ì†ì‹¤ í•¨ìˆ˜** | MSE | Cross-Entropy |\n| **Encoder** | Linear | CNN |\n| **ë°ì´í„°ì…‹** | D4RL | DQN Replay |\n\n---\n\n## ğŸ“¦ ì‚¬ì „ ìš”êµ¬ì‚¬í•­\n\n```python\n# í•„ìš”í•œ íŒ¨í‚¤ì§€\n- Python 3.7+\n- PyTorch 1.8+\n- NumPy\n- Matplotlib\n\n# ì„ íƒì  (ì‹¤ì œ ë°ì´í„° ì‚¬ìš© ì‹œ)\n- gsutil (Google Cloud Storage ì ‘ê·¼ìš©)\n- Atari ROMs\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“¦ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì •\n# ============================================================\n# Atari Decision Transformer ë¶„ì„ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ì…ë‹ˆë‹¤.\n#\n# ì£¼ìš” íŒ¨í‚¤ì§€:\n#   - torch: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ (CNN, Transformer)\n#   - numpy: ì´ë¯¸ì§€ ë°ì´í„° ì²˜ë¦¬\n#   - matplotlib: ì‹œê°í™”\n# ============================================================\n\nimport os\nimport sys\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nprint(\"ğŸ”§ í™˜ê²½ ì •ë³´\")\nprint(\"=\"*50)\nprint(f\"   Python: {sys.version.split()[0]}\")\nprint(f\"   PyTorch: {torch.__version__}\")\nprint(f\"   NumPy: {np.__version__}\")\nprint(f\"   CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"   CUDA device: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# 1. í™˜ê²½ ì„¤ì •\n\n## ğŸ¯ ëª©í‘œ\n> Atari Decision Transformer ì½”ë“œì— ì ‘ê·¼í•˜ê¸° ìœ„í•œ ê²½ë¡œë¥¼ ì„¤ì •í•˜ê³ , ë°ì´í„° êµ¬ì¡°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n\n## ğŸ“‚ Atari í”„ë¡œì íŠ¸ êµ¬ì¡°\n\n```\ndecision-transformer/\nâ”œâ”€â”€ atari/                          â† ì´ë²ˆ Phaseì—ì„œ ë¶„ì„í•  ì½”ë“œ\nâ”‚   â”œâ”€â”€ mingpt/\nâ”‚   â”‚   â”œâ”€â”€ model_atari.py          â† í•µì‹¬: GPT ëª¨ë¸ + CNN Encoder\nâ”‚   â”‚   â”œâ”€â”€ trainer_atari.py        â† í•™ìŠµ ë£¨í”„\nâ”‚   â”‚   â””â”€â”€ utils.py\nâ”‚   â”œâ”€â”€ run_dt_atari.py             â† ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸\nâ”‚   â”œâ”€â”€ create_dataset.py           â† DQN replay â†’ í•™ìŠµ ë°ì´í„° ë³€í™˜\nâ”‚   â”œâ”€â”€ fixed_replay_buffer.py      â† Replay buffer ë¡œë”©\nâ”‚   â””â”€â”€ dqn_replay/                 â† ë°ì´í„°ì…‹ (ë‹¤ìš´ë¡œë“œ í•„ìš”)\nâ”‚       â”œâ”€â”€ Breakout/\nâ”‚       â”œâ”€â”€ Pong/\nâ”‚       â””â”€â”€ ...\nâ””â”€â”€ doc/notebooks/                  â† í˜„ì¬ ìœ„ì¹˜\n```\n\n## ğŸ® ì§€ì›ë˜ëŠ” Atari ê²Œì„\n\n| ê²Œì„ | Action ìˆ˜ | íŠ¹ì§• |\n|:---|:---:|:---|\n| Breakout | 4 | ë²½ëŒê¹¨ê¸°, ê°„ë‹¨í•œ ì•¡ì…˜ |\n| Pong | 6 | íƒêµ¬, 2ì¸ ëŒ€ì „ |\n| Qbert | 6 | í”Œë«í¼ ì í”„ |\n| Seaquest | 18 | ë³µì¡í•œ ì•¡ì…˜ ê³µê°„ |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“‚ í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •\n# ============================================================\n# Pythonì´ atari/ í´ë”ì˜ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ê²½ë¡œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n# ============================================================\n\nPROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\nATARI_PATH = os.path.join(PROJECT_ROOT, 'atari')\n\nif ATARI_PATH not in sys.path:\n    sys.path.insert(0, ATARI_PATH)\n\nprint(\"ğŸ“‚ ê²½ë¡œ ì„¤ì • ê²°ê³¼\")\nprint(\"=\"*50)\nprint(f\"   Project root: {PROJECT_ROOT}\")\nprint(f\"   Atari path:   {ATARI_PATH}\")\nprint()\n\n# ê²½ë¡œ ê²€ì¦\nif os.path.exists(ATARI_PATH):\n    print(\"âœ… Atari ê²½ë¡œ ì¡´ì¬ í™•ì¸!\")\n    mingpt_path = os.path.join(ATARI_PATH, 'mingpt')\n    if os.path.exists(mingpt_path):\n        print(\"âœ… mingpt ëª¨ë“ˆ ì¡´ì¬ í™•ì¸!\")\n        # ì£¼ìš” íŒŒì¼ í™•ì¸\n        key_files = ['model_atari.py', 'trainer_atari.py']\n        for f in key_files:\n            fpath = os.path.join(mingpt_path, f)\n            status = \"âœ…\" if os.path.exists(fpath) else \"âŒ\"\n            print(f\"   {status} {f}\")\nelse:\n    print(\"âŒ Atari ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ® ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ í™•ì¸\n# ============================================================\n# DQN replay ë°ì´í„°ì…‹ì´ ì €ì¥ë˜ëŠ” ìœ„ì¹˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n#\n# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë°©ë²•:\n#   gsutil -m cp -R gs://atari-replay-datasets/dqn/Breakout dqn_replay/\n#\n# ê° ê²Œì„ í´ë”ì—ëŠ” 50ê°œì˜ checkpointê°€ ìˆìŠµë‹ˆë‹¤ (1/, 2/, ..., 50/)\n# ============================================================\n\ndata_dir = os.path.join(ATARI_PATH, 'dqn_replay')\n\nprint(\"ğŸ“‚ ë°ì´í„°ì…‹ í™•ì¸\")\nprint(\"=\"*50)\n\nif os.path.exists(data_dir):\n    games = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n    \n    if games:\n        print(f\"\\nâœ… ì‚¬ìš© ê°€ëŠ¥í•œ ê²Œì„ ({len(games)}ê°œ):\\n\")\n        for game in sorted(games):\n            game_path = os.path.join(data_dir, game)\n            checkpoints = [d for d in os.listdir(game_path) if d.isdigit()]\n            print(f\"   ğŸ® {game}\")\n            print(f\"      â””â”€ {len(checkpoints)}ê°œ checkpoint\")\n    else:\n        print(\"\\nâš ï¸ ê²Œì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\nelse:\n    print(f\"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {data_dir}\")\n\nprint(\"\\n\" + \"-\"*50)\nprint(\"ğŸ“¥ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë°©ë²•:\")\nprint(\"   mkdir -p dqn_replay\")\nprint(\"   gsutil -m cp -R gs://atari-replay-datasets/dqn/Breakout dqn_replay/\")\nprint(\"\\nğŸ’¡ gsutilì´ ì—†ë‹¤ë©´ Google Cloud SDKë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# 2. ë°ì´í„°ì…‹ ìƒì„± ë¶„ì„\n\n## ğŸ¯ ëª©í‘œ\n> DQN Replay Bufferì˜ êµ¬ì¡°ì™€ Atari ìŠ¤íƒ€ì¼ RTG ê³„ì‚° ë°©ì‹ì„ ì´í•´í•©ë‹ˆë‹¤.\n\n## ğŸ“š DQN Replay Bufferë€?\n\n**DQN (Deep Q-Network)**ì´ í•™ìŠµí•˜ë©´ì„œ ì €ì¥í•œ ê²½í—˜ ë°ì´í„°ì…ë‹ˆë‹¤.\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                       DQN Replay Buffer êµ¬ì¡°                               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                            â”‚\nâ”‚  DQN í•™ìŠµ ê³¼ì •:                                                            â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚  â”‚                                                                      â”‚ â”‚\nâ”‚  â”‚  checkpoint 1/  â”€â”€â–¶ 100,000 transitions (ì´ˆë³´ ì •ì±…)                 â”‚ â”‚\nâ”‚  â”‚  checkpoint 2/  â”€â”€â–¶ 100,000 transitions                             â”‚ â”‚\nâ”‚  â”‚  checkpoint 3/  â”€â”€â–¶ 100,000 transitions                             â”‚ â”‚\nâ”‚  â”‚       ...                                                            â”‚ â”‚\nâ”‚  â”‚  checkpoint 50/ â”€â”€â–¶ 100,000 transitions (ì „ë¬¸ê°€ ì •ì±…)               â”‚ â”‚\nâ”‚  â”‚                                                                      â”‚ â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ”‚                                                                            â”‚\nâ”‚  ê° transition:                                                            â”‚\nâ”‚    (observation, action, reward, terminal)                                â”‚\nâ”‚         â”‚           â”‚       â”‚        â”‚                                    â”‚\nâ”‚         â”‚           â”‚       â”‚        â””â”€ ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€               â”‚\nâ”‚         â”‚           â”‚       â””â”€ ì¦‰ì‹œ ë³´ìƒ                                 â”‚\nâ”‚         â”‚           â””â”€ ìˆ˜í–‰í•œ í–‰ë™ (0~17)                                â”‚\nâ”‚         â””â”€ ê²Œì„ í™”ë©´ (4 stacked frames, 84Ã—84)                          â”‚\nâ”‚                                                                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## ğŸ’¡ ì™œ 50ê°œ ë²„í¼ë¥¼ ì‚¬ìš©í• ê¹Œ?\n\n| ë²„í¼ ë²ˆí˜¸ | ì •ì±… í’ˆì§ˆ | íŠ¹ì§• |\n|:---:|:---|:---|\n| 1~10 | ë‚®ìŒ (ì´ˆë³´) | ëœë¤ì— ê°€ê¹Œìš´ í–‰ë™ |\n| 11~30 | ì¤‘ê°„ | í•™ìŠµ ì¤‘ì¸ ì •ì±… |\n| 31~50 | ë†’ìŒ (ì „ë¬¸ê°€) | ìµœì í™”ëœ ì •ì±… |\n\n> ğŸ’¡ **ë‹¤ì–‘í•œ í’ˆì§ˆì˜ ë°ì´í„°**ë¥¼ ì‚¬ìš©í•˜ë©´ DTê°€ ë” ì˜ í•™ìŠµí•©ë‹ˆë‹¤!  \n> RTG ì¡°ê±´ì— ë”°ë¼ ì´ˆë³´~ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ í–‰ë™ì„ ëª¨ë‘ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.1 DQN Replay Buffer êµ¬ì¡° ìƒì„¸\n\n### ğŸ“¦ Observationì˜ êµ¬ì¡°\n\nAtari ê²Œì„ì˜ observationì€ **4ê°œ í”„ë ˆì„ì„ ìŒ“ì€(stack)** ì´ë¯¸ì§€ì…ë‹ˆë‹¤.\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    4 Frame Stackì˜ ì˜ë¯¸                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                     â”‚\nâ”‚   ì™œ 4ê°œ í”„ë ˆì„ì„ ìŒ“ì„ê¹Œ?                                            â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚\nâ”‚   ë‹¨ì¼ í”„ë ˆì„ë§Œìœ¼ë¡œëŠ” ë¬¼ì²´ì˜ \"ì›€ì§ì„ ë°©í–¥\"ì„ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.          â”‚\nâ”‚                                                                     â”‚\nâ”‚   ì˜ˆ: Pong ê²Œì„ì—ì„œ ê³µì˜ ìœ„ì¹˜ë§Œìœ¼ë¡œëŠ” ê³µì´ ì–´ë””ë¡œ ê°€ëŠ”ì§€ ëª¨ë¦„        â”‚\nâ”‚                                                                     â”‚\nâ”‚   Frame 1     Frame 2     Frame 3     Frame 4                       â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”                       â”‚\nâ”‚   â”‚  â—‹  â”‚     â”‚   â—‹ â”‚     â”‚    â—‹â”‚     â”‚     â”‚â—‹                      â”‚\nâ”‚   â”‚     â”‚ â”€â”€â–¶ â”‚     â”‚ â”€â”€â–¶ â”‚     â”‚ â”€â”€â–¶ â”‚     â”‚                       â”‚\nâ”‚   â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚                       â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜                       â”‚\nâ”‚                                                                     â”‚\nâ”‚   4ê°œë¥¼ ìŒ“ìœ¼ë©´: \"ê³µì´ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™ ì¤‘\" ì„ ì•Œ ìˆ˜ ìˆìŒ!            â”‚\nâ”‚                                                                     â”‚\nâ”‚   ìµœì¢… Shape: (4, 84, 84)                                           â”‚\nâ”‚               â””â”€ 4 ì±„ë„ (í”„ë ˆì„ ìˆ˜)                                 â”‚\nâ”‚                    â””â”€ 84Ã—84 ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€                     â”‚\nâ”‚                                                                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### ğŸ“Š ë°ì´í„° í¬ê¸°\n\n| í•­ëª© | ê°’ | ì„¤ëª… |\n|:---|:---|:---|\n| ë‹¨ì¼ observation | (4, 84, 84) | 28,224 í”½ì…€ |\n| ë°ì´í„° íƒ€ì… | uint8 | 0~255 ì •ìˆ˜ |\n| ë©”ëª¨ë¦¬ (1ê°œ) | ~27KB | ì••ì¶• ì—†ì´ |\n| 1 ë²„í¼ (100K) | ~2.7GB | 100,000ê°œ |\n| 50 ë²„í¼ ì „ì²´ | ~135GB | ëŒ€ìš©ëŸ‰! |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ® ë”ë¯¸ ê¶¤ì  ë°ì´í„° ìƒì„±\n# ============================================================\n# ì‹¤ì œ DQN replayê°€ ì—†ì–´ë„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n# êµ¬ì¡°ëŠ” ì‹¤ì œ ë°ì´í„°ì™€ ë™ì¼í•©ë‹ˆë‹¤.\n# ============================================================\n\ndef create_dummy_trajectory(length=100):\n    \"\"\"\n    Atari ìŠ¤íƒ€ì¼ì˜ ë”ë¯¸ ê¶¤ì  ìƒì„±\n    \n    Returns:\n        dict with keys:\n        - observations: (length, 4, 84, 84) - 4 stacked frames\n        - actions: (length,) - discrete actions\n        - rewards: (length,) - sparse rewards (0 or positive)\n        - terminals: (length,) - episode end flags\n    \"\"\"\n    return {\n        # 84x84 ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€ 4ê°œ ìŠ¤íƒ\n        'observations': np.random.randint(0, 256, (length, 4, 84, 84), dtype=np.uint8),\n        \n        # ì´ì‚° í–‰ë™ (Breakout: 0~3)\n        'actions': np.random.randint(0, 4, length),\n        \n        # Atari rewardëŠ” í¬ì†Œ(sparse): ëŒ€ë¶€ë¶„ 0, ê°€ë” ì–‘ìˆ˜\n        'rewards': np.random.choice([0, 0, 0, 1, 5, 10], length).astype(np.float32),\n        \n        # ì—í”¼ì†Œë“œ ì¢…ë£Œ í”Œë˜ê·¸\n        'terminals': np.zeros(length, dtype=bool)\n    }\n\n# ì—¬ëŸ¬ ê¶¤ì  ìƒì„± (ì—¬ëŸ¬ ì—í”¼ì†Œë“œ)\nprint(\"ğŸ® ë”ë¯¸ ê¶¤ì  ë°ì´í„° ìƒì„±\")\nprint(\"=\"*60)\n\ntrajectories = []\nfor i in range(10):\n    # ê° ì—í”¼ì†Œë“œëŠ” 50~200 ìŠ¤í…\n    traj = create_dummy_trajectory(np.random.randint(50, 200))\n    traj['terminals'][-1] = True  # ë§ˆì§€ë§‰ ìŠ¤í…ì€ terminal\n    trajectories.append(traj)\n\nprint(f\"ìƒì„±ëœ ì—í”¼ì†Œë“œ ìˆ˜: {len(trajectories)}\")\n\n# ì²« ë²ˆì§¸ ê¶¤ì  ë¶„ì„\ntraj = trajectories[0]\nprint(f\"\\nğŸ“Š ì²« ë²ˆì§¸ ì—í”¼ì†Œë“œ ë¶„ì„:\")\nprint(f\"   â”Œâ”€ observations: {traj['observations'].shape}\")\nprint(f\"   â”‚                â””â”€ (ì—í”¼ì†Œë“œ ê¸¸ì´, í”„ë ˆì„ ìˆ˜, ë†’ì´, ë„ˆë¹„)\")\nprint(f\"   â”œâ”€ actions:      {traj['actions'].shape}\")\nprint(f\"   â”‚                â””â”€ ì´ì‚° action (0~3)\")\nprint(f\"   â”œâ”€ rewards:      {traj['rewards'].shape}\")\nprint(f\"   â”‚                â””â”€ í¬ì†Œ reward\")\nprint(f\"   â””â”€ terminals:    {traj['terminals'].shape}\")\n\n# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\nmemory_mb = traj['observations'].nbytes / (1024 * 1024)\nprint(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_mb:.1f} MB (í•œ ì—í”¼ì†Œë“œ)\")\n\n# Reward ë¶„í¬\nunique, counts = np.unique(traj['rewards'], return_counts=True)\nprint(f\"\\nğŸ“ˆ Reward ë¶„í¬:\")\nfor u, c in zip(unique, counts):\n    pct = c / len(traj['rewards']) * 100\n    print(f\"   reward={u:>2}: {c:>3}íšŒ ({pct:>5.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.2 RTG ê³„ì‚° (Atari ìŠ¤íƒ€ì¼)\n\n> Atariì—ì„œ RTG ê³„ì‚° ì‹œ ì¤‘ìš”í•œ ì : **ì—í”¼ì†Œë“œ ê²½ê³„ë¥¼ ë„˜ì§€ ì•ŠëŠ”ë‹¤!**\n\n### âš ï¸ ì—í”¼ì†Œë“œ ê²½ê³„ ì²˜ë¦¬\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    ì—í”¼ì†Œë“œ ê²½ê³„ì™€ RTG                               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                     â”‚\nâ”‚  ì—í”¼ì†Œë“œ 1              ì—í”¼ì†Œë“œ 2                                  â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”‚\nâ”‚  [r0, r1, r2, r3]        [r4, r5, r6]                               â”‚\nâ”‚  [ 1,  2,  3,  4]        [ 5, 10, 15]                               â”‚\nâ”‚                                                                     â”‚\nâ”‚  âŒ ì˜ëª»ëœ ê³„ì‚° (ê²½ê³„ ë¬´ì‹œ):                                         â”‚\nâ”‚     RTG[3] = r3 + r4 + r5 + r6 = 4 + 5 + 10 + 15 = 34 (í‹€ë¦¼!)      â”‚\nâ”‚                                                                     â”‚\nâ”‚  âœ… ì˜¬ë°”ë¥¸ ê³„ì‚° (ê²½ê³„ ì¡´ì¤‘):                                         â”‚\nâ”‚     RTG[3] = r3 = 4 (ì—í”¼ì†Œë“œ 1ì˜ ë§ˆì§€ë§‰)                           â”‚\nâ”‚     RTG[4] = r4 + r5 + r6 = 30 (ì—í”¼ì†Œë“œ 2ì˜ ì²˜ìŒ)                  â”‚\nâ”‚                                                                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### ğŸ’¡ ì™œ ê²½ê³„ë¥¼ ì¡´ì¤‘í•´ì•¼ í• ê¹Œ?\n\n- **ì—í”¼ì†Œë“œ 1ì—ì„œ ì£½ìœ¼ë©´** â†’ ì—í”¼ì†Œë“œ 2ì˜ ë³´ìƒì„ ë°›ì„ ìˆ˜ ì—†ìŒ\n- **RTGëŠ” \"ì•ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆëŠ” ë³´ìƒ\"** ì„ ì˜ë¯¸\n- ì£½ì€ í›„ì—ëŠ” ì•„ë¬´ê²ƒë„ ë°›ì„ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ **ì—í”¼ì†Œë“œ ë‚´ì—ì„œë§Œ** ê³„ì‚°"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“ˆ Atari ìŠ¤íƒ€ì¼ RTG ê³„ì‚° í•¨ìˆ˜\n# ============================================================\n# create_dataset.pyì—ì„œ ì‚¬ìš©í•˜ëŠ” RTG ê³„ì‚° ë°©ì‹ì…ë‹ˆë‹¤.\n# í•µì‹¬: ì—í”¼ì†Œë“œ ê²½ê³„(done_idxs)ë¥¼ ì¡´ì¤‘í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.\n# ============================================================\n\ndef calculate_rtg_atari_style(stepwise_returns, done_idxs):\n    \"\"\"\n    Atari ìŠ¤íƒ€ì¼ RTG ê³„ì‚°\n    \n    Parameters:\n    -----------\n    stepwise_returns : np.ndarray\n        ëª¨ë“  ìŠ¤í…ì˜ reward (ì—¬ëŸ¬ ì—í”¼ì†Œë“œê°€ ì—°ê²°ëœ í˜•íƒœ)\n    done_idxs : list\n        ê° ì—í”¼ì†Œë“œê°€ ëë‚˜ëŠ” ì¸ë±ìŠ¤ (exclusive)\n        ì˜ˆ: [4, 7] â†’ ì—í”¼ì†Œë“œ1ì€ ì¸ë±ìŠ¤ 0~3, ì—í”¼ì†Œë“œ2ëŠ” 4~6\n        \n    Returns:\n    --------\n    np.ndarray\n        ì—í”¼ì†Œë“œ ê²½ê³„ë¥¼ ì¡´ì¤‘í•˜ëŠ” RTG\n    \"\"\"\n    rtg = np.zeros_like(stepwise_returns)\n    start_index = 0\n    \n    for done_idx in done_idxs:\n        done_idx = int(done_idx)\n        \n        # í˜„ì¬ ì—í”¼ì†Œë“œì˜ rewardë§Œ ì¶”ì¶œ\n        curr_traj_returns = stepwise_returns[start_index:done_idx]\n        \n        # ì—­ë°©í–¥ìœ¼ë¡œ RTG ê³„ì‚° (ëì—ì„œ ì‹œì‘ìœ¼ë¡œ)\n        for j in range(done_idx - 1, start_index - 1, -1):\n            # jë¶€í„° ì—í”¼ì†Œë“œ ëê¹Œì§€ì˜ reward í•©\n            rtg_j = curr_traj_returns[j - start_index:done_idx - start_index]\n            rtg[j] = sum(rtg_j)\n        \n        # ë‹¤ìŒ ì—í”¼ì†Œë“œë¡œ ì´ë™\n        start_index = done_idx\n    \n    return rtg\n\n# ============================================================\n# ğŸ§ª RTG ê³„ì‚° í…ŒìŠ¤íŠ¸\n# ============================================================\nprint(\"ğŸ“ˆ Atari ìŠ¤íƒ€ì¼ RTG ê³„ì‚° ì˜ˆì‹œ\")\nprint(\"=\"*60)\n\n# ë‘ ì—í”¼ì†Œë“œ ìƒì„±\nrewards_ep1 = [1, 2, 3, 4]    # ì—í”¼ì†Œë“œ 1: ì´ reward = 10\nrewards_ep2 = [5, 10, 15]     # ì—í”¼ì†Œë“œ 2: ì´ reward = 30\n\n# ì—°ê²°\nall_rewards = np.array(rewards_ep1 + rewards_ep2, dtype=np.float32)\n\n# ì—í”¼ì†Œë“œ ê²½ê³„ ì¸ë±ìŠ¤ (exclusive)\ndone_idxs = [len(rewards_ep1), len(rewards_ep1) + len(rewards_ep2)]  # [4, 7]\n\n# RTG ê³„ì‚°\nrtg = calculate_rtg_atari_style(all_rewards, done_idxs)\n\nprint(f\"\\nì…ë ¥ ë°ì´í„°:\")\nprint(f\"   rewards:    {all_rewards}\")\nprint(f\"   done_idxs:  {done_idxs}\")\n\nprint(f\"\\nê³„ì‚°ëœ RTG:\")\nprint(f\"   rtg:        {rtg}\")\n\nprint(f\"\\nğŸ“Š ì—í”¼ì†Œë“œë³„ ë¶„ì„:\")\nprint(f\"   â”Œâ”€ ì—í”¼ì†Œë“œ 1 (idx 0~3):\")\nprint(f\"   â”‚    rewards: {rewards_ep1}\")\nprint(f\"   â”‚    RTG:     {rtg[:4].tolist()}\")\nprint(f\"   â”‚    â””â”€ idx 0ì˜ RTG = 1+2+3+4 = {rtg[0]}\")\nprint(f\"   â”‚\")\nprint(f\"   â””â”€ ì—í”¼ì†Œë“œ 2 (idx 4~6):\")\nprint(f\"        rewards: {rewards_ep2}\")\nprint(f\"        RTG:     {rtg[4:].tolist()}\")\nprint(f\"        â””â”€ idx 4ì˜ RTG = 5+10+15 = {rtg[4]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“Š RTG ê³„ì‚° ì‹œê°í™”\n# ============================================================\n# ì—í”¼ì†Œë“œ ê²½ê³„ì—ì„œ RTGê°€ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ì‹œê°í™”í•©ë‹ˆë‹¤.\n# ============================================================\n\nprint(\"ğŸ“Š RTG ì‹œê°í™”\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# ìƒ‰ìƒ ì§€ì •: ì—í”¼ì†Œë“œë³„ë¡œ ë‹¤ë¥¸ ìƒ‰\ncolors = ['steelblue'] * 4 + ['coral'] * 3\n\n# 1. Reward ì‹œê°í™”\nax1 = axes[0]\nbars1 = ax1.bar(range(len(all_rewards)), all_rewards, color=colors, alpha=0.7, edgecolor='black')\nax1.axvline(x=3.5, color='black', linestyle='--', linewidth=2, label='Episode boundary')\nax1.set_xlabel('Step Index', fontsize=12)\nax1.set_ylabel('Reward', fontsize=12)\nax1.set_title('Step-wise Reward (ì—í”¼ì†Œë“œë³„ ìƒ‰ìƒ êµ¬ë¶„)', fontsize=14)\nax1.set_xticks(range(len(all_rewards)))\nax1.legend()\nax1.grid(True, alpha=0.3, axis='y')\n\n# ê°’ í‘œì‹œ\nfor i, (r, bar) in enumerate(zip(all_rewards, bars1)):\n    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, \n             f'{int(r)}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n\n# 2. RTG ì‹œê°í™”\nax2 = axes[1]\nbars2 = ax2.bar(range(len(rtg)), rtg, color=colors, alpha=0.7, edgecolor='black')\nax2.axvline(x=3.5, color='black', linestyle='--', linewidth=2, label='Episode boundary')\nax2.set_xlabel('Step Index', fontsize=12)\nax2.set_ylabel('Return-to-Go', fontsize=12)\nax2.set_title('RTG (ì—í”¼ì†Œë“œ ë‚´ì—ì„œë§Œ ê³„ì‚°)', fontsize=14)\nax2.set_xticks(range(len(rtg)))\nax2.legend()\nax2.grid(True, alpha=0.3, axis='y')\n\n# ê°’ í‘œì‹œ\nfor i, (r, bar) in enumerate(zip(rtg, bars2)):\n    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n             f'{int(r)}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# í•µì‹¬ í¬ì¸íŠ¸\nprint(\"\\nğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:\")\nprint(\"   â€¢ ì—í”¼ì†Œë“œ 1ì˜ RTGëŠ” ì—í”¼ì†Œë“œ 1 ë‚´ì—ì„œë§Œ ê³„ì‚°ë¨\")\nprint(\"   â€¢ ì—í”¼ì†Œë“œ 2ì˜ RTGëŠ” ì—í”¼ì†Œë“œ 2 ë‚´ì—ì„œë§Œ ê³„ì‚°ë¨\")\nprint(\"   â€¢ ê²½ê³„(ì¸ë±ìŠ¤ 3â†’4)ì—ì„œ RTGê°€ ê¸‰ê²©íˆ ë³€í™”\")\nprint(\"   â€¢ ì´ëŠ” 'ì£½ìœ¼ë©´ ë‹¤ìŒ ì—í”¼ì†Œë“œ ë³´ìƒì„ ë°›ì„ ìˆ˜ ì—†ë‹¤'ëŠ” ì˜ë¯¸ ë°˜ì˜\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.3 StateActionReturnDataset\n\n> PyTorch Datasetìœ¼ë¡œ ë°°ì¹˜ í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ë¡œë”ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n\n### ğŸ“¦ Dataset êµ¬ì¡°\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              StateActionReturnDataset ë™ì‘ ë°©ì‹                      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                     â”‚\nâ”‚  ì „ì²´ ë°ì´í„°: [ep1 data][ep2 data][ep3 data]...                     â”‚\nâ”‚              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”‚\nâ”‚                                                                     â”‚\nâ”‚  __getitem__(idx):                                                  â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚\nâ”‚  â”‚ 1. idx ìœ„ì¹˜ í™•ì¸                                               â”‚â”‚\nâ”‚  â”‚ 2. ì—í”¼ì†Œë“œ ê²½ê³„ ì²´í¬ (done_idx ì°¾ê¸°)                          â”‚â”‚\nâ”‚  â”‚ 3. context_lengthë§Œí¼ ì¶”ì¶œ (ê²½ê³„ ë„˜ì§€ ì•Šê²Œ)                    â”‚â”‚\nâ”‚  â”‚ 4. ì´ë¯¸ì§€ ì •ê·œí™” (0~255 â†’ 0~1)                                â”‚â”‚\nâ”‚  â”‚ 5. (states, actions, rtgs, timesteps) ë°˜í™˜                     â”‚â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚\nâ”‚                                                                     â”‚\nâ”‚  ë°˜í™˜ í˜•íƒœ:                                                         â”‚\nâ”‚    states:    (context_len, 4*84*84) - í‰íƒ„í™”ëœ ì´ë¯¸ì§€             â”‚\nâ”‚    actions:   (context_len, 1) - ì´ì‚° ì•¡ì…˜                         â”‚\nâ”‚    rtgs:      (context_len, 1) - RTG                               â”‚\nâ”‚    timesteps: (1, 1) - ì‹œì‘ ì‹œê°„                                   â”‚\nâ”‚                                                                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“¦ StateActionReturnDataset êµ¬í˜„\n# ============================================================\n# run_dt_atari.pyì—ì„œ ì‚¬ìš©í•˜ëŠ” Dataset í´ë˜ìŠ¤ì…ë‹ˆë‹¤.\n# ì—í”¼ì†Œë“œ ê²½ê³„ë¥¼ ì¡´ì¤‘í•˜ë©´ì„œ context_length í¬ê¸°ì˜ ì‹œí€€ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n# ============================================================\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass StateActionReturnDataset(Dataset):\n    \"\"\"\n    Atariìš© Dataset (run_dt_atari.py ì°¸ì¡°)\n    \n    Parameters:\n    -----------\n    data : list\n        observation ë¦¬ìŠ¤íŠ¸ (ê° ìš”ì†ŒëŠ” (4, 84, 84) ì´ë¯¸ì§€)\n    block_size : int\n        context_length * 3 (RTG, State, Action ì„¸ ì¢…ë¥˜)\n    actions : np.ndarray\n        action ë°°ì—´\n    done_idxs : list\n        ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¸ë±ìŠ¤\n    rtgs : np.ndarray\n        RTG ë°°ì—´\n    timesteps : np.ndarray\n        ê° ìŠ¤í…ì˜ ì—í”¼ì†Œë“œ ë‚´ ì‹œê°„\n    \"\"\"\n    \n    def __init__(self, data, block_size, actions, done_idxs, rtgs, timesteps):\n        self.block_size = block_size  # context_length * 3\n        self.vocab_size = int(max(actions)) + 1  # action ìˆ˜\n        self.data = data\n        self.actions = actions\n        self.done_idxs = done_idxs\n        self.rtgs = rtgs\n        self.timesteps = timesteps\n        \n    def __len__(self):\n        # context_lengthë¥¼ í™•ë³´í•  ìˆ˜ ìˆëŠ” ìƒ˜í”Œ ìˆ˜\n        return len(self.data) - self.block_size // 3\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        idx ìœ„ì¹˜ì—ì„œ context_length í¬ê¸°ì˜ ì‹œí€€ìŠ¤ ë°˜í™˜\n        \"\"\"\n        block_size = self.block_size // 3  # ì‹¤ì œ íƒ€ì„ìŠ¤í… ìˆ˜\n        done_idx = idx + block_size\n        \n        # ì—í”¼ì†Œë“œ ê²½ê³„ í™•ì¸: í˜„ì¬ idxë³´ë‹¤ í° ì²« ë²ˆì§¸ done_idx ì°¾ê¸°\n        for i in self.done_idxs:\n            if i > idx:\n                # ì—í”¼ì†Œë“œ ëì„ ë„˜ì§€ ì•Šë„ë¡ ì¡°ì •\n                done_idx = min(int(i), done_idx)\n                break\n        \n        # ì‹œì‘ ì¸ë±ìŠ¤ ì¡°ì • (block_size í™•ë³´)\n        idx = done_idx - block_size\n        \n        # ===== States =====\n        # ì´ë¯¸ì§€ë¥¼ í‰íƒ„í™”í•˜ê³  ì •ê·œí™” (0~255 â†’ 0~1)\n        states = torch.tensor(\n            np.array(self.data[idx:done_idx]), \n            dtype=torch.float32\n        ).reshape(block_size, -1) / 255.0  # ì •ê·œí™”!\n        \n        # ===== Actions =====\n        actions = torch.tensor(\n            self.actions[idx:done_idx], \n            dtype=torch.long\n        ).unsqueeze(1)\n        \n        # ===== RTGs =====\n        rtgs = torch.tensor(\n            self.rtgs[idx:done_idx], \n            dtype=torch.float32\n        ).unsqueeze(1)\n        \n        # ===== Timesteps =====\n        # ì‹œì‘ timestepë§Œ ë°˜í™˜ (ë‚˜ë¨¸ì§€ëŠ” ëª¨ë¸ì—ì„œ ê³„ì‚°)\n        timesteps = torch.tensor(\n            self.timesteps[idx:idx+1], \n            dtype=torch.int64\n        ).unsqueeze(1)\n        \n        return states, actions, rtgs, timesteps\n\n# ============================================================\n# ğŸ§ª Dataset í…ŒìŠ¤íŠ¸\n# ============================================================\nprint(\"ğŸ§ª StateActionReturnDataset í…ŒìŠ¤íŠ¸\")\nprint(\"=\"*60)\n\n# ë”ë¯¸ ë°ì´í„° ì¤€ë¹„\nnum_samples = 500\ncontext_length = 30\n\n# ì´ë¯¸ì§€ ë°ì´í„° (4 stacked frames)\ndummy_data = [np.random.randint(0, 256, (4, 84, 84), dtype=np.uint8) for _ in range(num_samples)]\ndummy_actions = np.random.randint(0, 4, num_samples)\ndummy_rewards = np.random.choice([0, 1, 5], num_samples).astype(np.float32)\n\n# ì—í”¼ì†Œë“œ ê²½ê³„ (4ê°œ ì—í”¼ì†Œë“œ)\ndummy_done_idxs = [100, 200, 350, 500]\n\n# RTG ê³„ì‚°\ndummy_rtgs = calculate_rtg_atari_style(dummy_rewards, dummy_done_idxs)\n\n# Timesteps (ì—í”¼ì†Œë“œ ë‚´ ì‹œê°„)\ndummy_timesteps = np.zeros(num_samples, dtype=np.int64)\nstart = 0\nfor done in dummy_done_idxs:\n    dummy_timesteps[start:done] = np.arange(done - start)\n    start = done\n\n# Dataset ìƒì„±\ndataset = StateActionReturnDataset(\n    dummy_data, \n    context_length * 3,  # block_size\n    dummy_actions, \n    dummy_done_idxs, \n    dummy_rtgs, \n    dummy_timesteps\n)\n\nprint(f\"\\nğŸ“Š Dataset ì •ë³´:\")\nprint(f\"   ì´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}\")\nprint(f\"   Vocab size (action ìˆ˜): {dataset.vocab_size}\")\nprint(f\"   Block size: {dataset.block_size}\")\nprint(f\"   Context length: {context_length}\")\n\n# ìƒ˜í”Œ í•˜ë‚˜ ê°€ì ¸ì˜¤ê¸°\nstates, actions, rtgs, timesteps = dataset[0]\n\nprint(f\"\\nğŸ“¦ ìƒ˜í”Œ Shapes:\")\nprint(f\"   states:    {states.shape}\")\nprint(f\"              â””â”€ ({context_length}, 4Ã—84Ã—84) = ({context_length}, 28224)\")\nprint(f\"   actions:   {actions.shape}\")\nprint(f\"              â””â”€ ({context_length}, 1)\")\nprint(f\"   rtgs:      {rtgs.shape}\")\nprint(f\"              â””â”€ ({context_length}, 1)\")\nprint(f\"   timesteps: {timesteps.shape}\")\nprint(f\"              â””â”€ (1, 1)\")\n\nprint(f\"\\nğŸ“ˆ ë°ì´í„° ë²”ìœ„:\")\nprint(f\"   states: [{states.min():.3f}, {states.max():.3f}] (ì •ê·œí™”ë¨)\")\nprint(f\"   actions: {actions.unique().tolist()} (ì´ì‚°ê°’)\")\nprint(f\"   rtgs: [{rtgs.min():.1f}, {rtgs.max():.1f}]\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# 3. CNN Encoder ì´í•´\n\n## ğŸ¯ ëª©í‘œ\n> Atari ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” CNN Encoderì˜ êµ¬ì¡°ì™€ ë™ì‘ì„ ì´í•´í•©ë‹ˆë‹¤.\n\n## ğŸ–¼ï¸ ì™œ CNNì´ í•„ìš”í•œê°€?\n\n**Gym í™˜ê²½ (Phase 3):**\n- State: 11ì°¨ì› ë²¡í„°\n- ì§ì ‘ Linear ë ˆì´ì–´ë¡œ ì„ë² ë”© ê°€ëŠ¥\n\n**Atari í™˜ê²½:**\n- State: 84Ã—84Ã—4 = **28,224**ì°¨ì› ì´ë¯¸ì§€\n- ì´ë¯¸ì§€ì˜ ê³µê°„ì  êµ¬ì¡°ë¥¼ í™œìš©í•´ì•¼ í•¨\n- **CNNìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ** í›„ ì„ë² ë”©\n\n## ğŸ—ï¸ CNN Encoder êµ¬ì¡°\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        CNN Encoder ì•„í‚¤í…ì²˜                                â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                            â”‚\nâ”‚  ì…ë ¥: (batch, 4, 84, 84) - 4 stacked grayscale frames                    â”‚\nâ”‚                                                                            â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚ Conv1: Conv2d(4â†’32, kernel=8, stride=4)                             â”‚  â”‚\nâ”‚  â”‚        (batch, 4, 84, 84) â†’ (batch, 32, 20, 20)                     â”‚  â”‚\nâ”‚  â”‚        + ReLU                                                       â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                              â†“                                             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚ Conv2: Conv2d(32â†’64, kernel=4, stride=2)                            â”‚  â”‚\nâ”‚  â”‚        (batch, 32, 20, 20) â†’ (batch, 64, 9, 9)                      â”‚  â”‚\nâ”‚  â”‚        + ReLU                                                       â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                              â†“                                             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚ Conv3: Conv2d(64â†’64, kernel=3, stride=1)                            â”‚  â”‚\nâ”‚  â”‚        (batch, 64, 9, 9) â†’ (batch, 64, 7, 7)                        â”‚  â”‚\nâ”‚  â”‚        + ReLU                                                       â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                              â†“                                             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚ Flatten: (batch, 64, 7, 7) â†’ (batch, 3136)                          â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                              â†“                                             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚ Linear: Linear(3136 â†’ n_embd) + Tanh                                â”‚  â”‚\nâ”‚  â”‚         (batch, 3136) â†’ (batch, 128)                                â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                            â”‚\nâ”‚  ì¶œë ¥: (batch, 128) - State ì„ë² ë”© ë²¡í„°                                    â”‚\nâ”‚                                                                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n> ğŸ’¡ **ì´ êµ¬ì¡°ëŠ” DQN ë…¼ë¬¸ì˜ CNNê³¼ ë™ì¼í•©ë‹ˆë‹¤!** (Atari ê°•í™”í•™ìŠµì˜ í‘œì¤€)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ—ï¸ CNN Encoder êµ¬í˜„\n# ============================================================\n# model_atari.pyì˜ CNN Encoderë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n# DQN ë…¼ë¬¸ê³¼ ë™ì¼í•œ êµ¬ì¡°ì…ë‹ˆë‹¤.\n# ============================================================\n\nclass AtariCNNEncoder(nn.Module):\n    \"\"\"\n    DQN ìŠ¤íƒ€ì¼ CNN Encoder\n    \n    Parameters:\n    -----------\n    n_embd : int\n        ì¶œë ¥ ì„ë² ë”© ì°¨ì› (default: 128)\n        \n    ì…ë ¥: (batch, 4, 84, 84) - 4 stacked grayscale frames\n    ì¶œë ¥: (batch, n_embd) - State ì„ë² ë”© ë²¡í„°\n    \"\"\"\n    \n    def __init__(self, n_embd=128):\n        super().__init__()\n        \n        self.encoder = nn.Sequential(\n            # ===== Conv1 =====\n            # ì…ë ¥: (4, 84, 84) â†’ ì¶œë ¥: (32, 20, 20)\n            # ê³„ì‚°: (84 - 8) / 4 + 1 = 20\n            nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n            nn.ReLU(),\n            \n            # ===== Conv2 =====\n            # ì…ë ¥: (32, 20, 20) â†’ ì¶œë ¥: (64, 9, 9)\n            # ê³„ì‚°: (20 - 4) / 2 + 1 = 9\n            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n            nn.ReLU(),\n            \n            # ===== Conv3 =====\n            # ì…ë ¥: (64, 9, 9) â†’ ì¶œë ¥: (64, 7, 7)\n            # ê³„ì‚°: (9 - 3) / 1 + 1 = 7\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            \n            # ===== Flatten =====\n            # ì…ë ¥: (64, 7, 7) â†’ ì¶œë ¥: (3136)\n            # ê³„ì‚°: 64 Ã— 7 Ã— 7 = 3136\n            nn.Flatten(),\n            \n            # ===== Linear =====\n            # ì…ë ¥: (3136) â†’ ì¶œë ¥: (n_embd)\n            nn.Linear(64 * 7 * 7, n_embd),\n            nn.Tanh()  # ì¶œë ¥ì„ [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”\n        )\n        \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: (batch, 4, 84, 84) - 4 stacked grayscale frames\n            \n        Returns:\n            (batch, n_embd) - State ì„ë² ë”© ë²¡í„°\n        \"\"\"\n        return self.encoder(x)\n\n# ============================================================\n# ğŸ§ª CNN Encoder í…ŒìŠ¤íŠ¸\n# ============================================================\nprint(\"ğŸ—ï¸ CNN Encoder í…ŒìŠ¤íŠ¸\")\nprint(\"=\"*60)\n\nencoder = AtariCNNEncoder(n_embd=128)\n\nprint(\"\\nğŸ“¦ ëª¨ë¸ êµ¬ì¡°:\")\nprint(encoder)\n\n# íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°\ntotal_params = sum(p.numel() for p in encoder.parameters())\nprint(f\"\\nğŸ“Š ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}\")\n\n# Forward pass í…ŒìŠ¤íŠ¸\nx = torch.randn(4, 4, 84, 84)  # batch=4\nout = encoder(x)\n\nprint(f\"\\nğŸ”„ Forward Pass:\")\nprint(f\"   ì…ë ¥: {x.shape}\")\nprint(f\"   ì¶œë ¥: {out.shape}\")\nprint(f\"\\n   ì°¨ì› ì¶•ì†Œ ë¹„ìœ¨: {4*84*84} â†’ {out.shape[-1]}\")\nprint(f\"   ({4*84*84 / out.shape[-1]:.0f}ë°° ì••ì¶•)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“ ê° ë ˆì´ì–´ì˜ ì¶œë ¥ í¬ê¸° ì¶”ì \n# ============================================================\n# CNNì—ì„œ ì¶œë ¥ í¬ê¸° ê³„ì‚° ê³µì‹:\n#   output_size = (input_size - kernel_size + 2*padding) / stride + 1\n#\n# ì´ ê³µì‹ì„ ì‚¬ìš©í•´ ê° ë ˆì´ì–´ì˜ ì¶œë ¥ì„ ì¶”ì í•©ë‹ˆë‹¤.\n# ============================================================\n\ndef calc_conv_output(input_size, kernel_size, stride, padding=0):\n    \"\"\"Conv2d ì¶œë ¥ í¬ê¸° ê³„ì‚°\"\"\"\n    return (input_size - kernel_size + 2 * padding) // stride + 1\n\nprint(\"ğŸ“ CNN ì¶œë ¥ í¬ê¸° ë‹¨ê³„ë³„ ì¶”ì \")\nprint(\"=\"*60)\n\n# ë‹¨ê³„ë³„ ì¶œë ¥ ê¸°ë¡\nstages = []\n\n# ì…ë ¥\nsize = 84\nchannels = 4\nstages.append(('Input', channels, size, size))\n\n# Conv1\nchannels = 32\nsize = calc_conv_output(size, kernel_size=8, stride=4)\nstages.append(('Conv1 (k=8, s=4)', channels, size, size))\n\n# Conv2\nchannels = 64\nsize = calc_conv_output(size, kernel_size=4, stride=2)\nstages.append(('Conv2 (k=4, s=2)', channels, size, size))\n\n# Conv3\nchannels = 64\nsize = calc_conv_output(size, kernel_size=3, stride=1)\nstages.append(('Conv3 (k=3, s=1)', channels, size, size))\n\n# í‘œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥\nprint(f\"\\n{'Stage':<20} {'Shape':<20} {'Elements':>12}\")\nprint(\"-\"*55)\n\nfor stage_name, c, h, w in stages:\n    shape = f\"({c}, {h}, {w})\"\n    elements = c * h * w\n    print(f\"{stage_name:<20} {shape:<20} {elements:>12,}\")\n\n# Flatten\nflatten_size = stages[-1][1] * stages[-1][2] * stages[-1][3]\nprint(f\"{'Flatten':<20} {'(' + str(flatten_size) + ')':<20} {flatten_size:>12,}\")\n\n# Linear\nfinal_size = 128\nprint(f\"{'Linear':<20} {'(' + str(final_size) + ')':<20} {final_size:>12,}\")\n\nprint(\"\\n\" + \"-\"*55)\nprint(f\"ì´ ì••ì¶•ë¥ : {4*84*84:,} â†’ {final_size} ({4*84*84/final_size:.0f}ë°°)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ” ì‹¤ì œ í…ì„œë¡œ ì¶”ì  í™•ì¸\n# ============================================================\n# ì‹¤ì œë¡œ í…ì„œë¥¼ í†µê³¼ì‹œí‚¤ë©° ê° ë ˆì´ì–´ì˜ ì¶œë ¥ì„ í™•ì¸í•©ë‹ˆë‹¤.\n# ============================================================\n\ndef trace_cnn_with_tensor(x):\n    \"\"\"\n    ì‹¤ì œ í…ì„œë¡œ CNN ê° ë‹¨ê³„ë¥¼ ì¶”ì \n    \"\"\"\n    print(f\"ğŸ” ì‹¤ì œ í…ì„œë¡œ CNN ì¶”ì \")\n    print(\"=\"*60)\n    print(f\"ì…ë ¥: {x.shape}\")\n    print(\"-\"*60)\n    \n    # Conv1\n    conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n    x = F.relu(conv1(x))\n    print(f\"Conv1 (k=8, s=4) + ReLU: {x.shape}\")\n    print(f\"  â””â”€ ê³„ì‚°: (84-8)/4 + 1 = 20\")\n    \n    # Conv2\n    conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n    x = F.relu(conv2(x))\n    print(f\"Conv2 (k=4, s=2) + ReLU: {x.shape}\")\n    print(f\"  â””â”€ ê³„ì‚°: (20-4)/2 + 1 = 9\")\n    \n    # Conv3\n    conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n    x = F.relu(conv3(x))\n    print(f\"Conv3 (k=3, s=1) + ReLU: {x.shape}\")\n    print(f\"  â””â”€ ê³„ì‚°: (9-3)/1 + 1 = 7\")\n    \n    # Flatten\n    x = x.flatten(1)  # batch ì°¨ì› ìœ ì§€\n    print(f\"Flatten: {x.shape}\")\n    print(f\"  â””â”€ 64 Ã— 7 Ã— 7 = 3136\")\n    \n    # Linear\n    linear = nn.Linear(3136, 128)\n    x = torch.tanh(linear(x))\n    print(f\"Linear + Tanh: {x.shape}\")\n    \n    print(\"-\"*60)\n    print(f\"ìµœì¢… ì¶œë ¥: {x.shape}\")\n    return x\n\n# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\ntest_input = torch.randn(1, 4, 84, 84)  # batch=1\noutput = trace_cnn_with_tensor(test_input)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# 4. ëª¨ë¸ êµ¬ì¡° ë¶„ì„\n\n## ğŸ¯ ëª©í‘œ\n> Atariìš© Decision Transformer (AtariGPT)ì˜ ì „ì²´ êµ¬ì¡°ë¥¼ ì´í•´í•©ë‹ˆë‹¤.\n\n## ğŸ—ï¸ AtariGPT ì „ì²´ ì•„í‚¤í…ì²˜\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         AtariGPT ì „ì²´ êµ¬ì¡°                                  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                            â”‚\nâ”‚  ì…ë ¥: states (B, K, 4*84*84), actions (B, K, 1), rtgs (B, K, 1)          â”‚\nâ”‚                                                                            â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚                        Embedding Layer                              â”‚  â”‚\nâ”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚  â”‚\nâ”‚  â”‚  â”‚ CNN        â”‚   â”‚ RTG        â”‚   â”‚ Action     â”‚                  â”‚  â”‚\nâ”‚  â”‚  â”‚ Encoder    â”‚   â”‚ Embedding  â”‚   â”‚ Embedding  â”‚                  â”‚  â”‚\nâ”‚  â”‚  â”‚ (state)    â”‚   â”‚ (Linear)   â”‚   â”‚ (Lookup)   â”‚                  â”‚  â”‚\nâ”‚  â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                  â”‚  â”‚\nâ”‚  â”‚        â”‚                â”‚                â”‚                          â”‚  â”‚\nâ”‚  â”‚        â–¼                â–¼                â–¼                          â”‚  â”‚\nâ”‚  â”‚     (B,K,128)        (B,K,128)        (B,K,128)                     â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                â”‚                                          â”‚\nâ”‚                                â–¼                                          â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚  Sequence Construction (Reward-Conditioned)                         â”‚  â”‚\nâ”‚  â”‚  [Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚, aâ‚, ..., Râ‚–, sâ‚–, aâ‚–]                         â”‚  â”‚\nâ”‚  â”‚                                                                     â”‚  â”‚\nâ”‚  â”‚  Shape: (B, K*3, 128) = (B, 90, 128) when K=30                     â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                â”‚                                          â”‚\nâ”‚                                â–¼                                          â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚  Transformer Blocks (Ã— N layers)                                    â”‚  â”‚\nâ”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚\nâ”‚  â”‚  â”‚ LayerNorm â†’ CausalSelfAttention â†’ + â†’ LayerNorm â†’ MLP â†’ +   â”‚   â”‚  â”‚\nâ”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                â”‚                                          â”‚\nâ”‚                                â–¼                                          â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚  Prediction Head                                                    â”‚  â”‚\nâ”‚  â”‚  Linear(128 â†’ vocab_size) at state positions (1::3)                â”‚  â”‚\nâ”‚  â”‚  â†’ action logits (ë¶„ë¥˜ ë¬¸ì œ!)                                       â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                            â”‚\nâ”‚  ì†ì‹¤: Cross-Entropy (action ë¶„ë¥˜)                                         â”‚\nâ”‚                                                                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## ğŸ’¡ Gym vs Atari ì†ì‹¤ í•¨ìˆ˜ ë¹„êµ\n\n| í™˜ê²½ | Action íƒ€ì… | ì†ì‹¤ í•¨ìˆ˜ | ì¶œë ¥ |\n|:---|:---|:---|:---|\n| **Gym** | ì—°ì†ê°’ | MSE | (B, K, action_dim) |\n| **Atari** | ì´ì‚°ê°’ | Cross-Entropy | (B, K, vocab_size) logits |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ—ï¸ Transformer êµ¬ì„± ìš”ì†Œ êµ¬í˜„\n# ============================================================\n# model_atari.pyì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n# - CausalSelfAttention: ì¸ê³¼ì  self-attention\n# - Block: Transformer ë¸”ë¡\n# ============================================================\n\nimport math\n\nclass CausalSelfAttention(nn.Module):\n    \"\"\"\n    Causal Self-Attention (ë¯¸ë˜ ì •ë³´ë¥¼ ë³´ì§€ ì•ŠëŠ” attention)\n    \n    GPT ìŠ¤íƒ€ì¼ì˜ unidirectional attentionì…ë‹ˆë‹¤.\n    í˜„ì¬ í† í°ì€ ì´ì „ í† í°ë“¤ë§Œ ì°¸ì¡°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n    \"\"\"\n    \n    def __init__(self, n_embd, n_head, block_size, attn_pdrop=0.1, resid_pdrop=0.1):\n        super().__init__()\n        assert n_embd % n_head == 0, \"n_embd must be divisible by n_head\"\n        \n        # Q, K, V projection\n        self.key = nn.Linear(n_embd, n_embd)\n        self.query = nn.Linear(n_embd, n_embd)\n        self.value = nn.Linear(n_embd, n_embd)\n        \n        # Dropout layers\n        self.attn_drop = nn.Dropout(attn_pdrop)\n        self.resid_drop = nn.Dropout(resid_pdrop)\n        \n        # Output projection\n        self.proj = nn.Linear(n_embd, n_embd)\n        \n        # Causal mask: í•˜ì‚¼ê° í–‰ë ¬ (ë¯¸ë˜ ì •ë³´ ì°¨ë‹¨)\n        # mask[i,j] = 1 if j <= i else 0\n        self.register_buffer(\n            \"mask\",\n            torch.tril(torch.ones(block_size + 1, block_size + 1))\n                 .view(1, 1, block_size + 1, block_size + 1)\n        )\n        \n        self.n_head = n_head\n        \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: (B, T, C) - ì…ë ¥ ì‹œí€€ìŠ¤\n            \n        Returns:\n            (B, T, C) - attention ì ìš© í›„ ì¶œë ¥\n        \"\"\"\n        B, T, C = x.size()\n        \n        # Q, K, V ê³„ì‚° í›„ multi-headë¡œ reshape\n        # (B, T, C) â†’ (B, T, n_head, head_dim) â†’ (B, n_head, T, head_dim)\n        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        \n        # Attention ê³„ì‚°\n        # (B, n_head, T, head_dim) @ (B, n_head, head_dim, T) â†’ (B, n_head, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        \n        # Causal masking: ë¯¸ë˜ ìœ„ì¹˜ì— -inf ì ìš©\n        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n        \n        # Softmax\n        att = F.softmax(att, dim=-1)\n        att = self.attn_drop(att)\n        \n        # Valueì™€ ê³±í•˜ê¸°\n        # (B, n_head, T, T) @ (B, n_head, T, head_dim) â†’ (B, n_head, T, head_dim)\n        y = att @ v\n        \n        # Reshape back\n        # (B, n_head, T, head_dim) â†’ (B, T, C)\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        \n        # Output projection\n        y = self.resid_drop(self.proj(y))\n        \n        return y\n\n\nclass Block(nn.Module):\n    \"\"\"\n    Transformer Block\n    \n    êµ¬ì¡°: LayerNorm â†’ Attention â†’ + â†’ LayerNorm â†’ MLP â†’ +\n    (Pre-LN êµ¬ì¡°)\n    \"\"\"\n    \n    def __init__(self, n_embd, n_head, block_size, resid_pdrop=0.1):\n        super().__init__()\n        \n        # LayerNorm layers\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n        \n        # Self-Attention\n        self.attn = CausalSelfAttention(n_embd, n_head, block_size)\n        \n        # MLP (Feed-Forward Network)\n        # hidden dim = 4 * n_embd (GPT-2 í‘œì¤€)\n        self.mlp = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.GELU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(resid_pdrop),\n        )\n        \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: (B, T, C)\n            \n        Returns:\n            (B, T, C)\n        \"\"\"\n        # Attention with residual connection\n        x = x + self.attn(self.ln1(x))\n        \n        # MLP with residual connection\n        x = x + self.mlp(self.ln2(x))\n        \n        return x\n\n# ============================================================\n# ğŸ§ª Block í…ŒìŠ¤íŠ¸\n# ============================================================\nprint(\"ğŸ—ï¸ Transformer Block í…ŒìŠ¤íŠ¸\")\nprint(\"=\"*60)\n\nblock = Block(n_embd=128, n_head=8, block_size=90)\nx = torch.randn(4, 90, 128)  # (batch, seq_len, hidden)\n\nout = block(x)\n\nprint(f\"ì…ë ¥: {x.shape}\")\nprint(f\"ì¶œë ¥: {out.shape}\")\nprint(f\"ì”ì°¨ ì—°ê²°ë¡œ shape ìœ ì§€ë¨ âœ“\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ® AtariGPT ì „ì²´ ëª¨ë¸ êµ¬í˜„\n# ============================================================\n# model_atari.pyì˜ GPT í´ë˜ìŠ¤ë¥¼ ê°„ì†Œí™”í•˜ì—¬ êµ¬í˜„í•©ë‹ˆë‹¤.\n# ë‘ ê°€ì§€ ëª¨ë“œë¥¼ ì§€ì›í•©ë‹ˆë‹¤:\n#   - reward_conditioned: Decision Transformer (RTG ì‚¬ìš©)\n#   - naive: Behavior Cloning (RTG ë¯¸ì‚¬ìš©)\n# ============================================================\n\nclass AtariGPT(nn.Module):\n    \"\"\"\n    Atariìš© Decision Transformer\n    \n    Parameters:\n    -----------\n    vocab_size : int\n        Action ìˆ˜ (ê²Œì„ë§ˆë‹¤ ë‹¤ë¦„: Breakout=4, Seaquest=18)\n    block_size : int\n        ì´ ì‹œí€€ìŠ¤ ê¸¸ì´ (context_length * 3)\n    n_layer : int\n        Transformer ë ˆì´ì–´ ìˆ˜\n    n_head : int\n        Attention head ìˆ˜\n    n_embd : int\n        Hidden dimension\n    max_timestep : int\n        ì—í”¼ì†Œë“œ ë‚´ ìµœëŒ€ timestep\n    model_type : str\n        'reward_conditioned' (DT) or 'naive' (BC)\n    \"\"\"\n    \n    def __init__(self, vocab_size, block_size, n_layer=6, n_head=8, n_embd=128,\n                 max_timestep=4000, model_type='reward_conditioned'):\n        super().__init__()\n        \n        self.model_type = model_type\n        self.block_size = block_size\n        self.n_embd = n_embd\n        \n        # ===== Position Embeddings =====\n        # ì‹œí€€ìŠ¤ ë‚´ ìœ„ì¹˜ ì„ë² ë”©\n        self.pos_emb = nn.Parameter(torch.zeros(1, block_size + 1, n_embd))\n        # ì—í”¼ì†Œë“œ ë‚´ ì ˆëŒ€ ì‹œê°„ ì„ë² ë”©\n        self.global_pos_emb = nn.Parameter(torch.zeros(1, max_timestep + 1, n_embd))\n        \n        self.drop = nn.Dropout(0.1)\n        \n        # ===== Transformer Blocks =====\n        self.blocks = nn.Sequential(*[\n            Block(n_embd, n_head, block_size) for _ in range(n_layer)\n        ])\n        self.ln_f = nn.LayerNorm(n_embd)\n        \n        # ===== Prediction Head =====\n        # Action ë¶„ë¥˜ (logits ì¶œë ¥)\n        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n        \n        # ===== Encoders =====\n        # State: CNN Encoder\n        self.state_encoder = AtariCNNEncoder(n_embd)\n        \n        # RTG: Linear + Tanh\n        self.ret_emb = nn.Sequential(nn.Linear(1, n_embd), nn.Tanh())\n        \n        # Action: Embedding Lookup + Tanh\n        self.action_embeddings = nn.Sequential(\n            nn.Embedding(vocab_size, n_embd),\n            nn.Tanh()\n        )\n        \n    def forward(self, states, actions, targets=None, rtgs=None, timesteps=None):\n        \"\"\"\n        Forward pass\n        \n        Args:\n            states: (B, K, 4*84*84) - í‰íƒ„í™”ëœ ì´ë¯¸ì§€\n            actions: (B, K, 1) - ì´ì‚° action\n            targets: (B, K) - ì •ë‹µ action (ì†ì‹¤ ê³„ì‚°ìš©)\n            rtgs: (B, K, 1) - RTG\n            timesteps: (B, 1, 1) - ì‹œì‘ timestep\n            \n        Returns:\n            logits: (B, K, vocab_size) - action logits\n            loss: scalar or None\n        \"\"\"\n        batch_size = states.shape[0]\n        seq_len = states.shape[1]  # K = context_length\n        \n        # ===== State Embedding via CNN =====\n        # (B, K, 4*84*84) â†’ (B*K, 4, 84, 84)\n        states_reshaped = states.reshape(-1, 4, 84, 84)\n        # CNN í†µê³¼\n        state_embeddings = self.state_encoder(states_reshaped)\n        # (B*K, n_embd) â†’ (B, K, n_embd)\n        state_embeddings = state_embeddings.reshape(batch_size, seq_len, self.n_embd)\n        \n        # ===== ëª¨ë“œë³„ ì‹œí€€ìŠ¤ êµ¬ì„± =====\n        if self.model_type == 'reward_conditioned':\n            # Decision Transformer: [R, s, a, R, s, a, ...]\n            rtg_embeddings = self.ret_emb(rtgs.float())\n            action_embeddings = self.action_embeddings(actions.squeeze(-1).long())\n            \n            # ì‹œí€€ìŠ¤ êµ¬ì„±: 3K ê¸¸ì´\n            token_embeddings = torch.zeros(\n                batch_size, seq_len * 3, self.n_embd,\n                device=states.device\n            )\n            token_embeddings[:, 0::3, :] = rtg_embeddings     # ìœ„ì¹˜ 0, 3, 6, ...\n            token_embeddings[:, 1::3, :] = state_embeddings   # ìœ„ì¹˜ 1, 4, 7, ...\n            token_embeddings[:, 2::3, :] = action_embeddings  # ìœ„ì¹˜ 2, 5, 8, ...\n            \n        else:  # naive (Behavior Cloning)\n            # BC: [s, a, s, a, ...]\n            action_embeddings = self.action_embeddings(actions.squeeze(-1).long())\n            \n            # ì‹œí€€ìŠ¤ êµ¬ì„±: 2K ê¸¸ì´\n            token_embeddings = torch.zeros(\n                batch_size, seq_len * 2, self.n_embd,\n                device=states.device\n            )\n            token_embeddings[:, 0::2, :] = state_embeddings   # ìœ„ì¹˜ 0, 2, 4, ...\n            token_embeddings[:, 1::2, :] = action_embeddings  # ìœ„ì¹˜ 1, 3, 5, ...\n        \n        # ===== Position Embedding ì¶”ê°€ =====\n        position_embeddings = self.pos_emb[:, :token_embeddings.shape[1], :]\n        \n        # ===== Transformer í†µê³¼ =====\n        x = self.drop(token_embeddings + position_embeddings)\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        \n        # ===== Prediction Head =====\n        logits = self.head(x)\n        \n        # ===== State ìœ„ì¹˜ì˜ ì¶œë ¥ë§Œ ì‚¬ìš© =====\n        if self.model_type == 'reward_conditioned':\n            logits = logits[:, 1::3, :]  # State ìœ„ì¹˜ (1, 4, 7, ...)\n        else:\n            logits = logits[:, 0::2, :]  # State ìœ„ì¹˜ (0, 2, 4, ...)\n        \n        # ===== ì†ì‹¤ ê³„ì‚° =====\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(\n                logits.reshape(-1, logits.size(-1)),\n                targets.reshape(-1)\n            )\n        \n        return logits, loss\n\n# ============================================================\n# ğŸ§ª AtariGPT í…ŒìŠ¤íŠ¸\n# ============================================================\nprint(\"ğŸ® AtariGPT ëª¨ë¸ ìƒì„± ë° í…ŒìŠ¤íŠ¸\")\nprint(\"=\"*60)\n\nmodel = AtariGPT(\n    vocab_size=4,       # Breakout: 4 actions\n    block_size=90,      # context_length * 3 = 30 * 3\n    n_layer=6,\n    n_head=8,\n    n_embd=128,\n    model_type='reward_conditioned'\n)\n\n# íŒŒë¼ë¯¸í„° ìˆ˜\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"\\nğŸ“Š ëª¨ë¸ í†µê³„:\")\nprint(f\"   ì´ íŒŒë¼ë¯¸í„°: {total_params:,}\")\nprint(f\"   ëª¨ë¸ íƒ€ì…: {model.model_type}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ”„ Forward Pass í…ŒìŠ¤íŠ¸\n# ============================================================\n# ëª¨ë¸ì— ë”ë¯¸ ì…ë ¥ì„ ë„£ì–´ ë™ì‘ì„ í™•ì¸í•©ë‹ˆë‹¤.\n# ============================================================\n\nprint(\"ğŸ”„ Forward Pass í…ŒìŠ¤íŠ¸\")\nprint(\"=\"*60)\n\nbatch_size = 2\ncontext_len = 30\n\n# ì…ë ¥ ìƒì„± (ì •ê·œí™”ëœ ì´ë¯¸ì§€)\nstates = torch.randn(batch_size, context_len, 4 * 84 * 84)     # í‰íƒ„í™”ëœ ì´ë¯¸ì§€\nactions = torch.randint(0, 4, (batch_size, context_len, 1))    # ì´ì‚° action\nrtgs = torch.randn(batch_size, context_len, 1)                  # RTG\ntimesteps = torch.zeros(batch_size, 1, 1, dtype=torch.long)    # ì‹œì‘ timestep\n\nprint(\"\\nğŸ“¥ ì…ë ¥ Shapes:\")\nprint(f\"   states:    {states.shape}\")\nprint(f\"              â””â”€ (batch, context_len, 4Ã—84Ã—84)\")\nprint(f\"   actions:   {actions.shape}\")\nprint(f\"              â””â”€ (batch, context_len, 1)\")\nprint(f\"   rtgs:      {rtgs.shape}\")\nprint(f\"              â””â”€ (batch, context_len, 1)\")\nprint(f\"   timesteps: {timesteps.shape}\")\nprint(f\"              â””â”€ (batch, 1, 1)\")\n\n# Forward (ì†ì‹¤ ê³„ì‚° í¬í•¨)\nmodel.eval()\nwith torch.no_grad():\n    targets = actions.squeeze(-1)  # ì •ë‹µ action\n    logits, loss = model(states, actions, targets, rtgs, timesteps)\n\nprint(f\"\\nğŸ“¤ ì¶œë ¥ Shapes:\")\nprint(f\"   logits: {logits.shape}\")\nprint(f\"           â””â”€ (batch, context_len, vocab_size={4})\")\nprint(f\"   loss:   {loss.item():.4f}\")\n\n# Logits â†’ Action ë³€í™˜ (argmax)\npredicted_actions = logits.argmax(dim=-1)\nprint(f\"\\nğŸ¯ ì˜ˆì¸¡ëœ Action (ë§ˆì§€ë§‰ timestep):\")\nprint(f\"   batch 0: {predicted_actions[0, -1].item()}\")\nprint(f\"   batch 1: {predicted_actions[1, -1].item()}\")\n\n# Softmaxë¡œ í™•ë¥  ë³€í™˜\nprobs = F.softmax(logits[0, -1], dim=-1)\nprint(f\"\\nğŸ“Š Action í™•ë¥  ë¶„í¬ (batch 0, ë§ˆì§€ë§‰ timestep):\")\nfor i, p in enumerate(probs):\n    bar = 'â–ˆ' * int(p * 20)\n    print(f\"   Action {i}: {p:.3f} {bar}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# 5. Reward-Conditioned vs Naive ë¹„êµ\n\n## ğŸ¯ ëª©í‘œ\n> Decision Transformerì˜ ë‘ ëª¨ë“œ (reward_conditioned, naive)ì˜ ì°¨ì´ë¥¼ ì´í•´í•©ë‹ˆë‹¤.\n\n## ğŸ“Š ë‘ ëª¨ë“œ ë¹„êµ\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚        í•­ëª©        â”‚   Reward-Conditioned (DT)    â”‚         Naive (BC)           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ ì‹œí€€ìŠ¤ êµ¬ì„±        â”‚ [Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚, aâ‚,...] â”‚ [sâ‚€, aâ‚€, sâ‚, aâ‚, ...]        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ ì‹œí€€ìŠ¤ ê¸¸ì´        â”‚ K Ã— 3                        â”‚ K Ã— 2                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ ì˜ˆì¸¡ ì¶”ì¶œ ìœ„ì¹˜     â”‚ 1::3 (State ìœ„ì¹˜)            â”‚ 0::2 (State ìœ„ì¹˜)            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ ì¡°ê±´í™”             â”‚ âœ… RTGë¡œ ëª©í‘œ return ì§€ì •   â”‚ âŒ ì¡°ê±´ ì—†ìŒ                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ ê²Œì„ë³„ ëª©í‘œ RTG    â”‚ Breakout: 90                 â”‚                              â”‚\nâ”‚                    â”‚ Seaquest: 1150               â”‚ ì‚¬ìš© ì•ˆ í•¨                   â”‚\nâ”‚                    â”‚ Qbert: 14000                 â”‚ (0ìœ¼ë¡œ ì„¤ì •)                 â”‚\nâ”‚                    â”‚ Pong: 20                     â”‚                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ ì¶”ë¡  ì‹œ RTG        â”‚ ë™ì  ì—…ë°ì´íŠ¸ (rtg -= r)     â”‚ ê³ ì •                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ ì„±ëŠ¥ ì¡°ì ˆ          â”‚ RTGë¥¼ ë†’ê²Œ â†’ ë” ë‚˜ì€ ì„±ëŠ¥    â”‚ ì¡°ì ˆ ë¶ˆê°€                    â”‚\nâ”‚                    â”‚ RTGë¥¼ ë‚®ê²Œ â†’ ë” ì•ˆì „í•œ í–‰ë™  â”‚ (ë°ì´í„° í‰ê·  ì„±ëŠ¥)           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## ğŸ’¡ í•µì‹¬ ì°¨ì´\n\n**Reward-Conditioned (DT):**\n- \"1000ì ì„ ì–»ê³  ì‹¶ë‹¤\" â†’ 1000ì  ì–»ì„ ë•Œì˜ í–‰ë™ ì˜ˆì¸¡\n- ë‹¤ì–‘í•œ ìˆ˜ì¤€ì˜ ì •ì±… ìƒì„± ê°€ëŠ¥\n\n**Naive (BC):**\n- ë‹¨ìˆœíˆ ë°ì´í„°ì˜ í‰ê· ì ì¸ í–‰ë™ ëª¨ë°©\n- ì¡°ê±´ ì—†ì´ ë¬´ì¡°ê±´ ëª¨ë°©"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“Š ë‘ ëª¨ë“œ ë¹„êµí‘œ ì¶œë ¥\n# ============================================================\n# ASCII ì•„íŠ¸ë¡œ ë‘ ëª¨ë“œì˜ ì°¨ì´ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.\n# ============================================================\n\ncomparison = \"\"\"\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    Reward-Conditioned vs Naive ë¹„êµ                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                            â”‚\nâ”‚  ğŸ¯ Reward-Conditioned (Decision Transformer)                              â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚\nâ”‚  ì‹œí€€ìŠ¤: [Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚, aâ‚, Râ‚‚, sâ‚‚, aâ‚‚, ...]                        â”‚\nâ”‚           â†‘    â†‘    â†‘                                                      â”‚\nâ”‚          RTG  State Action                                                 â”‚\nâ”‚                                                                            â”‚\nâ”‚  ì˜ˆì¸¡:   Râ‚€ â†’ sâ‚€ â†’ aâ‚€?    (RTGì™€ Stateê°€ ì£¼ì–´ì§€ë©´ Action ì˜ˆì¸¡)             â”‚\nâ”‚               â†‘                                                            â”‚\nâ”‚          1::3 ìœ„ì¹˜ì—ì„œ ì¶”ì¶œ                                                â”‚\nâ”‚                                                                            â”‚\nâ”‚  íŠ¹ì§•:                                                                     â”‚\nâ”‚    â€¢ ëª©í‘œ RTGë¥¼ ì§€ì •í•˜ë©´ ê·¸ì— ë§ëŠ” í–‰ë™ ì˜ˆì¸¡                               â”‚\nâ”‚    â€¢ ë†’ì€ RTG â†’ ê³ ì„±ëŠ¥ ì •ì±…, ë‚®ì€ RTG â†’ ë³´ìˆ˜ì  ì •ì±…                        â”‚\nâ”‚    â€¢ \"ë‚˜ëŠ” 90ì ì„ ì›í•´!\" â†’ 90ì  ë‹¬ì„± ì‹œì˜ í–‰ë™ ì˜ˆì¸¡                        â”‚\nâ”‚                                                                            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                            â”‚\nâ”‚  ğŸ“‹ Naive (Behavior Cloning)                                               â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚\nâ”‚  ì‹œí€€ìŠ¤: [sâ‚€, aâ‚€, sâ‚, aâ‚, sâ‚‚, aâ‚‚, ...]                                     â”‚\nâ”‚           â†‘    â†‘                                                           â”‚\nâ”‚         State Action (RTG ì—†ìŒ!)                                           â”‚\nâ”‚                                                                            â”‚\nâ”‚  ì˜ˆì¸¡:   sâ‚€ â†’ aâ‚€?    (Stateë§Œ ë³´ê³  Action ì˜ˆì¸¡)                            â”‚\nâ”‚          â†‘                                                                 â”‚\nâ”‚     0::2 ìœ„ì¹˜ì—ì„œ ì¶”ì¶œ                                                     â”‚\nâ”‚                                                                            â”‚\nâ”‚  íŠ¹ì§•:                                                                     â”‚\nâ”‚    â€¢ RTG ì¡°ê±´ ì—†ì´ ë‹¨ìˆœ ëª¨ë°©                                               â”‚\nâ”‚    â€¢ ë°ì´í„° í‰ê·  ìˆ˜ì¤€ì˜ ì •ì±…                                               â”‚\nâ”‚    â€¢ \"ê·¸ëƒ¥ ë°ì´í„°ì—ì„œ ë³¸ ëŒ€ë¡œ í–‰ë™í• ê²Œ!\"                                   â”‚\nâ”‚                                                                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\"\"\"\nprint(comparison)\n\n# ê²Œì„ë³„ ëª©í‘œ RTG\nprint(\"\\nğŸ“ˆ ê²Œì„ë³„ ëª©í‘œ RTG (run_dt_atari.pyì—ì„œ ì‚¬ìš©):\")\ngame_rtgs = {\n    'Breakout': 90,\n    'Pong': 20,\n    'Qbert': 14000,\n    'Seaquest': 1150,\n}\nprint(f\"{'ê²Œì„':<15} {'ëª©í‘œ RTG':>10} {'ì„¤ëª…':<30}\")\nprint(\"-\"*60)\nfor game, rtg in game_rtgs.items():\n    if game == 'Breakout':\n        desc = \"ë²½ëŒ 90ê°œ ê¹¨ê¸°\"\n    elif game == 'Pong':\n        desc = \"21ì  ì¤‘ 20ì  íšë“\"\n    elif game == 'Qbert':\n        desc = \"ë†’ì€ ì ìˆ˜ ëª©í‘œ\"\n    else:\n        desc = \"ì ìˆ˜í•¨ ê²Œì„\"\n    print(f\"{game:<15} {rtg:>10} {desc:<30}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“Š ì‹œí€€ìŠ¤ êµ¬ì„± ì‹œê°í™”\n# ============================================================\n# ë‘ ëª¨ë“œì˜ ì‹œí€€ìŠ¤ êµ¬ì„±ì„ ì‹œê°ì ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤.\n# ============================================================\n\nprint(\"ğŸ“Š ì‹œí€€ìŠ¤ êµ¬ì„± ì‹œê°í™”\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 7))\n\n# ===== Reward-Conditioned =====\nrc_seq = ['Râ‚€', 'sâ‚€', 'aâ‚€', 'Râ‚', 'sâ‚', 'aâ‚', 'Râ‚‚', 'sâ‚‚', 'aâ‚‚']\nrc_colors = ['#ff6b6b', '#4ecdc4', '#45b7d1'] * 3  # ë¹¨ê°•, ì²­ë¡, íŒŒë‘\n\nax1 = axes[0]\nfor i, (token, color) in enumerate(zip(rc_seq, rc_colors)):\n    rect = plt.Rectangle((i, 0), 0.9, 1, facecolor=color, edgecolor='black', alpha=0.8)\n    ax1.add_patch(rect)\n    ax1.text(i + 0.45, 0.5, token, ha='center', va='center', fontsize=12, fontweight='bold')\n    \n    # State ìœ„ì¹˜ì—ì„œ ì˜ˆì¸¡ í™”ì‚´í‘œ (1::3)\n    if i % 3 == 1:\n        ax1.annotate('', xy=(i + 0.45, -0.15), xytext=(i + 0.45, -0.4),\n                    arrowprops=dict(arrowstyle='->', color='red', lw=2))\n        ax1.text(i + 0.45, -0.6, f'â†’a{i//3} ì˜ˆì¸¡', ha='center', color='red', fontsize=10, fontweight='bold')\n\nax1.set_xlim(-0.5, len(rc_seq) + 0.5)\nax1.set_ylim(-1, 1.5)\nax1.set_title('ğŸ¯ Reward-Conditioned: [R, s, a] íŠ¸ë¦¬í”Œ Ã— K\\nì˜ˆì¸¡ ìœ„ì¹˜: 1::3 (State ìœ„ì¹˜)', fontsize=14)\nax1.axis('off')\n\n# ===== Naive =====\nnaive_seq = ['sâ‚€', 'aâ‚€', 'sâ‚', 'aâ‚', 'sâ‚‚', 'aâ‚‚']\nnaive_colors = ['#4ecdc4', '#45b7d1'] * 3  # ì²­ë¡, íŒŒë‘ (RTG ì—†ìŒ)\n\nax2 = axes[1]\nfor i, (token, color) in enumerate(zip(naive_seq, naive_colors)):\n    rect = plt.Rectangle((i, 0), 0.9, 1, facecolor=color, edgecolor='black', alpha=0.8)\n    ax2.add_patch(rect)\n    ax2.text(i + 0.45, 0.5, token, ha='center', va='center', fontsize=12, fontweight='bold')\n    \n    # State ìœ„ì¹˜ì—ì„œ ì˜ˆì¸¡ í™”ì‚´í‘œ (0::2)\n    if i % 2 == 0:\n        ax2.annotate('', xy=(i + 0.45, -0.15), xytext=(i + 0.45, -0.4),\n                    arrowprops=dict(arrowstyle='->', color='red', lw=2))\n        ax2.text(i + 0.45, -0.6, f'â†’a{i//2} ì˜ˆì¸¡', ha='center', color='red', fontsize=10, fontweight='bold')\n\nax2.set_xlim(-0.5, len(naive_seq) + 0.5)\nax2.set_ylim(-1, 1.5)\nax2.set_title('ğŸ“‹ Naive (BC): [s, a] í˜ì–´ Ã— K\\nì˜ˆì¸¡ ìœ„ì¹˜: 0::2 (State ìœ„ì¹˜)', fontsize=14)\nax2.axis('off')\n\n# ë²”ë¡€\nfrom matplotlib.patches import Patch\nlegend_elements = [\n    Patch(facecolor='#ff6b6b', edgecolor='black', label='RTG (R)'),\n    Patch(facecolor='#4ecdc4', edgecolor='black', label='State (s)'),\n    Patch(facecolor='#45b7d1', edgecolor='black', label='Action (a)'),\n]\nfig.legend(handles=legend_elements, loc='upper right', ncol=3, fontsize=11)\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.85)\nplt.show()\n\nprint(\"\\nğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:\")\nprint(\"   â€¢ Reward-ConditionedëŠ” RTG í† í°ì´ ì¶”ê°€ë˜ì–´ ì‹œí€€ìŠ¤ê°€ 1.5ë°° ê¸¸ì–´ì§\")\nprint(\"   â€¢ ë‘ ëª¨ë“œ ëª¨ë‘ State ìœ„ì¹˜ì—ì„œ ë‹¤ìŒ Actionì„ ì˜ˆì¸¡\")\nprint(\"   â€¢ ì˜ˆì¸¡ ì¶”ì¶œ ì¸ë±ì‹±: RCëŠ” 1::3, NaiveëŠ” 0::2\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“Š ë‘ ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¹„êµ\n# ============================================================\n# Reward-Conditionedì™€ Naive ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.\n# ============================================================\n\nprint(\"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¹„êµ\")\nprint(\"=\"*60)\n\n# ë‘ ëª¨ë¸ ìƒì„±\nmodel_rc = AtariGPT(vocab_size=4, block_size=90, model_type='reward_conditioned')\nmodel_naive = AtariGPT(vocab_size=4, block_size=60, model_type='naive')  # 2/3 ê¸¸ì´\n\nparams_rc = sum(p.numel() for p in model_rc.parameters())\nparams_naive = sum(p.numel() for p in model_naive.parameters())\n\nprint(f\"\\n{'ëª¨ë¸':<25} {'íŒŒë¼ë¯¸í„° ìˆ˜':>15}\")\nprint(\"-\"*45)\nprint(f\"{'Reward-Conditioned':<25} {params_rc:>15,}\")\nprint(f\"{'Naive (BC)':<25} {params_naive:>15,}\")\nprint(f\"{'ì°¨ì´':<25} {params_rc - params_naive:>15,}\")\n\nprint(f\"\\nğŸ“ ì°¨ì´ ë°œìƒ ì´ìœ :\")\nprint(f\"   â€¢ Block size: RC={90} vs Naive={60}\")\nprint(f\"   â€¢ Position embedding í¬ê¸°ê°€ ë‹¤ë¦„\")\nprint(f\"   â€¢ RTG embedding ë ˆì´ì–´ (Naiveì—ëŠ” ì—†ìŒ)\")\nprint(f\"   â€¢ í•˜ì§€ë§Œ ë©”ì¸ Transformer íŒŒë¼ë¯¸í„°ëŠ” ë™ì¼\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ® ì¶”ë¡  ì‹œ RTG ì—…ë°ì´íŠ¸\n\n> Reward-Conditioned ëª¨ë“œì—ì„œëŠ” ì¶”ë¡  ì‹œ RTGë¥¼ ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n\n### RTG ì—…ë°ì´íŠ¸ ê³¼ì •\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    Breakout ê²Œì„ ì¶”ë¡  ì‹œë®¬ë ˆì´ì…˜                            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                            â”‚\nâ”‚  ëª©í‘œ: 90ì  (ë²½ëŒ 90ê°œ)                                                    â”‚\nâ”‚                                                                            â”‚\nâ”‚  Step 0:  RTG = 90  â†’  Model  â†’  Action: ì˜¤ë¥¸ìª½                           â”‚\nâ”‚           â”‚                       â†“                                        â”‚\nâ”‚           â”‚              í™˜ê²½: reward = 0 (ë¹—ë‚˜ê°)                         â”‚\nâ”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚\nâ”‚                                                                            â”‚\nâ”‚  Step 1:  RTG = 90 - 0 = 90  â†’  Model  â†’  Action: ê³µ ë°œì‚¬                 â”‚\nâ”‚           â”‚                       â†“                                        â”‚\nâ”‚           â”‚              í™˜ê²½: reward = 5 (ë²½ëŒ 5ê°œ íŒŒê´´!)                 â”‚\nâ”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚\nâ”‚                                                                            â”‚\nâ”‚  Step 2:  RTG = 90 - 5 = 85  â†’  Model  â†’  Action: ì™¼ìª½                    â”‚\nâ”‚           â”‚                       â†“                                        â”‚\nâ”‚           â”‚              í™˜ê²½: reward = 10 (ë²½ëŒ 10ê°œ íŒŒê´´!)               â”‚\nâ”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚\nâ”‚                                                                            â”‚\nâ”‚  Step 3:  RTG = 85 - 10 = 75  â†’  ...                                      â”‚\nâ”‚                                                                            â”‚\nâ”‚  ...ê³„ì†...                                                                â”‚\nâ”‚                                                                            â”‚\nâ”‚  Step N:  RTG â‰ˆ 0  (ëª©í‘œ ë‹¬ì„±!)                                           â”‚\nâ”‚                                                                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### ğŸ’¡ í•µì‹¬ ì›ë¦¬\n\n- **RTG = ë‚¨ì€ ëª©í‘œ**\n- rewardë¥¼ ë°›ìœ¼ë©´ **RTG -= reward**\n- ëª¨ë¸ì€ **\"ë‚¨ì€ RTGë¥¼ ë‹¬ì„±í•˜ë ¤ë©´ ì–´ë–¤ í–‰ë™?\"** ì„ ì˜ˆì¸¡\n- RTGê°€ 0ì— ê°€ê¹Œì›Œì§€ë©´ ëª©í‘œ ë‹¬ì„±!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ® Breakout ê²Œì„ ì¶”ë¡  ì‹œë®¬ë ˆì´ì…˜\n# ============================================================\n# ì‹¤ì œ í™˜ê²½ ì—†ì´ RTG ì—…ë°ì´íŠ¸ ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.\n# ============================================================\n\ndef simulate_breakout_inference(target_rtg=90, max_steps=30):\n    \"\"\"\n    Breakout ê²Œì„ì—ì„œì˜ RTG ê¸°ë°˜ ì¶”ë¡  ì‹œë®¬ë ˆì´ì…˜\n    \n    Parameters:\n    -----------\n    target_rtg : int\n        ëª©í‘œ ì ìˆ˜ (ë²½ëŒ ê°œìˆ˜)\n    max_steps : int\n        ì‹œë®¬ë ˆì´ì…˜ ìµœëŒ€ ìŠ¤í…\n        \n    Returns:\n    --------\n    list: ê° ìŠ¤í…ì˜ ê¸°ë¡\n    \"\"\"\n    rtg = target_rtg\n    steps = []\n    \n    # Breakout ë³´ìƒ íŒ¨í„´ ì‹œë®¬ë ˆì´ì…˜\n    # ì‹¤ì œë¡œëŠ” ë²½ëŒì„ ê¹¨ë©´ 1~7ì  (ìƒ‰ìƒë³„)\n    reward_pattern = [0, 0, 0, 5, 0, 0, 0, 5, 0, 5,\n                      0, 0, 10, 0, 0, 5, 0, 0, 0, 10,\n                      0, 5, 0, 0, 5, 0, 0, 10, 0, 0]\n    \n    print(f\"ğŸ® Breakout ì¶”ë¡  ì‹œë®¬ë ˆì´ì…˜\")\n    print(f\"   ëª©í‘œ ì ìˆ˜: {target_rtg}\")\n    print(\"=\"*60)\n    \n    total_reward = 0\n    for t in range(min(max_steps, len(reward_pattern))):\n        reward = reward_pattern[t]\n        \n        # ê¸°ë¡\n        steps.append({\n            't': t,\n            'rtg_before': rtg,\n            'reward': reward,\n            'rtg_after': rtg - reward,\n            'total_reward': total_reward + reward\n        })\n        \n        # ë²½ëŒì„ ê¹¼ì„ ë•Œë§Œ ì¶œë ¥\n        if reward > 0:\n            action = np.random.choice(['â†', 'â†’', 'â—‹'])  # ê°€ìƒ ì•¡ì…˜\n            print(f\"Step {t:2d}: RTG={rtg:>3} â†’ Action={action} â†’ ë²½ëŒ íŒŒê´´! reward={reward:>2} â†’ ìƒˆ RTG={rtg-reward:>3}\")\n        \n        # RTG ì—…ë°ì´íŠ¸\n        rtg = rtg - reward\n        total_reward += reward\n        \n        # ëª©í‘œ ë‹¬ì„± ì²´í¬\n        if rtg <= 0:\n            print()\n            print(f\"ğŸ‰ ëª©í‘œ ë‹¬ì„±! (RTG â‰¤ 0)\")\n            break\n    \n    print(\"-\"*60)\n    print(f\"\\nğŸ“Š ê²°ê³¼ ìš”ì•½:\")\n    print(f\"   ì´ ìŠ¤í…: {len(steps)}\")\n    print(f\"   ì´ íšë“ ì ìˆ˜: {total_reward}\")\n    print(f\"   ëª©í‘œ ëŒ€ë¹„: {total_reward/target_rtg*100:.1f}%\")\n    print(f\"   ìµœì¢… RTG: {rtg}\")\n    \n    return steps\n\n# ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰\nsteps = simulate_breakout_inference(target_rtg=90)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“ˆ RTG ë³€í™” ì‹œê°í™”\n# ============================================================\n# ì¶”ë¡  ê³¼ì •ì—ì„œ RTGì™€ ëˆ„ì  ë³´ìƒì´ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ì‹œê°í™”í•©ë‹ˆë‹¤.\n# ============================================================\n\nprint(\"ğŸ“ˆ RTG ë³€í™” ì‹œê°í™”\")\nprint(\"=\"*60)\n\n# ë°ì´í„° ì¶”ì¶œ\ntimesteps_viz = [s['t'] for s in steps]\nrtgs_before = [s['rtg_before'] for s in steps]\nrewards = [s['reward'] for s in steps]\ntotal_rewards = [s['total_reward'] for s in steps]\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 8))\n\n# ===== RTG ë³€í™” =====\nax1 = axes[0]\nax1.plot(timesteps_viz, rtgs_before, 'o-', color='coral', linewidth=2, markersize=6, label='RTG')\nax1.axhline(y=0, color='green', linestyle='--', linewidth=2, label='Goal (RTG=0)')\nax1.fill_between(timesteps_viz, rtgs_before, 0, alpha=0.3, color='coral')\n\n# ë²½ëŒ íŒŒê´´ ì§€ì  í‘œì‹œ\nhit_steps = [s['t'] for s in steps if s['reward'] > 0]\nhit_rtgs = [s['rtg_before'] for s in steps if s['reward'] > 0]\nax1.scatter(hit_steps, hit_rtgs, color='red', s=100, zorder=5, label='Hit!')\n\nax1.set_ylabel('Return-to-Go', fontsize=12)\nax1.set_title('RTG Evolution During Inference (Breakout)', fontsize=14)\nax1.legend(loc='upper right')\nax1.grid(True, alpha=0.3)\nax1.set_xlim(-1, len(steps))\n\n# ===== ëˆ„ì  ë³´ìƒ =====\nax2 = axes[1]\n\n# ë³´ìƒ ë§‰ëŒ€ ê·¸ë˜í”„\ncolors = ['green' if r > 0 else 'lightgray' for r in rewards]\nax2.bar(timesteps_viz, rewards, color=colors, alpha=0.7, label='Instant Reward')\n\n# ëˆ„ì  ë³´ìƒ ì„  ê·¸ë˜í”„\nax2_twin = ax2.twinx()\nax2_twin.plot(timesteps_viz, total_rewards, 'b-', linewidth=2, marker='s', markersize=4, label='Cumulative Reward')\nax2_twin.set_ylabel('Cumulative Reward', fontsize=12, color='blue')\nax2_twin.tick_params(axis='y', labelcolor='blue')\n\nax2.set_xlabel('Step', fontsize=12)\nax2.set_ylabel('Instant Reward', fontsize=12)\nax2.set_title('Reward Accumulation (ë…¹ìƒ‰ = ë²½ëŒ íŒŒê´´)', fontsize=14)\nax2.set_xlim(-1, len(steps))\nax2.legend(loc='upper left')\nax2_twin.legend(loc='upper right')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# í•µì‹¬ ì¸ì‚¬ì´íŠ¸\nprint(\"\\nğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:\")\nprint(f\"   â€¢ RTGëŠ” rewardë¥¼ ë°›ì„ ë•Œë§ˆë‹¤ ê°ì†Œ (= ëª©í‘œì— ê°€ê¹Œì›Œì§)\")\nprint(f\"   â€¢ ëˆ„ì  ë³´ìƒì€ RTGì˜ ì—­ë°©í–¥ìœ¼ë¡œ ì¦ê°€\")\nprint(f\"   â€¢ RTG + ëˆ„ì ë³´ìƒ â‰ˆ ì´ˆê¸° ëª©í‘œ (= {steps[0]['rtg_before']})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# ğŸ“ ì—°ìŠµ ë¬¸ì œ\n\n> ì´í•´ë„ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ì—°ìŠµ ë¬¸ì œì…ë‹ˆë‹¤.\n\n## ì—°ìŠµ 1: CNN ì¶œë ¥ í¬ê¸° ê³„ì‚°\n\n**ë¬¸ì œ**: ì…ë ¥ ì´ë¯¸ì§€ê°€ 64Ã—64 (84Ã—84 ëŒ€ì‹ )ì¼ ë•Œ, CNN ì¶œë ¥ í¬ê¸°ëŠ”?\n\n```\nConv1: kernel=8, stride=4\nConv2: kernel=4, stride=2\nConv3: kernel=3, stride=1\n```\n\n**ê³µì‹**: `output = (input - kernel) / stride + 1`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ============================================================\n# ì—°ìŠµ 1: ì§ì ‘ ê³„ì‚°í•´ë³´ì„¸ìš”!\n# ============================================================\n# TODO: ì•„ë˜ ì½”ë“œë¥¼ ì™„ì„±í•˜ì„¸ìš”\n\ndef calc_output(input_size, kernel, stride):\n    \"\"\"Conv ì¶œë ¥ í¬ê¸° ê³„ì‚°\"\"\"\n    return (input_size - kernel) // stride + 1\n\n# 64x64 ì…ë ¥ì— ëŒ€í•œ ê³„ì‚°\nsize = 64\nprint(f\"ì…ë ¥: {size}x{size}\")\n\n# Conv1\n# size = calc_output(size, ???, ???)\n# print(f\"Conv1: {size}x{size}\")\n\n# Conv2\n# size = calc_output(size, ???, ???)\n# print(f\"Conv2: {size}x{size}\")\n\n# Conv3\n# size = calc_output(size, ???, ???)\n# print(f\"Conv3: {size}x{size}\")\n\n# print(f\"\\nFlatten: 64 Ã— {size} Ã— {size} = {64 * size * size}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“– ì—°ìŠµ 1 ì •ë‹µ\n# ============================================================\n# ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ì •ë‹µì„ í™•ì¸í•˜ì„¸ìš”.\n# ============================================================\n\nprint(\"ğŸ“– ì—°ìŠµ 1 ì •ë‹µ\")\nprint(\"=\"*40)\n\nsize = 64\nprint(f\"ì…ë ¥: {size}x{size}\")\n\n# Conv1: (64-8)/4 + 1 = 15\nsize = calc_output(size, 8, 4)\nprint(f\"Conv1 (k=8, s=4): {size}x{size}\")\n\n# Conv2: (15-4)/2 + 1 = 6\nsize = calc_output(size, 4, 2)\nprint(f\"Conv2 (k=4, s=2): {size}x{size}\")\n\n# Conv3: (6-3)/1 + 1 = 4\nsize = calc_output(size, 3, 1)\nprint(f\"Conv3 (k=3, s=1): {size}x{size}\")\n\nprint(f\"\\nFlatten: 64 Ã— {size} Ã— {size} = {64 * size * size}\")\nprint(f\"\\nğŸ’¡ 84x84ì¼ ë•Œì˜ 3136ê³¼ ë¹„êµí•˜ë©´ {64 * size * size}ì€ ë” ì‘ìŠµë‹ˆë‹¤!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ì—°ìŠµ 2: ëª¨ë“œë³„ ì‹œí€€ìŠ¤ ê¸¸ì´\n\n**ë¬¸ì œ**: `context_length = 50`ì¼ ë•Œ, ê° ëª¨ë“œì˜ ì‹¤ì œ í† í° ì‹œí€€ìŠ¤ ê¸¸ì´ëŠ”?\n\n| ëª¨ë“œ | ì‹œí€€ìŠ¤ êµ¬ì„± | ê¸¸ì´ ê³„ì‚° | ì •ë‹µ |\n|:---|:---|:---|:---|\n| Reward-Conditioned | [R, s, a, ...] | 50 Ã— ? | ? |\n| Naive | [s, a, ...] | 50 Ã— ? | ? |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ============================================================\n# ì—°ìŠµ 2: ì§ì ‘ ê³„ì‚°í•´ë³´ì„¸ìš”!\n# ============================================================\n\ncontext_length = 50\n\n# TODO: ê° ëª¨ë“œì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ê³„ì‚°í•˜ì„¸ìš”\n# reward_conditioned_seq_len = context_length * ???\n# naive_seq_len = context_length * ???\n\n# print(f\"Reward-Conditioned: {reward_conditioned_seq_len}\")\n# print(f\"Naive: {naive_seq_len}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“– ì—°ìŠµ 2 ì •ë‹µ\n# ============================================================\n\nprint(\"ğŸ“– ì—°ìŠµ 2 ì •ë‹µ\")\nprint(\"=\"*40)\n\ncontext_length = 50\n\n# Reward-conditioned: [R, s, a] íŠ¸ë¦¬í”Œ â†’ 3ë°°\nreward_conditioned_seq_len = context_length * 3\n\n# Naive: [s, a] í˜ì–´ â†’ 2ë°°\nnaive_seq_len = context_length * 2\n\nprint(f\"Context length: {context_length}\")\nprint()\nprint(f\"Reward-Conditioned:\")\nprint(f\"   ì‹œí€€ìŠ¤ êµ¬ì„±: [R, s, a, R, s, a, ...]\")\nprint(f\"   ê¸¸ì´: {context_length} Ã— 3 = {reward_conditioned_seq_len}\")\nprint()\nprint(f\"Naive (BC):\")\nprint(f\"   ì‹œí€€ìŠ¤ êµ¬ì„±: [s, a, s, a, ...]\")\nprint(f\"   ê¸¸ì´: {context_length} Ã— 2 = {naive_seq_len}\")\nprint()\nprint(f\"ğŸ’¡ RCê°€ Naiveë³´ë‹¤ {reward_conditioned_seq_len - naive_seq_len}ê°œ í† í° ë” ë§ìŒ!\")\nprint(f\"   (RTG í† í° {context_length}ê°œê°€ ì¶”ê°€ë¨)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ì´ ì…€ì€ ì‚­ì œë¨ (ì—°ìŠµë¬¸ì œ ì •ë‹µê³¼ ë³‘í•©)\npass"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# ğŸ“ Phase 4 ì™„ë£Œ!\n\n## ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! ëª¨ë“  Phaseë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!\n\n---\n\n## ğŸ“š ì´ Phaseì—ì„œ ë°°ìš´ ë‚´ìš©\n\n### 1. DQN Replay Buffer\n```\nâ€¢ 50ê°œì˜ checkpoint (ì´ˆë³´â†’ì „ë¬¸ê°€ ì •ì±…)\nâ€¢ ê° 100,000 transitions\nâ€¢ (observation, action, reward, terminal) êµ¬ì¡°\nâ€¢ 4 stacked frames (84Ã—84Ã—4)\n```\n\n### 2. Atari ìŠ¤íƒ€ì¼ RTG ê³„ì‚°\n```\nâ€¢ ì—í”¼ì†Œë“œ ê²½ê³„ë¥¼ ì¡´ì¤‘í•˜ì—¬ ê³„ì‚°\nâ€¢ done_idxsë¡œ ê²½ê³„ ê´€ë¦¬\nâ€¢ ê° ì—í”¼ì†Œë“œ ë‚´ì—ì„œë§Œ ëˆ„ì \n```\n\n### 3. CNN Encoder\n```\nì…ë ¥: (4, 84, 84)\n  â†“ Conv1 (k=8, s=4)\n(32, 20, 20)\n  â†“ Conv2 (k=4, s=2)\n(64, 9, 9)\n  â†“ Conv3 (k=3, s=1)\n(64, 7, 7)\n  â†“ Flatten\n(3136)\n  â†“ Linear\n(128)\n```\n\n### 4. AtariGPT êµ¬ì¡°\n```\nâ€¢ CNN Encoder + Transformer\nâ€¢ Reward-Conditioned: [R, s, a] Ã— K\nâ€¢ Naive (BC): [s, a] Ã— K\nâ€¢ Cross-Entropy ì†ì‹¤ (ì´ì‚° action)\n```\n\n### 5. ì¶”ë¡  ì‹œ RTG ì—…ë°ì´íŠ¸\n```\nì´ˆê¸°: RTG = ëª©í‘œ ì ìˆ˜\në§¤ ìŠ¤í…: RTG = RTG - reward\nì¢…ë£Œ: RTG â‰ˆ 0 (ëª©í‘œ ë‹¬ì„±!)\n```\n\n---\n\n## âœ… Phase 4 ì²´í¬ë¦¬ìŠ¤íŠ¸\n\n| ì™„ë£Œ | í•­ëª© |\n|:---:|:---|\n| â˜ | DQN replay buffer êµ¬ì¡° ì´í•´ |\n| â˜ | 4 frame stackì˜ ì˜ë¯¸ ì´í•´ |\n| â˜ | Atari ìŠ¤íƒ€ì¼ RTG ê³„ì‚° (ì—í”¼ì†Œë“œ ê²½ê³„) ì´í•´ |\n| â˜ | StateActionReturnDataset ë™ì‘ ì´í•´ |\n| â˜ | CNN Encoder êµ¬ì¡° ë° ì¶œë ¥ í¬ê¸° ê³„ì‚° |\n| â˜ | AtariGPT ì „ì²´ êµ¬ì¡° ì´í•´ |\n| â˜ | Reward-Conditioned vs Naive ì°¨ì´ ì´í•´ |\n| â˜ | ì¶”ë¡  ì‹œ RTG ë™ì  ì—…ë°ì´íŠ¸ ì´í•´ |\n\n---\n\n## ğŸš€ ë‹¤ìŒ ë‹¨ê³„ (ì„ íƒ)\n\n### 1. ì‹¤ì œ ë°ì´í„°ë¡œ í•™ìŠµ\n```bash\ncd atari\ngsutil -m cp -R gs://atari-replay-datasets/dqn/Breakout dqn_replay/\npython run_dt_atari.py --game Breakout --epochs 5\n```\n\n### 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‹¤í—˜\n- context_length ë³€ê²½ (30 â†’ 50)\n- n_layer ë³€ê²½ (6 â†’ 12)\n- n_head ë³€ê²½ (8 â†’ 4)\n\n### 3. ë‹¤ë¥¸ ê²Œì„ ì‹œë„\n- Pong, Qbert, Seaquest\n- ê²Œì„ë³„ ëª©í‘œ RTG ì¡°ì •\n\n### 4. ê´€ë ¨ ë…¼ë¬¸ ì½ê¸°\n- Online Decision Transformer\n- Trajectory Transformer\n- Conservative Decision Transformer\n\n---\n\n## ğŸ—ºï¸ ì „ì²´ í•™ìŠµ ë¡œë“œë§µ ì™„ë£Œ!\n\n```\nPhase 1 â”€â”€â–¶ Phase 2 â”€â”€â–¶ Phase 3 â”€â”€â–¶ Phase 4\n  RL &        RTG &       Gym         Atari\nTransformer  ì‹œí€€ìŠ¤      ì‹¤ìŠµ         ì‹¤ìŠµ\n  ê¸°ì´ˆ        êµ¬ì„±       (ì—°ì†)      (ì´ë¯¸ì§€)\n   âœ…          âœ…          âœ…          âœ…\n```\n\n---\n\n> ğŸ‰ **ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!**  \n> Decision Transformerì˜ ì „ì²´ êµ¬ì¡°ì™€ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•˜ì…¨ìŠµë‹ˆë‹¤.  \n> ì´ì œ ì‹¤ì œ ë°ì´í„°ë¡œ í•™ìŠµí•˜ê³  ì‹¤í—˜í•´ë³´ì„¸ìš”!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}