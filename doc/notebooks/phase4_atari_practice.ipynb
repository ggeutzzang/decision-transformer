{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ® Phase 4: Atari í™˜ê²½ ì‹¤ìŠµ\n",
    "\n",
    "> **ëª©í‘œ**: Atari ê²Œì„ í™˜ê²½ì—ì„œ Decision Transformerê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì´í•´í•˜ê³ ,  \n",
    "> ì´ë¯¸ì§€ ê¸°ë°˜ ê°•í™”í•™ìŠµì˜ íŠ¹ìˆ˜ì„±ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ºï¸ ì´ Phaseì—ì„œ ë°°ìš¸ ë‚´ìš©\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph DQN[\"ğŸ“¦ DQN Replay\\në°ì´í„°ì…‹\"]\n",
    "        D1[\"84Ã—84 í”½ì…€ ì´ë¯¸ì§€\"]\n",
    "        D2[\"4 í”„ë ˆì„ ìŠ¤íƒ\"]\n",
    "        D3[\"50ê°œ ë²„í¼\"]\n",
    "    end\n",
    "\n",
    "    subgraph CNN[\"ğŸ” CNN\\nEncoder\"]\n",
    "        C1[\"Conv ë ˆì´ì–´\"]\n",
    "        C2[\"íŠ¹ì§• ì¶”ì¶œ\"]\n",
    "        C3[\"3136â†’128 ì••ì¶•\"]\n",
    "    end\n",
    "\n",
    "    subgraph GPT[\"ğŸ§  Atari\\nGPT ëª¨ë¸\"]\n",
    "        G1[\"Reward-Conditioned\"]\n",
    "        G2[\"Naive / BC\"]\n",
    "        G3[\"Cross-Entropy Loss\"]\n",
    "    end\n",
    "\n",
    "    DQN -->|\"ì´ë¯¸ì§€ ì…ë ¥\"| CNN -->|\"ì„ë² ë”© ë²¡í„°\"| GPT\n",
    "\n",
    "    style DQN fill:#fff3e0,stroke:#e65100\n",
    "    style CNN fill:#e3f2fd,stroke:#1565c0\n",
    "    style GPT fill:#e8f5e9,stroke:#2e7d32\n",
    "```\n",
    "\n",
    "## ğŸ“‹ ëª©ì°¨\n",
    "\n",
    "| ì„¹ì…˜ | ì£¼ì œ | í•™ìŠµ ë‚´ìš© |\n",
    "|:---:|:---|:---|\n",
    "| **1** | [í™˜ê²½ ì„¤ì •](#1-í™˜ê²½-ì„¤ì •) | ê²½ë¡œ ì„¤ì • ë° ë°ì´í„° í™•ì¸ |\n",
    "| **2** | [ë°ì´í„°ì…‹ ìƒì„± ë¶„ì„](#2-ë°ì´í„°ì…‹-ìƒì„±-ë¶„ì„) | DQN Replay Bufferì™€ RTG ê³„ì‚° |\n",
    "| **3** | [CNN Encoder ì´í•´](#3-cnn-encoder-ì´í•´) | ì´ë¯¸ì§€ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì • |\n",
    "| **4** | [ëª¨ë¸ êµ¬ì¡° ë¶„ì„](#4-ëª¨ë¸-êµ¬ì¡°-ë¶„ì„) | Atariìš© GPT ëª¨ë¸ êµ¬ì¡° |\n",
    "| **5** | [Reward-Conditioned vs Naive](#5-reward-conditioned-vs-naive-ë¹„êµ) | ë‘ ëª¨ë“œì˜ ì°¨ì´ì  |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Phase ì—°ê²°\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    P1[\"Phase 1\\nRL & Transformer\\nê¸°ì´ˆ ê°œë…\"] --> P2[\"Phase 2\\nRTG & ì‹œí€€ìŠ¤\\nêµ¬ì„± ì›ë¦¬\"]\n",
    "    P2 --> P3[\"Phase 3\\nGym ì‹¤ìŠµ\\nì—°ì† ìƒíƒœ (11D ë²¡í„°)\"]\n",
    "    P3 --> P4[\"Phase 4\\nAtari ì‹¤ìŠµ\\nì´ë¯¸ì§€ ìƒíƒœ (84Ã—84Ã—4)\"]\n",
    "\n",
    "    style P1 fill:#e8eaf6,stroke:#3f51b5\n",
    "    style P2 fill:#e0f2f1,stroke:#009688\n",
    "    style P3 fill:#e8f5e9,stroke:#4caf50\n",
    "    style P4 fill:#fff3e0,stroke:#ff9800,stroke-width:3px\n",
    "```\n",
    "\n",
    "## ğŸ¯ Gym vs Atari ë¹„êµ\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Gym[\"ğŸ¤¸ Gym (Phase 3)\"]\n",
    "        direction TB\n",
    "        GS[\"State: 11ì°¨ì› ë²¡í„°\"]\n",
    "        GA[\"Action: ì—°ì†ê°’ (3ì°¨ì›)\"]\n",
    "        GL[\"Loss: MSE\"]\n",
    "        GE[\"Encoder: Linear\"]\n",
    "        GD[\"Dataset: D4RL\"]\n",
    "    end\n",
    "\n",
    "    subgraph Atari[\"ğŸ® Atari (Phase 4)\"]\n",
    "        direction TB\n",
    "        AS[\"State: 84Ã—84Ã—4 ì´ë¯¸ì§€\"]\n",
    "        AA[\"Action: ì´ì‚°ê°’ (4~18ê°œ)\"]\n",
    "        AL[\"Loss: Cross-Entropy\"]\n",
    "        AE[\"Encoder: CNN\"]\n",
    "        AD[\"Dataset: DQN Replay\"]\n",
    "    end\n",
    "\n",
    "    Gym -.->|\"ë¬´ì—‡ì´ ë‹¬ë¼ì§ˆê¹Œ?\"| Atari\n",
    "\n",
    "    style Gym fill:#e8f5e9,stroke:#4caf50\n",
    "    style Atari fill:#fff3e0,stroke:#ff9800\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¦ ì‚¬ì „ ìš”êµ¬ì‚¬í•­\n",
    "\n",
    "```python\n",
    "# í•„ìš”í•œ íŒ¨í‚¤ì§€\n",
    "- Python 3.7+\n",
    "- PyTorch 1.8+\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "\n",
    "# ì„ íƒì  (ì‹¤ì œ ë°ì´í„° ì‚¬ìš© ì‹œ)\n",
    "- gsutil (Google Cloud Storage ì ‘ê·¼ìš©)\n",
    "- Atari ROMs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“¦ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì •\n# ============================================================\n# Atari Decision Transformer ë¶„ì„ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ì…ë‹ˆë‹¤.\n#\n# ì£¼ìš” íŒ¨í‚¤ì§€:\n#   - torch: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ (CNN, Transformer)\n#   - numpy: ì´ë¯¸ì§€ ë°ì´í„° ì²˜ë¦¬\n#   - matplotlib: ì‹œê°í™”\n# ============================================================\n\nimport os\nimport sys\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nprint(\"ğŸ”§ í™˜ê²½ ì •ë³´\")\nprint(\"=\"*50)\nprint(f\"   Python: {sys.version.split()[0]}\")\nprint(f\"   PyTorch: {torch.__version__}\")\nprint(f\"   NumPy: {np.__version__}\")\nprint(f\"   CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"   CUDA device: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "## ğŸ¯ ëª©í‘œ\n",
    "> Atari Decision Transformer ì½”ë“œì— ì ‘ê·¼í•˜ê¸° ìœ„í•œ ê²½ë¡œë¥¼ ì„¤ì •í•˜ê³ , ë°ì´í„° êµ¬ì¡°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“‚ Atari í”„ë¡œì íŠ¸ êµ¬ì¡°\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Root[\"ğŸ“ decision-transformer/\"]\n",
    "        subgraph Atari[\"ğŸ® atari/ â† ì´ë²ˆ Phaseì—ì„œ ë¶„ì„\"]\n",
    "            subgraph MinGPT[\"mingpt/\"]\n",
    "                M1[\"model_atari.py\\ní•µì‹¬: GPT ëª¨ë¸ + CNN Encoder\"]\n",
    "                M2[\"trainer_atari.py\\ní•™ìŠµ ë£¨í”„\"]\n",
    "                M3[\"utils.py\"]\n",
    "            end\n",
    "\n",
    "            R1[\"run_dt_atari.py\\në©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸\"]\n",
    "            R2[\"create_dataset.py\\nDQN replay â†’ í•™ìŠµ ë°ì´í„° ë³€í™˜\"]\n",
    "            R3[\"fixed_replay_buffer.py\\nReplay buffer ë¡œë”©\"]\n",
    "\n",
    "            subgraph Data[\"dqn_replay/ (ë‹¤ìš´ë¡œë“œ í•„ìš”)\"]\n",
    "                D1[\"Breakout/\"]\n",
    "                D2[\"Pong/\"]\n",
    "                D3[\"...\"]\n",
    "            end\n",
    "        end\n",
    "\n",
    "        Doc[\"ğŸ““ doc/notebooks/ â† í˜„ì¬ ìœ„ì¹˜\"]\n",
    "    end\n",
    "\n",
    "    style Atari fill:#fff3e0,stroke:#e65100,stroke-width:2px\n",
    "    style MinGPT fill:#e3f2fd,stroke:#1565c0\n",
    "    style Data fill:#f3e5f5,stroke:#7b1fa2\n",
    "```\n",
    "\n",
    "## ğŸ® ì§€ì›ë˜ëŠ” Atari ê²Œì„\n",
    "\n",
    "| ê²Œì„ | Action ìˆ˜ | íŠ¹ì§• |\n",
    "|:---|:---:|:---|\n",
    "| Breakout | 4 | ë²½ëŒê¹¨ê¸°, ê°„ë‹¨í•œ ì•¡ì…˜ |\n",
    "| Pong | 6 | íƒêµ¬, 2ì¸ ëŒ€ì „ |\n",
    "| Qbert | 6 | í”Œë«í¼ ì í”„ |\n",
    "| Seaquest | 18 | ë³µì¡í•œ ì•¡ì…˜ ê³µê°„ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ“‚ í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •\n",
    "# ============================================================\n",
    "# Pythonì´ atari/ í´ë”ì˜ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ê²½ë¡œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "#\n",
    "# í™˜ê²½ ìë™ ê°ì§€:\n",
    "#   - Google Colab: git clone í›„ /content/decision-transformer ì‚¬ìš©\n",
    "#   - ë¡œì»¬ í™˜ê²½: ë…¸íŠ¸ë¶ ìœ„ì¹˜ì—ì„œ ìƒëŒ€ ê²½ë¡œë¡œ ìë™ íƒìƒ‰\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def find_project_root():\n",
    "    \"\"\"í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ìë™ìœ¼ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "    # 1) Colab í™˜ê²½ ê°ì§€\n",
    "    if 'google.colab' in sys.modules:\n",
    "        colab_path = '/content/decision-transformer'\n",
    "        if not os.path.exists(colab_path):\n",
    "            print(\"ğŸ“¥ Colab í™˜ê²½ ê°ì§€! ë ˆí¬ë¥¼ í´ë¡ í•©ë‹ˆë‹¤...\")\n",
    "            os.system('git clone https://github.com/ggeutzzang/decision-transformer.git /content/decision-transformer')\n",
    "        return colab_path\n",
    "\n",
    "    # 2) ë¡œì»¬ í™˜ê²½: í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ìƒìœ„ë¡œ ì˜¬ë¼ê°€ë©° íƒìƒ‰\n",
    "    #    'gym/' ê³¼ 'atari/' í´ë”ê°€ ëª¨ë‘ ìˆëŠ” ë””ë ‰í† ë¦¬ë¥¼ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ íŒë‹¨\n",
    "    search_dir = os.path.abspath(os.getcwd())\n",
    "    for _ in range(5):  # ìµœëŒ€ 5ë‹¨ê³„ ìƒìœ„ê¹Œì§€ íƒìƒ‰\n",
    "        if os.path.isdir(os.path.join(search_dir, 'gym')) and \\\n",
    "           os.path.isdir(os.path.join(search_dir, 'atari')):\n",
    "            return search_dir\n",
    "        search_dir = os.path.dirname(search_dir)\n",
    "\n",
    "    # 3) ëª» ì°¾ìœ¼ë©´ ê¸°ë³¸ê°’\n",
    "    return os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "ATARI_PATH = os.path.join(PROJECT_ROOT, 'atari')\n",
    "\n",
    "if ATARI_PATH not in sys.path:\n",
    "    sys.path.insert(0, ATARI_PATH)\n",
    "\n",
    "print(\"ğŸ“‚ ê²½ë¡œ ì„¤ì • ê²°ê³¼\")\n",
    "print(\"=\"*50)\n",
    "print(f\"   Project root: {PROJECT_ROOT}\")\n",
    "print(f\"   Atari path:   {ATARI_PATH}\")\n",
    "print()\n",
    "\n",
    "# ê²½ë¡œ ê²€ì¦\n",
    "if os.path.exists(ATARI_PATH):\n",
    "    print(\"âœ… Atari ê²½ë¡œ ì¡´ì¬ í™•ì¸!\")\n",
    "    mingpt_path = os.path.join(ATARI_PATH, 'mingpt')\n",
    "    if os.path.exists(mingpt_path):\n",
    "        print(\"âœ… mingpt ëª¨ë“ˆ ì¡´ì¬ í™•ì¸!\")\n",
    "        # ì£¼ìš” íŒŒì¼ í™•ì¸\n",
    "        key_files = ['model_atari.py', 'trainer_atari.py']\n",
    "        for f in key_files:\n",
    "            fpath = os.path.join(mingpt_path, f)\n",
    "            status = \"âœ…\" if os.path.exists(fpath) else \"âŒ\"\n",
    "            print(f\"   {status} {f}\")\n",
    "else:\n",
    "    print(\"âŒ Atari ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!\")\n",
    "    print(\"   ë…¸íŠ¸ë¶ ìœ„ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ® ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ í™•ì¸\n# ============================================================\n# DQN replay ë°ì´í„°ì…‹ì´ ì €ì¥ë˜ëŠ” ìœ„ì¹˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n#\n# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë°©ë²•:\n#   gsutil -m cp -R gs://atari-replay-datasets/dqn/Breakout dqn_replay/\n#\n# ê° ê²Œì„ í´ë”ì—ëŠ” 50ê°œì˜ checkpointê°€ ìˆìŠµë‹ˆë‹¤ (1/, 2/, ..., 50/)\n# ============================================================\n\ndata_dir = os.path.join(ATARI_PATH, 'dqn_replay')\n\nprint(\"ğŸ“‚ ë°ì´í„°ì…‹ í™•ì¸\")\nprint(\"=\"*50)\n\nif os.path.exists(data_dir):\n    games = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n    \n    if games:\n        print(f\"\\nâœ… ì‚¬ìš© ê°€ëŠ¥í•œ ê²Œì„ ({len(games)}ê°œ):\\n\")\n        for game in sorted(games):\n            game_path = os.path.join(data_dir, game)\n            checkpoints = [d for d in os.listdir(game_path) if d.isdigit()]\n            print(f\"   ğŸ® {game}\")\n            print(f\"      â””â”€ {len(checkpoints)}ê°œ checkpoint\")\n    else:\n        print(\"\\nâš ï¸ ê²Œì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\nelse:\n    print(f\"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {data_dir}\")\n\nprint(\"\\n\" + \"-\"*50)\nprint(\"ğŸ“¥ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë°©ë²•:\")\nprint(\"   mkdir -p dqn_replay\")\nprint(\"   gsutil -m cp -R gs://atari-replay-datasets/dqn/Breakout dqn_replay/\")\nprint(\"\\nğŸ’¡ gsutilì´ ì—†ë‹¤ë©´ Google Cloud SDKë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. ë°ì´í„°ì…‹ ìƒì„± ë¶„ì„\n",
    "\n",
    "## ğŸ¯ ëª©í‘œ\n",
    "> DQN Replay Bufferì˜ êµ¬ì¡°ì™€ Atari ìŠ¤íƒ€ì¼ RTG ê³„ì‚° ë°©ì‹ì„ ì´í•´í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“š DQN Replay Bufferë€?\n",
    "\n",
    "**DQN (Deep Q-Network)**ì´ í•™ìŠµí•˜ë©´ì„œ ì €ì¥í•œ ê²½í—˜ ë°ì´í„°ì…ë‹ˆë‹¤.\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph DQNLearning[\"ğŸ¤– DQN í•™ìŠµ ê³¼ì •\"]\n",
    "        direction TB\n",
    "        CP1[\"checkpoint 1/\\n100,000 transitions\\nğŸŸ¡ ì´ˆë³´ ì •ì±…\"]\n",
    "        CP2[\"checkpoint 2/\\n100,000 transitions\"]\n",
    "        CP3[\"checkpoint 3/\\n100,000 transitions\"]\n",
    "        DOTS[\"...\"]\n",
    "        CP50[\"checkpoint 50/\\n100,000 transitions\\nğŸŸ¢ ì „ë¬¸ê°€ ì •ì±…\"]\n",
    "\n",
    "        CP1 --> CP2 --> CP3 --> DOTS --> CP50\n",
    "    end\n",
    "\n",
    "    subgraph Transition[\"ğŸ“ ê° Transition êµ¬ì¡°\"]\n",
    "        direction LR\n",
    "        OBS[\"observation\\nê²Œì„ í™”ë©´\\n(4Ã—84Ã—84)\"]\n",
    "        ACT[\"action\\nìˆ˜í–‰í•œ í–‰ë™\\n(0~17)\"]\n",
    "        REW[\"reward\\nì¦‰ì‹œ ë³´ìƒ\"]\n",
    "        TERM[\"terminal\\nì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€\"]\n",
    "    end\n",
    "\n",
    "    DQNLearning --> Transition\n",
    "\n",
    "    style DQNLearning fill:#e3f2fd,stroke:#1565c0\n",
    "    style Transition fill:#fff3e0,stroke:#e65100\n",
    "    style CP1 fill:#fff9c4,stroke:#f9a825\n",
    "    style CP50 fill:#c8e6c9,stroke:#2e7d32\n",
    "```\n",
    "\n",
    "## ğŸ’¡ ì™œ 50ê°œ ë²„í¼ë¥¼ ì‚¬ìš©í• ê¹Œ?\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Quality[\"ğŸ“Š ë°ì´í„° í’ˆì§ˆ ë¶„í¬\"]\n",
    "        direction TB\n",
    "        Low[\"ğŸŸ¡ ë²„í¼ 1~10\\në‚®ìŒ (ì´ˆë³´)\\nëœë¤ì— ê°€ê¹Œìš´ í–‰ë™\"]\n",
    "        Mid[\"ğŸŸ  ë²„í¼ 11~30\\nì¤‘ê°„\\ní•™ìŠµ ì¤‘ì¸ ì •ì±…\"]\n",
    "        High[\"ğŸŸ¢ ë²„í¼ 31~50\\në†’ìŒ (ì „ë¬¸ê°€)\\nìµœì í™”ëœ ì •ì±…\"]\n",
    "\n",
    "        Low --> Mid --> High\n",
    "    end\n",
    "\n",
    "    subgraph Benefit[\"âœ… ì¥ì \"]\n",
    "        B1[\"ë‹¤ì–‘í•œ í’ˆì§ˆì˜ ë°ì´í„°ë¡œ\\nDTê°€ ë” ì˜ í•™ìŠµ\"]\n",
    "        B2[\"RTG ì¡°ê±´ì— ë”°ë¼\\nì´ˆë³´~ì „ë¬¸ê°€ ìˆ˜ì¤€\\ní–‰ë™ì„ ëª¨ë‘ í•™ìŠµ\"]\n",
    "    end\n",
    "\n",
    "    Quality --> Benefit\n",
    "\n",
    "    style Low fill:#fff9c4,stroke:#f9a825\n",
    "    style Mid fill:#ffe0b2,stroke:#ef6c00\n",
    "    style High fill:#c8e6c9,stroke:#2e7d32\n",
    "    style Benefit fill:#e8f5e9,stroke:#4caf50\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 DQN Replay Buffer êµ¬ì¡° ìƒì„¸\n",
    "\n",
    "### ğŸ“¦ Observationì˜ êµ¬ì¡°\n",
    "\n",
    "Atari ê²Œì„ì˜ observationì€ **4ê°œ í”„ë ˆì„ì„ ìŒ“ì€(stack)** ì´ë¯¸ì§€ì…ë‹ˆë‹¤.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Why[\"â“ ì™œ 4ê°œ í”„ë ˆì„ì„ ìŒ“ì„ê¹Œ?\"]\n",
    "        direction TB\n",
    "        Problem[\"ë‹¨ì¼ í”„ë ˆì„ë§Œìœ¼ë¡œëŠ”\\në¬¼ì²´ì˜ 'ì›€ì§ì„ ë°©í–¥'ì„\\nì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"]\n",
    "    end\n",
    "\n",
    "    subgraph Frames[\"ğŸ® Pong ê²Œì„ ì˜ˆì‹œ\"]\n",
    "        F1[\"Frame 1\\nğŸŸ¡ â—‹ \"]\n",
    "        F2[\"Frame 2\\nğŸŸ¡  â—‹\"]\n",
    "        F3[\"Frame 3\\nğŸŸ¡   â—‹\"]\n",
    "        F4[\"Frame 4\\nğŸŸ¡    â—‹\"]\n",
    "\n",
    "        F1 -->|\"ì‹œê°„ â†’\"| F2 -->|\"ì‹œê°„ â†’\"| F3 -->|\"ì‹œê°„ â†’\"| F4\n",
    "    end\n",
    "\n",
    "    subgraph Result[\"âœ… ê²°ê³¼\"]\n",
    "        direction TB\n",
    "        R1[\"4ê°œë¥¼ ìŒ“ìœ¼ë©´:\\n'ê³µì´ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™ ì¤‘'\\nì„ ì•Œ ìˆ˜ ìˆìŒ!\"]\n",
    "        R2[\"ìµœì¢… Shape:\\n(4, 84, 84)\\n4ì±„ë„ Ã— 84Ã—84 ê·¸ë ˆì´ìŠ¤ì¼€ì¼\"]\n",
    "    end\n",
    "\n",
    "    Why --> Frames --> Result\n",
    "\n",
    "    style Why fill:#ffebee,stroke:#c62828\n",
    "    style Frames fill:#e3f2fd,stroke:#1565c0\n",
    "    style Result fill:#e8f5e9,stroke:#2e7d32\n",
    "```\n",
    "\n",
    "### ğŸ“Š ë°ì´í„° í¬ê¸°\n",
    "\n",
    "| í•­ëª© | ê°’ | ì„¤ëª… |\n",
    "|:---|:---|:---|\n",
    "| ë‹¨ì¼ observation | (4, 84, 84) | 28,224 í”½ì…€ |\n",
    "| ë°ì´í„° íƒ€ì… | uint8 | 0~255 ì •ìˆ˜ |\n",
    "| ë©”ëª¨ë¦¬ (1ê°œ) | ~27KB | ì••ì¶• ì—†ì´ |\n",
    "| 1 ë²„í¼ (100K) | ~2.7GB | 100,000ê°œ |\n",
    "| 50 ë²„í¼ ì „ì²´ | ~135GB | ëŒ€ìš©ëŸ‰! |\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Memory[\"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\"]\n",
    "        direction LR\n",
    "        O1[\"1 Observation\\n~27KB\"] -->|\"Ã—100K\"| B1[\"1 Buffer\\n~2.7GB\"] -->|\"Ã—50\"| T1[\"ì „ì²´\\n~135GB\"]\n",
    "    end\n",
    "\n",
    "    style O1 fill:#e8f5e9,stroke:#4caf50\n",
    "    style B1 fill:#fff3e0,stroke:#ef6c00\n",
    "    style T1 fill:#ffebee,stroke:#c62828\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ® ë”ë¯¸ ê¶¤ì  ë°ì´í„° ìƒì„±\n# ============================================================\n# ì‹¤ì œ DQN replayê°€ ì—†ì–´ë„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n# êµ¬ì¡°ëŠ” ì‹¤ì œ ë°ì´í„°ì™€ ë™ì¼í•©ë‹ˆë‹¤.\n# ============================================================\n\ndef create_dummy_trajectory(length=100):\n    \"\"\"\n    Atari ìŠ¤íƒ€ì¼ì˜ ë”ë¯¸ ê¶¤ì  ìƒì„±\n    \n    Returns:\n        dict with keys:\n        - observations: (length, 4, 84, 84) - 4 stacked frames\n        - actions: (length,) - discrete actions\n        - rewards: (length,) - sparse rewards (0 or positive)\n        - terminals: (length,) - episode end flags\n    \"\"\"\n    return {\n        # 84x84 ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€ 4ê°œ ìŠ¤íƒ\n        'observations': np.random.randint(0, 256, (length, 4, 84, 84), dtype=np.uint8),\n        \n        # ì´ì‚° í–‰ë™ (Breakout: 0~3)\n        'actions': np.random.randint(0, 4, length),\n        \n        # Atari rewardëŠ” í¬ì†Œ(sparse): ëŒ€ë¶€ë¶„ 0, ê°€ë” ì–‘ìˆ˜\n        'rewards': np.random.choice([0, 0, 0, 1, 5, 10], length).astype(np.float32),\n        \n        # ì—í”¼ì†Œë“œ ì¢…ë£Œ í”Œë˜ê·¸\n        'terminals': np.zeros(length, dtype=bool)\n    }\n\n# ì—¬ëŸ¬ ê¶¤ì  ìƒì„± (ì—¬ëŸ¬ ì—í”¼ì†Œë“œ)\nprint(\"ğŸ® ë”ë¯¸ ê¶¤ì  ë°ì´í„° ìƒì„±\")\nprint(\"=\"*60)\n\ntrajectories = []\nfor i in range(10):\n    # ê° ì—í”¼ì†Œë“œëŠ” 50~200 ìŠ¤í…\n    traj = create_dummy_trajectory(np.random.randint(50, 200))\n    traj['terminals'][-1] = True  # ë§ˆì§€ë§‰ ìŠ¤í…ì€ terminal\n    trajectories.append(traj)\n\nprint(f\"ìƒì„±ëœ ì—í”¼ì†Œë“œ ìˆ˜: {len(trajectories)}\")\n\n# ì²« ë²ˆì§¸ ê¶¤ì  ë¶„ì„\ntraj = trajectories[0]\nprint(f\"\\nğŸ“Š ì²« ë²ˆì§¸ ì—í”¼ì†Œë“œ ë¶„ì„:\")\nprint(f\"   â”Œâ”€ observations: {traj['observations'].shape}\")\nprint(f\"   â”‚                â””â”€ (ì—í”¼ì†Œë“œ ê¸¸ì´, í”„ë ˆì„ ìˆ˜, ë†’ì´, ë„ˆë¹„)\")\nprint(f\"   â”œâ”€ actions:      {traj['actions'].shape}\")\nprint(f\"   â”‚                â””â”€ ì´ì‚° action (0~3)\")\nprint(f\"   â”œâ”€ rewards:      {traj['rewards'].shape}\")\nprint(f\"   â”‚                â””â”€ í¬ì†Œ reward\")\nprint(f\"   â””â”€ terminals:    {traj['terminals'].shape}\")\n\n# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\nmemory_mb = traj['observations'].nbytes / (1024 * 1024)\nprint(f\"\\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_mb:.1f} MB (í•œ ì—í”¼ì†Œë“œ)\")\n\n# Reward ë¶„í¬\nunique, counts = np.unique(traj['rewards'], return_counts=True)\nprint(f\"\\nğŸ“ˆ Reward ë¶„í¬:\")\nfor u, c in zip(unique, counts):\n    pct = c / len(traj['rewards']) * 100\n    print(f\"   reward={u:>2}: {c:>3}íšŒ ({pct:>5.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 RTG ê³„ì‚° (Atari ìŠ¤íƒ€ì¼)\n",
    "\n",
    "> Atariì—ì„œ RTG ê³„ì‚° ì‹œ ì¤‘ìš”í•œ ì : **ì—í”¼ì†Œë“œ ê²½ê³„ë¥¼ ë„˜ì§€ ì•ŠëŠ”ë‹¤!**\n",
    "\n",
    "### âš ï¸ ì—í”¼ì†Œë“œ ê²½ê³„ ì²˜ë¦¬\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Episode[\"ğŸ“Š ì—í”¼ì†Œë“œ êµ¬ì¡°\"]\n",
    "        direction LR\n",
    "        subgraph Ep1[\"ì—í”¼ì†Œë“œ 1\"]\n",
    "            R0[\"r0=1\"] --> R1[\"r1=2\"] --> R2[\"r2=3\"] --> R3[\"r3=4\"]\n",
    "        end\n",
    "        subgraph Ep2[\"ì—í”¼ì†Œë“œ 2\"]\n",
    "            R4[\"r4=5\"] --> R5[\"r5=10\"] --> R6[\"r6=15\"]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    subgraph Wrong[\"âŒ ì˜ëª»ëœ ê³„ì‚° (ê²½ê³„ ë¬´ì‹œ)\"]\n",
    "        W1[\"RTG[3] = r3 + r4 + r5 + r6\\n= 4 + 5 + 10 + 15 = 34\\nì—í”¼ì†Œë“œ ê²½ê³„ë¥¼ ë„˜ì–´ ê³„ì‚°!\"]\n",
    "    end\n",
    "\n",
    "    subgraph Correct[\"âœ… ì˜¬ë°”ë¥¸ ê³„ì‚° (ê²½ê³„ ì¡´ì¤‘)\"]\n",
    "        C1[\"RTG[3] = r3 = 4\\nì—í”¼ì†Œë“œ 1ì˜ ë§ˆì§€ë§‰\"]\n",
    "        C2[\"RTG[4] = r4 + r5 + r6 = 30\\nì—í”¼ì†Œë“œ 2ì˜ ì²˜ìŒë¶€í„° ê³„ì‚°\"]\n",
    "    end\n",
    "\n",
    "    Episode --> Wrong\n",
    "    Episode --> Correct\n",
    "\n",
    "    style Ep1 fill:#e3f2fd,stroke:#1565c0\n",
    "    style Ep2 fill:#fff3e0,stroke:#ef6c00\n",
    "    style Wrong fill:#ffebee,stroke:#c62828\n",
    "    style Correct fill:#e8f5e9,stroke:#2e7d32\n",
    "```\n",
    "\n",
    "### ğŸ’¡ ì™œ ê²½ê³„ë¥¼ ì¡´ì¤‘í•´ì•¼ í• ê¹Œ?\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    Death[\"ğŸ’€ ì—í”¼ì†Œë“œ 1ì—ì„œ\\nê²Œì„ ì˜¤ë²„\"] --> NoReward[\"ì—í”¼ì†Œë“œ 2ì˜\\në³´ìƒì„ ë°›ì„ ìˆ˜ ì—†ìŒ\"]\n",
    "    NoReward --> Rule[\"RTGëŠ”\\n'ì•ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆëŠ” ë³´ìƒ'\\nì„ ì˜ë¯¸\"]\n",
    "    Rule --> Conclusion[\"ì—í”¼ì†Œë“œ ë‚´ì—ì„œë§Œ\\nRTG ê³„ì‚°!\"]\n",
    "\n",
    "    style Death fill:#ffebee,stroke:#c62828\n",
    "    style NoReward fill:#fff3e0,stroke:#ef6c00\n",
    "    style Rule fill:#e3f2fd,stroke:#1565c0\n",
    "    style Conclusion fill:#e8f5e9,stroke:#2e7d32\n",
    "```\n",
    "\n",
    "### ğŸ“ˆ RTG ê³„ì‚° ì•Œê³ ë¦¬ì¦˜\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Algo[\"ğŸ”„ create_dataset.pyì˜ RTG ê³„ì‚° ì•Œê³ ë¦¬ì¦˜\"]\n",
    "        direction TB\n",
    "        S1[\"1. done_idxsì—ì„œ\\nì—í”¼ì†Œë“œ ê²½ê³„ ì‹ë³„\"]\n",
    "        S2[\"2. ê° ì—í”¼ì†Œë“œ êµ¬ê°„\\n(start_idx ~ done_idx) ì¶”ì¶œ\"]\n",
    "        S3[\"3. êµ¬ê°„ ë‚´ì—ì„œ\\në’¤ì—ì„œë¶€í„° ëˆ„ì í•© ê³„ì‚°\\n(discount cumsum)\"]\n",
    "        S4[\"4. ê²°ê³¼ë¥¼ ì „ì²´ rtg ë°°ì—´ì— ì €ì¥\"]\n",
    "\n",
    "        S1 --> S2 --> S3 --> S4\n",
    "    end\n",
    "\n",
    "    subgraph Example[\"ğŸ“ ì˜ˆì‹œ: rewards = [1, 2, 3 | 5, 10]\"]\n",
    "        direction TB\n",
    "        E1[\"ì—í”¼ì†Œë“œ 1: [1, 2, 3]\"]\n",
    "        E2[\"RTG 1: [6, 5, 3]\\n(ë’¤ì—ì„œë¶€í„° ëˆ„ì )\"]\n",
    "        E3[\"ì—í”¼ì†Œë“œ 2: [5, 10]\"]\n",
    "        E4[\"RTG 2: [15, 10]\"]\n",
    "\n",
    "        E1 --> E2\n",
    "        E3 --> E4\n",
    "    end\n",
    "\n",
    "    Algo --> Example\n",
    "\n",
    "    style Algo fill:#e3f2fd,stroke:#1565c0\n",
    "    style Example fill:#e8f5e9,stroke:#2e7d32\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“ˆ Atari ìŠ¤íƒ€ì¼ RTG ê³„ì‚° í•¨ìˆ˜\n# ============================================================\n# create_dataset.pyì—ì„œ ì‚¬ìš©í•˜ëŠ” RTG ê³„ì‚° ë°©ì‹ì…ë‹ˆë‹¤.\n# í•µì‹¬: ì—í”¼ì†Œë“œ ê²½ê³„(done_idxs)ë¥¼ ì¡´ì¤‘í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.\n# ============================================================\n\ndef calculate_rtg_atari_style(stepwise_returns, done_idxs):\n    \"\"\"\n    Atari ìŠ¤íƒ€ì¼ RTG ê³„ì‚°\n    \n    Parameters:\n    -----------\n    stepwise_returns : np.ndarray\n        ëª¨ë“  ìŠ¤í…ì˜ reward (ì—¬ëŸ¬ ì—í”¼ì†Œë“œê°€ ì—°ê²°ëœ í˜•íƒœ)\n    done_idxs : list\n        ê° ì—í”¼ì†Œë“œê°€ ëë‚˜ëŠ” ì¸ë±ìŠ¤ (exclusive)\n        ì˜ˆ: [4, 7] â†’ ì—í”¼ì†Œë“œ1ì€ ì¸ë±ìŠ¤ 0~3, ì—í”¼ì†Œë“œ2ëŠ” 4~6\n        \n    Returns:\n    --------\n    np.ndarray\n        ì—í”¼ì†Œë“œ ê²½ê³„ë¥¼ ì¡´ì¤‘í•˜ëŠ” RTG\n    \"\"\"\n    rtg = np.zeros_like(stepwise_returns)\n    start_index = 0\n    \n    for done_idx in done_idxs:\n        done_idx = int(done_idx)\n        \n        # í˜„ì¬ ì—í”¼ì†Œë“œì˜ rewardë§Œ ì¶”ì¶œ\n        curr_traj_returns = stepwise_returns[start_index:done_idx]\n        \n        # ì—­ë°©í–¥ìœ¼ë¡œ RTG ê³„ì‚° (ëì—ì„œ ì‹œì‘ìœ¼ë¡œ)\n        for j in range(done_idx - 1, start_index - 1, -1):\n            # jë¶€í„° ì—í”¼ì†Œë“œ ëê¹Œì§€ì˜ reward í•©\n            rtg_j = curr_traj_returns[j - start_index:done_idx - start_index]\n            rtg[j] = sum(rtg_j)\n        \n        # ë‹¤ìŒ ì—í”¼ì†Œë“œë¡œ ì´ë™\n        start_index = done_idx\n    \n    return rtg\n\n# ============================================================\n# ğŸ§ª RTG ê³„ì‚° í…ŒìŠ¤íŠ¸\n# ============================================================\nprint(\"ğŸ“ˆ Atari ìŠ¤íƒ€ì¼ RTG ê³„ì‚° ì˜ˆì‹œ\")\nprint(\"=\"*60)\n\n# ë‘ ì—í”¼ì†Œë“œ ìƒì„±\nrewards_ep1 = [1, 2, 3, 4]    # ì—í”¼ì†Œë“œ 1: ì´ reward = 10\nrewards_ep2 = [5, 10, 15]     # ì—í”¼ì†Œë“œ 2: ì´ reward = 30\n\n# ì—°ê²°\nall_rewards = np.array(rewards_ep1 + rewards_ep2, dtype=np.float32)\n\n# ì—í”¼ì†Œë“œ ê²½ê³„ ì¸ë±ìŠ¤ (exclusive)\ndone_idxs = [len(rewards_ep1), len(rewards_ep1) + len(rewards_ep2)]  # [4, 7]\n\n# RTG ê³„ì‚°\nrtg = calculate_rtg_atari_style(all_rewards, done_idxs)\n\nprint(f\"\\nì…ë ¥ ë°ì´í„°:\")\nprint(f\"   rewards:    {all_rewards}\")\nprint(f\"   done_idxs:  {done_idxs}\")\n\nprint(f\"\\nê³„ì‚°ëœ RTG:\")\nprint(f\"   rtg:        {rtg}\")\n\nprint(f\"\\nğŸ“Š ì—í”¼ì†Œë“œë³„ ë¶„ì„:\")\nprint(f\"   â”Œâ”€ ì—í”¼ì†Œë“œ 1 (idx 0~3):\")\nprint(f\"   â”‚    rewards: {rewards_ep1}\")\nprint(f\"   â”‚    RTG:     {rtg[:4].tolist()}\")\nprint(f\"   â”‚    â””â”€ idx 0ì˜ RTG = 1+2+3+4 = {rtg[0]}\")\nprint(f\"   â”‚\")\nprint(f\"   â””â”€ ì—í”¼ì†Œë“œ 2 (idx 4~6):\")\nprint(f\"        rewards: {rewards_ep2}\")\nprint(f\"        RTG:     {rtg[4:].tolist()}\")\nprint(f\"        â””â”€ idx 4ì˜ RTG = 5+10+15 = {rtg[4]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“Š RTG ê³„ì‚° ì‹œê°í™”\n# ============================================================\n# ì—í”¼ì†Œë“œ ê²½ê³„ì—ì„œ RTGê°€ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ì‹œê°í™”í•©ë‹ˆë‹¤.\n# ============================================================\n\nprint(\"ğŸ“Š RTG ì‹œê°í™”\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# ìƒ‰ìƒ ì§€ì •: ì—í”¼ì†Œë“œë³„ë¡œ ë‹¤ë¥¸ ìƒ‰\ncolors = ['steelblue'] * 4 + ['coral'] * 3\n\n# 1. Reward ì‹œê°í™”\nax1 = axes[0]\nbars1 = ax1.bar(range(len(all_rewards)), all_rewards, color=colors, alpha=0.7, edgecolor='black')\nax1.axvline(x=3.5, color='black', linestyle='--', linewidth=2, label='Episode boundary')\nax1.set_xlabel('Step Index', fontsize=12)\nax1.set_ylabel('Reward', fontsize=12)\nax1.set_title('Step-wise Reward (ì—í”¼ì†Œë“œë³„ ìƒ‰ìƒ êµ¬ë¶„)', fontsize=14)\nax1.set_xticks(range(len(all_rewards)))\nax1.legend()\nax1.grid(True, alpha=0.3, axis='y')\n\n# ê°’ í‘œì‹œ\nfor i, (r, bar) in enumerate(zip(all_rewards, bars1)):\n    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, \n             f'{int(r)}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n\n# 2. RTG ì‹œê°í™”\nax2 = axes[1]\nbars2 = ax2.bar(range(len(rtg)), rtg, color=colors, alpha=0.7, edgecolor='black')\nax2.axvline(x=3.5, color='black', linestyle='--', linewidth=2, label='Episode boundary')\nax2.set_xlabel('Step Index', fontsize=12)\nax2.set_ylabel('Return-to-Go', fontsize=12)\nax2.set_title('RTG (ì—í”¼ì†Œë“œ ë‚´ì—ì„œë§Œ ê³„ì‚°)', fontsize=14)\nax2.set_xticks(range(len(rtg)))\nax2.legend()\nax2.grid(True, alpha=0.3, axis='y')\n\n# ê°’ í‘œì‹œ\nfor i, (r, bar) in enumerate(zip(rtg, bars2)):\n    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n             f'{int(r)}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# í•µì‹¬ í¬ì¸íŠ¸\nprint(\"\\nğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:\")\nprint(\"   â€¢ ì—í”¼ì†Œë“œ 1ì˜ RTGëŠ” ì—í”¼ì†Œë“œ 1 ë‚´ì—ì„œë§Œ ê³„ì‚°ë¨\")\nprint(\"   â€¢ ì—í”¼ì†Œë“œ 2ì˜ RTGëŠ” ì—í”¼ì†Œë“œ 2 ë‚´ì—ì„œë§Œ ê³„ì‚°ë¨\")\nprint(\"   â€¢ ê²½ê³„(ì¸ë±ìŠ¤ 3â†’4)ì—ì„œ RTGê°€ ê¸‰ê²©íˆ ë³€í™”\")\nprint(\"   â€¢ ì´ëŠ” 'ì£½ìœ¼ë©´ ë‹¤ìŒ ì—í”¼ì†Œë“œ ë³´ìƒì„ ë°›ì„ ìˆ˜ ì—†ë‹¤'ëŠ” ì˜ë¯¸ ë°˜ì˜\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 StateActionReturnDataset\n",
    "\n",
    "> PyTorch Datasetìœ¼ë¡œ ë°°ì¹˜ í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ë¡œë”ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ“¦ Dataset ë™ì‘ ë°©ì‹\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Data[\"ğŸ“‚ ì „ì²´ ë°ì´í„°\"]\n",
    "        direction LR\n",
    "        EP1[\"ep1 data\"]\n",
    "        EP2[\"ep2 data\"]\n",
    "        EP3[\"ep3 data\"]\n",
    "        DOTS[\"...\"]\n",
    "\n",
    "        EP1 --- EP2 --- EP3 --- DOTS\n",
    "    end\n",
    "\n",
    "    subgraph GetItem[\"ğŸ“‹ __getitem__(idx) ê³¼ì •\"]\n",
    "        direction TB\n",
    "        S1[\"1ï¸âƒ£ idx ìœ„ì¹˜ í™•ì¸\"]\n",
    "        S2[\"2ï¸âƒ£ ì—í”¼ì†Œë“œ ê²½ê³„ ì²´í¬\\n(done_idx ì°¾ê¸°)\"]\n",
    "        S3[\"3ï¸âƒ£ context_lengthë§Œí¼ ì¶”ì¶œ\\n(ê²½ê³„ ë„˜ì§€ ì•Šê²Œ)\"]\n",
    "        S4[\"4ï¸âƒ£ ì´ë¯¸ì§€ ì •ê·œí™”\\n(0~255 â†’ 0.0~1.0)\"]\n",
    "        S5[\"5ï¸âƒ£ íŠœí”Œ ë°˜í™˜\"]\n",
    "\n",
    "        S1 --> S2 --> S3 --> S4 --> S5\n",
    "    end\n",
    "\n",
    "    subgraph Output[\"ğŸ“¤ ë°˜í™˜ í˜•íƒœ\"]\n",
    "        direction TB\n",
    "        O1[\"states: (context_len, 4Ã—84Ã—84)\\ní‰íƒ„í™”ëœ ì´ë¯¸ì§€\"]\n",
    "        O2[\"actions: (context_len, 1)\\nì´ì‚° ì•¡ì…˜\"]\n",
    "        O3[\"rtgs: (context_len, 1)\\nRTG\"]\n",
    "        O4[\"timesteps: (1, 1)\\nì‹œì‘ ì‹œê°„\"]\n",
    "    end\n",
    "\n",
    "    Data --> GetItem --> Output\n",
    "\n",
    "    style Data fill:#e3f2fd,stroke:#1565c0\n",
    "    style GetItem fill:#fff3e0,stroke:#ef6c00\n",
    "    style Output fill:#e8f5e9,stroke:#2e7d32\n",
    "```\n",
    "\n",
    "### ğŸ”‘ ì—í”¼ì†Œë“œ ê²½ê³„ ì²˜ë¦¬ì˜ í•µì‹¬\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph SafeExtract[\"ì•ˆì „í•œ ì‹œí€€ìŠ¤ ì¶”ì¶œ\"]\n",
    "        direction TB\n",
    "        IDX[\"ìš”ì²­ëœ idx ìœ„ì¹˜\"] --> FindDone[\"ê°€ì¥ ê°€ê¹Œìš´\\ndone_idx ì°¾ê¸°\"]\n",
    "        FindDone --> Clamp[\"ì¶”ì¶œ ë²”ìœ„ë¥¼\\nì—í”¼ì†Œë“œ ë‚´ë¡œ ì œí•œ\"]\n",
    "        Clamp --> Pad[\"ë¶€ì¡±í•œ ë¶€ë¶„ì€\\n0ìœ¼ë¡œ íŒ¨ë”©\"]\n",
    "    end\n",
    "\n",
    "    style SafeExtract fill:#f3e5f5,stroke:#7b1fa2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“¦ StateActionReturnDataset êµ¬í˜„\n# ============================================================\n# run_dt_atari.pyì—ì„œ ì‚¬ìš©í•˜ëŠ” Dataset í´ë˜ìŠ¤ì…ë‹ˆë‹¤.\n# ì—í”¼ì†Œë“œ ê²½ê³„ë¥¼ ì¡´ì¤‘í•˜ë©´ì„œ context_length í¬ê¸°ì˜ ì‹œí€€ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n# ============================================================\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass StateActionReturnDataset(Dataset):\n    \"\"\"\n    Atariìš© Dataset (run_dt_atari.py ì°¸ì¡°)\n    \n    Parameters:\n    -----------\n    data : list\n        observation ë¦¬ìŠ¤íŠ¸ (ê° ìš”ì†ŒëŠ” (4, 84, 84) ì´ë¯¸ì§€)\n    block_size : int\n        context_length * 3 (RTG, State, Action ì„¸ ì¢…ë¥˜)\n    actions : np.ndarray\n        action ë°°ì—´\n    done_idxs : list\n        ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¸ë±ìŠ¤\n    rtgs : np.ndarray\n        RTG ë°°ì—´\n    timesteps : np.ndarray\n        ê° ìŠ¤í…ì˜ ì—í”¼ì†Œë“œ ë‚´ ì‹œê°„\n    \"\"\"\n    \n    def __init__(self, data, block_size, actions, done_idxs, rtgs, timesteps):\n        self.block_size = block_size  # context_length * 3\n        self.vocab_size = int(max(actions)) + 1  # action ìˆ˜\n        self.data = data\n        self.actions = actions\n        self.done_idxs = done_idxs\n        self.rtgs = rtgs\n        self.timesteps = timesteps\n        \n    def __len__(self):\n        # context_lengthë¥¼ í™•ë³´í•  ìˆ˜ ìˆëŠ” ìƒ˜í”Œ ìˆ˜\n        return len(self.data) - self.block_size // 3\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        idx ìœ„ì¹˜ì—ì„œ context_length í¬ê¸°ì˜ ì‹œí€€ìŠ¤ ë°˜í™˜\n        \"\"\"\n        block_size = self.block_size // 3  # ì‹¤ì œ íƒ€ì„ìŠ¤í… ìˆ˜\n        done_idx = idx + block_size\n        \n        # ì—í”¼ì†Œë“œ ê²½ê³„ í™•ì¸: í˜„ì¬ idxë³´ë‹¤ í° ì²« ë²ˆì§¸ done_idx ì°¾ê¸°\n        for i in self.done_idxs:\n            if i > idx:\n                # ì—í”¼ì†Œë“œ ëì„ ë„˜ì§€ ì•Šë„ë¡ ì¡°ì •\n                done_idx = min(int(i), done_idx)\n                break\n        \n        # ì‹œì‘ ì¸ë±ìŠ¤ ì¡°ì • (block_size í™•ë³´)\n        idx = done_idx - block_size\n        \n        # ===== States =====\n        # ì´ë¯¸ì§€ë¥¼ í‰íƒ„í™”í•˜ê³  ì •ê·œí™” (0~255 â†’ 0~1)\n        states = torch.tensor(\n            np.array(self.data[idx:done_idx]), \n            dtype=torch.float32\n        ).reshape(block_size, -1) / 255.0  # ì •ê·œí™”!\n        \n        # ===== Actions =====\n        actions = torch.tensor(\n            self.actions[idx:done_idx], \n            dtype=torch.long\n        ).unsqueeze(1)\n        \n        # ===== RTGs =====\n        rtgs = torch.tensor(\n            self.rtgs[idx:done_idx], \n            dtype=torch.float32\n        ).unsqueeze(1)\n        \n        # ===== Timesteps =====\n        # ì‹œì‘ timestepë§Œ ë°˜í™˜ (ë‚˜ë¨¸ì§€ëŠ” ëª¨ë¸ì—ì„œ ê³„ì‚°)\n        timesteps = torch.tensor(\n            self.timesteps[idx:idx+1], \n            dtype=torch.int64\n        ).unsqueeze(1)\n        \n        return states, actions, rtgs, timesteps\n\n# ============================================================\n# ğŸ§ª Dataset í…ŒìŠ¤íŠ¸\n# ============================================================\nprint(\"ğŸ§ª StateActionReturnDataset í…ŒìŠ¤íŠ¸\")\nprint(\"=\"*60)\n\n# ë”ë¯¸ ë°ì´í„° ì¤€ë¹„\nnum_samples = 500\ncontext_length = 30\n\n# ì´ë¯¸ì§€ ë°ì´í„° (4 stacked frames)\ndummy_data = [np.random.randint(0, 256, (4, 84, 84), dtype=np.uint8) for _ in range(num_samples)]\ndummy_actions = np.random.randint(0, 4, num_samples)\ndummy_rewards = np.random.choice([0, 1, 5], num_samples).astype(np.float32)\n\n# ì—í”¼ì†Œë“œ ê²½ê³„ (4ê°œ ì—í”¼ì†Œë“œ)\ndummy_done_idxs = [100, 200, 350, 500]\n\n# RTG ê³„ì‚°\ndummy_rtgs = calculate_rtg_atari_style(dummy_rewards, dummy_done_idxs)\n\n# Timesteps (ì—í”¼ì†Œë“œ ë‚´ ì‹œê°„)\ndummy_timesteps = np.zeros(num_samples, dtype=np.int64)\nstart = 0\nfor done in dummy_done_idxs:\n    dummy_timesteps[start:done] = np.arange(done - start)\n    start = done\n\n# Dataset ìƒì„±\ndataset = StateActionReturnDataset(\n    dummy_data, \n    context_length * 3,  # block_size\n    dummy_actions, \n    dummy_done_idxs, \n    dummy_rtgs, \n    dummy_timesteps\n)\n\nprint(f\"\\nğŸ“Š Dataset ì •ë³´:\")\nprint(f\"   ì´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}\")\nprint(f\"   Vocab size (action ìˆ˜): {dataset.vocab_size}\")\nprint(f\"   Block size: {dataset.block_size}\")\nprint(f\"   Context length: {context_length}\")\n\n# ìƒ˜í”Œ í•˜ë‚˜ ê°€ì ¸ì˜¤ê¸°\nstates, actions, rtgs, timesteps = dataset[0]\n\nprint(f\"\\nğŸ“¦ ìƒ˜í”Œ Shapes:\")\nprint(f\"   states:    {states.shape}\")\nprint(f\"              â””â”€ ({context_length}, 4Ã—84Ã—84) = ({context_length}, 28224)\")\nprint(f\"   actions:   {actions.shape}\")\nprint(f\"              â””â”€ ({context_length}, 1)\")\nprint(f\"   rtgs:      {rtgs.shape}\")\nprint(f\"              â””â”€ ({context_length}, 1)\")\nprint(f\"   timesteps: {timesteps.shape}\")\nprint(f\"              â””â”€ (1, 1)\")\n\nprint(f\"\\nğŸ“ˆ ë°ì´í„° ë²”ìœ„:\")\nprint(f\"   states: [{states.min():.3f}, {states.max():.3f}] (ì •ê·œí™”ë¨)\")\nprint(f\"   actions: {actions.unique().tolist()} (ì´ì‚°ê°’)\")\nprint(f\"   rtgs: [{rtgs.min():.1f}, {rtgs.max():.1f}]\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. CNN Encoder ì´í•´\n",
    "\n",
    "## ğŸ¯ ëª©í‘œ\n",
    "> Atari ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” CNN Encoderì˜ êµ¬ì¡°ì™€ ë™ì‘ì„ ì´í•´í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ–¼ï¸ ì™œ CNNì´ í•„ìš”í•œê°€?\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph GymWay[\"ğŸ¤¸ Gym í™˜ê²½ (Phase 3)\"]\n",
    "        GS[\"State: 11ì°¨ì› ë²¡í„°\"] --> GL[\"Linear ë ˆì´ì–´\"] --> GE[\"ì„ë² ë”©\"]\n",
    "    end\n",
    "\n",
    "    subgraph AtariWay[\"ğŸ® Atari í™˜ê²½\"]\n",
    "        AS[\"State: 84Ã—84Ã—4\\n= 28,224ì°¨ì› ì´ë¯¸ì§€\"] --> CNN[\"CNN Encoder\\nê³µê°„ì  êµ¬ì¡° í™œìš©\"] --> AE[\"128ì°¨ì› ì„ë² ë”©\"]\n",
    "    end\n",
    "\n",
    "    GymWay -.->|\"ì°¨ì›ì´ 2,500ë°°\\nì¦ê°€!\"| AtariWay\n",
    "\n",
    "    style GymWay fill:#e8f5e9,stroke:#4caf50\n",
    "    style AtariWay fill:#fff3e0,stroke:#ef6c00\n",
    "    style CNN fill:#e3f2fd,stroke:#1565c0,stroke-width:3px\n",
    "```\n",
    "\n",
    "## ğŸ—ï¸ CNN Encoder ì•„í‚¤í…ì²˜\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    Input[\"ğŸ“¥ ì…ë ¥\\n(batch, 4, 84, 84)\\n4 stacked grayscale frames\"]\n",
    "\n",
    "    subgraph Conv1[\"Conv1\"]\n",
    "        C1[\"Conv2d(4â†’32, kernel=8, stride=4)\\n+ ReLU\"]\n",
    "        C1S[\"(batch, 32, 20, 20)\"]\n",
    "    end\n",
    "\n",
    "    subgraph Conv2[\"Conv2\"]\n",
    "        C2[\"Conv2d(32â†’64, kernel=4, stride=2)\\n+ ReLU\"]\n",
    "        C2S[\"(batch, 64, 9, 9)\"]\n",
    "    end\n",
    "\n",
    "    subgraph Conv3[\"Conv3\"]\n",
    "        C3[\"Conv2d(64â†’64, kernel=3, stride=1)\\n+ ReLU\"]\n",
    "        C3S[\"(batch, 64, 7, 7)\"]\n",
    "    end\n",
    "\n",
    "    Flat[\"Flatten\\n(batch, 3136)\"]\n",
    "    Linear[\"Linear(3136 â†’ 128) + Tanh\\n(batch, 128)\"]\n",
    "    Output[\"ğŸ“¤ ì¶œë ¥: State ì„ë² ë”© ë²¡í„°\"]\n",
    "\n",
    "    Input --> Conv1 --> Conv2 --> Conv3 --> Flat --> Linear --> Output\n",
    "\n",
    "    style Input fill:#e1f5fe,stroke:#0277bd\n",
    "    style Conv1 fill:#fff3e0,stroke:#ef6c00\n",
    "    style Conv2 fill:#fff3e0,stroke:#ef6c00\n",
    "    style Conv3 fill:#fff3e0,stroke:#ef6c00\n",
    "    style Flat fill:#f3e5f5,stroke:#7b1fa2\n",
    "    style Linear fill:#e8f5e9,stroke:#2e7d32\n",
    "    style Output fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px\n",
    "```\n",
    "\n",
    "### ğŸ“ ì¶œë ¥ í¬ê¸° ê³„ì‚° ê³¼ì •\n",
    "\n",
    "> **ê³µì‹**: `output = (input - kernel) / stride + 1`\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    S0[\"84Ã—84\"] -->|\"(84-8)/4+1=20\"| S1[\"20Ã—20\"]\n",
    "    S1 -->|\"(20-4)/2+1=9\"| S2[\"9Ã—9\"]\n",
    "    S2 -->|\"(9-3)/1+1=7\"| S3[\"7Ã—7\"]\n",
    "    S3 -->|\"64Ã—7Ã—7\"| S4[\"3136\"]\n",
    "    S4 -->|\"Linear\"| S5[\"128\"]\n",
    "\n",
    "    style S0 fill:#e1f5fe,stroke:#0277bd\n",
    "    style S1 fill:#fff3e0,stroke:#ef6c00\n",
    "    style S2 fill:#fff3e0,stroke:#ef6c00\n",
    "    style S3 fill:#fff3e0,stroke:#ef6c00\n",
    "    style S4 fill:#f3e5f5,stroke:#7b1fa2\n",
    "    style S5 fill:#c8e6c9,stroke:#2e7d32\n",
    "```\n",
    "\n",
    "> ğŸ’¡ **ì´ êµ¬ì¡°ëŠ” DQN ë…¼ë¬¸ì˜ CNNê³¼ ë™ì¼í•©ë‹ˆë‹¤!** (Atari ê°•í™”í•™ìŠµì˜ í‘œì¤€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ—ï¸ CNN Encoder êµ¬í˜„\n# ============================================================\n# model_atari.pyì˜ CNN Encoderë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n# DQN ë…¼ë¬¸ê³¼ ë™ì¼í•œ êµ¬ì¡°ì…ë‹ˆë‹¤.\n# ============================================================\n\nclass AtariCNNEncoder(nn.Module):\n    \"\"\"\n    DQN ìŠ¤íƒ€ì¼ CNN Encoder\n    \n    Parameters:\n    -----------\n    n_embd : int\n        ì¶œë ¥ ì„ë² ë”© ì°¨ì› (default: 128)\n        \n    ì…ë ¥: (batch, 4, 84, 84) - 4 stacked grayscale frames\n    ì¶œë ¥: (batch, n_embd) - State ì„ë² ë”© ë²¡í„°\n    \"\"\"\n    \n    def __init__(self, n_embd=128):\n        super().__init__()\n        \n        self.encoder = nn.Sequential(\n            # ===== Conv1 =====\n            # ì…ë ¥: (4, 84, 84) â†’ ì¶œë ¥: (32, 20, 20)\n            # ê³„ì‚°: (84 - 8) / 4 + 1 = 20\n            nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n            nn.ReLU(),\n            \n            # ===== Conv2 =====\n            # ì…ë ¥: (32, 20, 20) â†’ ì¶œë ¥: (64, 9, 9)\n            # ê³„ì‚°: (20 - 4) / 2 + 1 = 9\n            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n            nn.ReLU(),\n            \n            # ===== Conv3 =====\n            # ì…ë ¥: (64, 9, 9) â†’ ì¶œë ¥: (64, 7, 7)\n            # ê³„ì‚°: (9 - 3) / 1 + 1 = 7\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            \n            # ===== Flatten =====\n            # ì…ë ¥: (64, 7, 7) â†’ ì¶œë ¥: (3136)\n            # ê³„ì‚°: 64 Ã— 7 Ã— 7 = 3136\n            nn.Flatten(),\n            \n            # ===== Linear =====\n            # ì…ë ¥: (3136) â†’ ì¶œë ¥: (n_embd)\n            nn.Linear(64 * 7 * 7, n_embd),\n            nn.Tanh()  # ì¶œë ¥ì„ [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”\n        )\n        \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: (batch, 4, 84, 84) - 4 stacked grayscale frames\n            \n        Returns:\n            (batch, n_embd) - State ì„ë² ë”© ë²¡í„°\n        \"\"\"\n        return self.encoder(x)\n\n# ============================================================\n# ğŸ§ª CNN Encoder í…ŒìŠ¤íŠ¸\n# ============================================================\nprint(\"ğŸ—ï¸ CNN Encoder í…ŒìŠ¤íŠ¸\")\nprint(\"=\"*60)\n\nencoder = AtariCNNEncoder(n_embd=128)\n\nprint(\"\\nğŸ“¦ ëª¨ë¸ êµ¬ì¡°:\")\nprint(encoder)\n\n# íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°\ntotal_params = sum(p.numel() for p in encoder.parameters())\nprint(f\"\\nğŸ“Š ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}\")\n\n# Forward pass í…ŒìŠ¤íŠ¸\nx = torch.randn(4, 4, 84, 84)  # batch=4\nout = encoder(x)\n\nprint(f\"\\nğŸ”„ Forward Pass:\")\nprint(f\"   ì…ë ¥: {x.shape}\")\nprint(f\"   ì¶œë ¥: {out.shape}\")\nprint(f\"\\n   ì°¨ì› ì¶•ì†Œ ë¹„ìœ¨: {4*84*84} â†’ {out.shape[-1]}\")\nprint(f\"   ({4*84*84 / out.shape[-1]:.0f}ë°° ì••ì¶•)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“ ê° ë ˆì´ì–´ì˜ ì¶œë ¥ í¬ê¸° ì¶”ì \n# ============================================================\n# CNNì—ì„œ ì¶œë ¥ í¬ê¸° ê³„ì‚° ê³µì‹:\n#   output_size = (input_size - kernel_size + 2*padding) / stride + 1\n#\n# ì´ ê³µì‹ì„ ì‚¬ìš©í•´ ê° ë ˆì´ì–´ì˜ ì¶œë ¥ì„ ì¶”ì í•©ë‹ˆë‹¤.\n# ============================================================\n\ndef calc_conv_output(input_size, kernel_size, stride, padding=0):\n    \"\"\"Conv2d ì¶œë ¥ í¬ê¸° ê³„ì‚°\"\"\"\n    return (input_size - kernel_size + 2 * padding) // stride + 1\n\nprint(\"ğŸ“ CNN ì¶œë ¥ í¬ê¸° ë‹¨ê³„ë³„ ì¶”ì \")\nprint(\"=\"*60)\n\n# ë‹¨ê³„ë³„ ì¶œë ¥ ê¸°ë¡\nstages = []\n\n# ì…ë ¥\nsize = 84\nchannels = 4\nstages.append(('Input', channels, size, size))\n\n# Conv1\nchannels = 32\nsize = calc_conv_output(size, kernel_size=8, stride=4)\nstages.append(('Conv1 (k=8, s=4)', channels, size, size))\n\n# Conv2\nchannels = 64\nsize = calc_conv_output(size, kernel_size=4, stride=2)\nstages.append(('Conv2 (k=4, s=2)', channels, size, size))\n\n# Conv3\nchannels = 64\nsize = calc_conv_output(size, kernel_size=3, stride=1)\nstages.append(('Conv3 (k=3, s=1)', channels, size, size))\n\n# í‘œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥\nprint(f\"\\n{'Stage':<20} {'Shape':<20} {'Elements':>12}\")\nprint(\"-\"*55)\n\nfor stage_name, c, h, w in stages:\n    shape = f\"({c}, {h}, {w})\"\n    elements = c * h * w\n    print(f\"{stage_name:<20} {shape:<20} {elements:>12,}\")\n\n# Flatten\nflatten_size = stages[-1][1] * stages[-1][2] * stages[-1][3]\nprint(f\"{'Flatten':<20} {'(' + str(flatten_size) + ')':<20} {flatten_size:>12,}\")\n\n# Linear\nfinal_size = 128\nprint(f\"{'Linear':<20} {'(' + str(final_size) + ')':<20} {final_size:>12,}\")\n\nprint(\"\\n\" + \"-\"*55)\nprint(f\"ì´ ì••ì¶•ë¥ : {4*84*84:,} â†’ {final_size} ({4*84*84/final_size:.0f}ë°°)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ” ì‹¤ì œ í…ì„œë¡œ ì¶”ì  í™•ì¸\n# ============================================================\n# ì‹¤ì œë¡œ í…ì„œë¥¼ í†µê³¼ì‹œí‚¤ë©° ê° ë ˆì´ì–´ì˜ ì¶œë ¥ì„ í™•ì¸í•©ë‹ˆë‹¤.\n# ============================================================\n\ndef trace_cnn_with_tensor(x):\n    \"\"\"\n    ì‹¤ì œ í…ì„œë¡œ CNN ê° ë‹¨ê³„ë¥¼ ì¶”ì \n    \"\"\"\n    print(f\"ğŸ” ì‹¤ì œ í…ì„œë¡œ CNN ì¶”ì \")\n    print(\"=\"*60)\n    print(f\"ì…ë ¥: {x.shape}\")\n    print(\"-\"*60)\n    \n    # Conv1\n    conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n    x = F.relu(conv1(x))\n    print(f\"Conv1 (k=8, s=4) + ReLU: {x.shape}\")\n    print(f\"  â””â”€ ê³„ì‚°: (84-8)/4 + 1 = 20\")\n    \n    # Conv2\n    conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n    x = F.relu(conv2(x))\n    print(f\"Conv2 (k=4, s=2) + ReLU: {x.shape}\")\n    print(f\"  â””â”€ ê³„ì‚°: (20-4)/2 + 1 = 9\")\n    \n    # Conv3\n    conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n    x = F.relu(conv3(x))\n    print(f\"Conv3 (k=3, s=1) + ReLU: {x.shape}\")\n    print(f\"  â””â”€ ê³„ì‚°: (9-3)/1 + 1 = 7\")\n    \n    # Flatten\n    x = x.flatten(1)  # batch ì°¨ì› ìœ ì§€\n    print(f\"Flatten: {x.shape}\")\n    print(f\"  â””â”€ 64 Ã— 7 Ã— 7 = 3136\")\n    \n    # Linear\n    linear = nn.Linear(3136, 128)\n    x = torch.tanh(linear(x))\n    print(f\"Linear + Tanh: {x.shape}\")\n    \n    print(\"-\"*60)\n    print(f\"ìµœì¢… ì¶œë ¥: {x.shape}\")\n    return x\n\n# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\ntest_input = torch.randn(1, 4, 84, 84)  # batch=1\noutput = trace_cnn_with_tensor(test_input)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. ëª¨ë¸ êµ¬ì¡° ë¶„ì„\n",
    "\n",
    "## ğŸ¯ ëª©í‘œ\n",
    "> Atariìš© Decision Transformer (AtariGPT)ì˜ ì „ì²´ êµ¬ì¡°ë¥¼ ì´í•´í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ—ï¸ AtariGPT ì „ì²´ ì•„í‚¤í…ì²˜\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Inputs[\"ğŸ“¥ ì…ë ¥\"]\n",
    "        direction LR\n",
    "        IS[\"states\\n(B, K, 4Ã—84Ã—84)\"]\n",
    "        IA[\"actions\\n(B, K, 1)\"]\n",
    "        IR[\"rtgs\\n(B, K, 1)\"]\n",
    "    end\n",
    "\n",
    "    subgraph Embedding[\"1ï¸âƒ£ Embedding Layer\"]\n",
    "        direction LR\n",
    "        subgraph CNNEnc[\"CNN Encoder (state)\"]\n",
    "            CE[\"Conv1â†’Conv2â†’Conv3\\nâ†’Linear\"]\n",
    "        end\n",
    "        subgraph RTGEmb[\"RTG Embedding\"]\n",
    "            RE[\"Linear(1â†’128)\"]\n",
    "        end\n",
    "        subgraph ActEmb[\"Action Embedding\"]\n",
    "            AE[\"nn.Embedding\\n(vocabâ†’128)\"]\n",
    "        end\n",
    "\n",
    "        CE --> SEout[\"(B,K,128)\"]\n",
    "        RE --> REout[\"(B,K,128)\"]\n",
    "        AE --> AEout[\"(B,K,128)\"]\n",
    "    end\n",
    "\n",
    "    subgraph SeqBuild[\"2ï¸âƒ£ Sequence Construction\"]\n",
    "        direction TB\n",
    "        Interleave[\"Interleave (Reward-Conditioned)\\n[Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚, aâ‚, ..., Râ‚–, sâ‚–, aâ‚–]\"]\n",
    "        SeqShape[\"Shape: (B, KÃ—3, 128)\\nì˜ˆ: K=30 â†’ (B, 90, 128)\"]\n",
    "        Interleave --> SeqShape\n",
    "    end\n",
    "\n",
    "    subgraph TFBlocks[\"3ï¸âƒ£ Transformer Blocks (Ã— N layers)\"]\n",
    "        direction LR\n",
    "        LN1[\"LayerNorm\"] --> CSA[\"Causal\\nSelfAttention\"]\n",
    "        CSA --> Add1[\"+\"]\n",
    "        Add1 --> LN2[\"LayerNorm\"]\n",
    "        LN2 --> MLP[\"MLP\"]\n",
    "        MLP --> Add2[\"+\"]\n",
    "    end\n",
    "\n",
    "    subgraph PredHead[\"4ï¸âƒ£ Prediction Head\"]\n",
    "        direction TB\n",
    "        Extract[\"State ìœ„ì¹˜ ì¶”ì¶œ\\n([:, 1::3, :])\"]\n",
    "        PredA[\"Linear(128 â†’ vocab_size)\\nâ†’ action logits â­\"]\n",
    "    end\n",
    "\n",
    "    Loss[\"ğŸ“‰ Cross-Entropy Loss\\n(action ë¶„ë¥˜)\"]\n",
    "\n",
    "    Inputs --> Embedding --> SeqBuild --> TFBlocks --> PredHead --> Loss\n",
    "\n",
    "    style Inputs fill:#e1f5fe,stroke:#0277bd\n",
    "    style Embedding fill:#fff3e0,stroke:#ef6c00\n",
    "    style SeqBuild fill:#f3e5f5,stroke:#7b1fa2\n",
    "    style TFBlocks fill:#e8f5e9,stroke:#2e7d32\n",
    "    style PredHead fill:#ffebee,stroke:#c62828\n",
    "    style PredA fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px\n",
    "    style Loss fill:#ffcdd2,stroke:#c62828\n",
    "```\n",
    "\n",
    "## ğŸ’¡ Gym vs Atari: í•µì‹¬ ì°¨ì´ì \n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph GymModel[\"ğŸ¤¸ Gym DT\"]\n",
    "        direction TB\n",
    "        GM_A[\"Action: ì—°ì†ê°’\"]\n",
    "        GM_L[\"Loss: MSE\"]\n",
    "        GM_O[\"ì¶œë ¥: (B, K, action_dim)\"]\n",
    "        GM_E[\"Encoder: Linear\"]\n",
    "\n",
    "        GM_A --> GM_L --> GM_O\n",
    "    end\n",
    "\n",
    "    subgraph AtariModel[\"ğŸ® Atari DT\"]\n",
    "        direction TB\n",
    "        AM_A[\"Action: ì´ì‚°ê°’ (4~18ê°œ)\"]\n",
    "        AM_L[\"Loss: Cross-Entropy\"]\n",
    "        AM_O[\"ì¶œë ¥: (B, K, vocab_size) logits\"]\n",
    "        AM_E[\"Encoder: CNN\"]\n",
    "\n",
    "        AM_A --> AM_L --> AM_O\n",
    "    end\n",
    "\n",
    "    GymModel -.->|\"ì—°ì† â†’ ì´ì‚°\\nMSE â†’ CE\"| AtariModel\n",
    "\n",
    "    style GymModel fill:#e8f5e9,stroke:#4caf50\n",
    "    style AtariModel fill:#fff3e0,stroke:#ef6c00\n",
    "```\n",
    "\n",
    "### ğŸ”‘ ì™œ Cross-Entropyë¥¼ ì‚¬ìš©í• ê¹Œ?\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Continuous[\"ì—°ì† Action (Gym)\"]\n",
    "        C1[\"ì¶œë ¥: [0.32, -1.5, 0.8]\"]\n",
    "        C2[\"ì‹¤ì œì™€ì˜ ê±°ë¦¬ ê³„ì‚°\"]\n",
    "        C3[\"MSE Loss\"]\n",
    "        C1 --> C2 --> C3\n",
    "    end\n",
    "\n",
    "    subgraph Discrete[\"ì´ì‚° Action (Atari)\"]\n",
    "        D1[\"ì¶œë ¥: logits [2.1, 0.3, -1.0, 0.5]\"]\n",
    "        D2[\"ë¶„ë¥˜ ë¬¸ì œ!\\n4ê°œ ì¤‘ í•˜ë‚˜ ì„ íƒ\"]\n",
    "        D3[\"Cross-Entropy Loss\"]\n",
    "        D1 --> D2 --> D3\n",
    "    end\n",
    "\n",
    "    style Continuous fill:#e8f5e9,stroke:#4caf50\n",
    "    style Discrete fill:#fff3e0,stroke:#ef6c00\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ—ï¸ Transformer êµ¬ì„± ìš”ì†Œ êµ¬í˜„\n# ============================================================\n# model_atari.pyì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n# - CausalSelfAttention: ì¸ê³¼ì  self-attention\n# - Block: Transformer ë¸”ë¡\n# ============================================================\n\nimport math\n\nclass CausalSelfAttention(nn.Module):\n    \"\"\"\n    Causal Self-Attention (ë¯¸ë˜ ì •ë³´ë¥¼ ë³´ì§€ ì•ŠëŠ” attention)\n    \n    GPT ìŠ¤íƒ€ì¼ì˜ unidirectional attentionì…ë‹ˆë‹¤.\n    í˜„ì¬ í† í°ì€ ì´ì „ í† í°ë“¤ë§Œ ì°¸ì¡°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n    \"\"\"\n    \n    def __init__(self, n_embd, n_head, block_size, attn_pdrop=0.1, resid_pdrop=0.1):\n        super().__init__()\n        assert n_embd % n_head == 0, \"n_embd must be divisible by n_head\"\n        \n        # Q, K, V projection\n        self.key = nn.Linear(n_embd, n_embd)\n        self.query = nn.Linear(n_embd, n_embd)\n        self.value = nn.Linear(n_embd, n_embd)\n        \n        # Dropout layers\n        self.attn_drop = nn.Dropout(attn_pdrop)\n        self.resid_drop = nn.Dropout(resid_pdrop)\n        \n        # Output projection\n        self.proj = nn.Linear(n_embd, n_embd)\n        \n        # Causal mask: í•˜ì‚¼ê° í–‰ë ¬ (ë¯¸ë˜ ì •ë³´ ì°¨ë‹¨)\n        # mask[i,j] = 1 if j <= i else 0\n        self.register_buffer(\n            \"mask\",\n            torch.tril(torch.ones(block_size + 1, block_size + 1))\n                 .view(1, 1, block_size + 1, block_size + 1)\n        )\n        \n        self.n_head = n_head\n        \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: (B, T, C) - ì…ë ¥ ì‹œí€€ìŠ¤\n            \n        Returns:\n            (B, T, C) - attention ì ìš© í›„ ì¶œë ¥\n        \"\"\"\n        B, T, C = x.size()\n        \n        # Q, K, V ê³„ì‚° í›„ multi-headë¡œ reshape\n        # (B, T, C) â†’ (B, T, n_head, head_dim) â†’ (B, n_head, T, head_dim)\n        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        \n        # Attention ê³„ì‚°\n        # (B, n_head, T, head_dim) @ (B, n_head, head_dim, T) â†’ (B, n_head, T, T)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        \n        # Causal masking: ë¯¸ë˜ ìœ„ì¹˜ì— -inf ì ìš©\n        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n        \n        # Softmax\n        att = F.softmax(att, dim=-1)\n        att = self.attn_drop(att)\n        \n        # Valueì™€ ê³±í•˜ê¸°\n        # (B, n_head, T, T) @ (B, n_head, T, head_dim) â†’ (B, n_head, T, head_dim)\n        y = att @ v\n        \n        # Reshape back\n        # (B, n_head, T, head_dim) â†’ (B, T, C)\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        \n        # Output projection\n        y = self.resid_drop(self.proj(y))\n        \n        return y\n\n\nclass Block(nn.Module):\n    \"\"\"\n    Transformer Block\n    \n    êµ¬ì¡°: LayerNorm â†’ Attention â†’ + â†’ LayerNorm â†’ MLP â†’ +\n    (Pre-LN êµ¬ì¡°)\n    \"\"\"\n    \n    def __init__(self, n_embd, n_head, block_size, resid_pdrop=0.1):\n        super().__init__()\n        \n        # LayerNorm layers\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n        \n        # Self-Attention\n        self.attn = CausalSelfAttention(n_embd, n_head, block_size)\n        \n        # MLP (Feed-Forward Network)\n        # hidden dim = 4 * n_embd (GPT-2 í‘œì¤€)\n        self.mlp = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.GELU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(resid_pdrop),\n        )\n        \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: (B, T, C)\n            \n        Returns:\n            (B, T, C)\n        \"\"\"\n        # Attention with residual connection\n        x = x + self.attn(self.ln1(x))\n        \n        # MLP with residual connection\n        x = x + self.mlp(self.ln2(x))\n        \n        return x\n\n# ============================================================\n# ğŸ§ª Block í…ŒìŠ¤íŠ¸\n# ============================================================\nprint(\"ğŸ—ï¸ Transformer Block í…ŒìŠ¤íŠ¸\")\nprint(\"=\"*60)\n\nblock = Block(n_embd=128, n_head=8, block_size=90)\nx = torch.randn(4, 90, 128)  # (batch, seq_len, hidden)\n\nout = block(x)\n\nprint(f\"ì…ë ¥: {x.shape}\")\nprint(f\"ì¶œë ¥: {out.shape}\")\nprint(f\"ì”ì°¨ ì—°ê²°ë¡œ shape ìœ ì§€ë¨ âœ“\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ® AtariGPT ì „ì²´ ëª¨ë¸ êµ¬í˜„\n# ============================================================\n# model_atari.pyì˜ GPT í´ë˜ìŠ¤ë¥¼ ê°„ì†Œí™”í•˜ì—¬ êµ¬í˜„í•©ë‹ˆë‹¤.\n# ë‘ ê°€ì§€ ëª¨ë“œë¥¼ ì§€ì›í•©ë‹ˆë‹¤:\n#   - reward_conditioned: Decision Transformer (RTG ì‚¬ìš©)\n#   - naive: Behavior Cloning (RTG ë¯¸ì‚¬ìš©)\n# ============================================================\n\nclass AtariGPT(nn.Module):\n    \"\"\"\n    Atariìš© Decision Transformer\n    \n    Parameters:\n    -----------\n    vocab_size : int\n        Action ìˆ˜ (ê²Œì„ë§ˆë‹¤ ë‹¤ë¦„: Breakout=4, Seaquest=18)\n    block_size : int\n        ì´ ì‹œí€€ìŠ¤ ê¸¸ì´ (context_length * 3)\n    n_layer : int\n        Transformer ë ˆì´ì–´ ìˆ˜\n    n_head : int\n        Attention head ìˆ˜\n    n_embd : int\n        Hidden dimension\n    max_timestep : int\n        ì—í”¼ì†Œë“œ ë‚´ ìµœëŒ€ timestep\n    model_type : str\n        'reward_conditioned' (DT) or 'naive' (BC)\n    \"\"\"\n    \n    def __init__(self, vocab_size, block_size, n_layer=6, n_head=8, n_embd=128,\n                 max_timestep=4000, model_type='reward_conditioned'):\n        super().__init__()\n        \n        self.model_type = model_type\n        self.block_size = block_size\n        self.n_embd = n_embd\n        \n        # ===== Position Embeddings =====\n        # ì‹œí€€ìŠ¤ ë‚´ ìœ„ì¹˜ ì„ë² ë”©\n        self.pos_emb = nn.Parameter(torch.zeros(1, block_size + 1, n_embd))\n        # ì—í”¼ì†Œë“œ ë‚´ ì ˆëŒ€ ì‹œê°„ ì„ë² ë”©\n        self.global_pos_emb = nn.Parameter(torch.zeros(1, max_timestep + 1, n_embd))\n        \n        self.drop = nn.Dropout(0.1)\n        \n        # ===== Transformer Blocks =====\n        self.blocks = nn.Sequential(*[\n            Block(n_embd, n_head, block_size) for _ in range(n_layer)\n        ])\n        self.ln_f = nn.LayerNorm(n_embd)\n        \n        # ===== Prediction Head =====\n        # Action ë¶„ë¥˜ (logits ì¶œë ¥)\n        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n        \n        # ===== Encoders =====\n        # State: CNN Encoder\n        self.state_encoder = AtariCNNEncoder(n_embd)\n        \n        # RTG: Linear + Tanh\n        self.ret_emb = nn.Sequential(nn.Linear(1, n_embd), nn.Tanh())\n        \n        # Action: Embedding Lookup + Tanh\n        self.action_embeddings = nn.Sequential(\n            nn.Embedding(vocab_size, n_embd),\n            nn.Tanh()\n        )\n        \n    def forward(self, states, actions, targets=None, rtgs=None, timesteps=None):\n        \"\"\"\n        Forward pass\n        \n        Args:\n            states: (B, K, 4*84*84) - í‰íƒ„í™”ëœ ì´ë¯¸ì§€\n            actions: (B, K, 1) - ì´ì‚° action\n            targets: (B, K) - ì •ë‹µ action (ì†ì‹¤ ê³„ì‚°ìš©)\n            rtgs: (B, K, 1) - RTG\n            timesteps: (B, 1, 1) - ì‹œì‘ timestep\n            \n        Returns:\n            logits: (B, K, vocab_size) - action logits\n            loss: scalar or None\n        \"\"\"\n        batch_size = states.shape[0]\n        seq_len = states.shape[1]  # K = context_length\n        \n        # ===== State Embedding via CNN =====\n        # (B, K, 4*84*84) â†’ (B*K, 4, 84, 84)\n        states_reshaped = states.reshape(-1, 4, 84, 84)\n        # CNN í†µê³¼\n        state_embeddings = self.state_encoder(states_reshaped)\n        # (B*K, n_embd) â†’ (B, K, n_embd)\n        state_embeddings = state_embeddings.reshape(batch_size, seq_len, self.n_embd)\n        \n        # ===== ëª¨ë“œë³„ ì‹œí€€ìŠ¤ êµ¬ì„± =====\n        if self.model_type == 'reward_conditioned':\n            # Decision Transformer: [R, s, a, R, s, a, ...]\n            rtg_embeddings = self.ret_emb(rtgs.float())\n            action_embeddings = self.action_embeddings(actions.squeeze(-1).long())\n            \n            # ì‹œí€€ìŠ¤ êµ¬ì„±: 3K ê¸¸ì´\n            token_embeddings = torch.zeros(\n                batch_size, seq_len * 3, self.n_embd,\n                device=states.device\n            )\n            token_embeddings[:, 0::3, :] = rtg_embeddings     # ìœ„ì¹˜ 0, 3, 6, ...\n            token_embeddings[:, 1::3, :] = state_embeddings   # ìœ„ì¹˜ 1, 4, 7, ...\n            token_embeddings[:, 2::3, :] = action_embeddings  # ìœ„ì¹˜ 2, 5, 8, ...\n            \n        else:  # naive (Behavior Cloning)\n            # BC: [s, a, s, a, ...]\n            action_embeddings = self.action_embeddings(actions.squeeze(-1).long())\n            \n            # ì‹œí€€ìŠ¤ êµ¬ì„±: 2K ê¸¸ì´\n            token_embeddings = torch.zeros(\n                batch_size, seq_len * 2, self.n_embd,\n                device=states.device\n            )\n            token_embeddings[:, 0::2, :] = state_embeddings   # ìœ„ì¹˜ 0, 2, 4, ...\n            token_embeddings[:, 1::2, :] = action_embeddings  # ìœ„ì¹˜ 1, 3, 5, ...\n        \n        # ===== Position Embedding ì¶”ê°€ =====\n        position_embeddings = self.pos_emb[:, :token_embeddings.shape[1], :]\n        \n        # ===== Transformer í†µê³¼ =====\n        x = self.drop(token_embeddings + position_embeddings)\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        \n        # ===== Prediction Head =====\n        logits = self.head(x)\n        \n        # ===== State ìœ„ì¹˜ì˜ ì¶œë ¥ë§Œ ì‚¬ìš© =====\n        if self.model_type == 'reward_conditioned':\n            logits = logits[:, 1::3, :]  # State ìœ„ì¹˜ (1, 4, 7, ...)\n        else:\n            logits = logits[:, 0::2, :]  # State ìœ„ì¹˜ (0, 2, 4, ...)\n        \n        # ===== ì†ì‹¤ ê³„ì‚° =====\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(\n                logits.reshape(-1, logits.size(-1)),\n                targets.reshape(-1)\n            )\n        \n        return logits, loss\n\n# ============================================================\n# ğŸ§ª AtariGPT í…ŒìŠ¤íŠ¸\n# ============================================================\nprint(\"ğŸ® AtariGPT ëª¨ë¸ ìƒì„± ë° í…ŒìŠ¤íŠ¸\")\nprint(\"=\"*60)\n\nmodel = AtariGPT(\n    vocab_size=4,       # Breakout: 4 actions\n    block_size=90,      # context_length * 3 = 30 * 3\n    n_layer=6,\n    n_head=8,\n    n_embd=128,\n    model_type='reward_conditioned'\n)\n\n# íŒŒë¼ë¯¸í„° ìˆ˜\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"\\nğŸ“Š ëª¨ë¸ í†µê³„:\")\nprint(f\"   ì´ íŒŒë¼ë¯¸í„°: {total_params:,}\")\nprint(f\"   ëª¨ë¸ íƒ€ì…: {model.model_type}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ”„ Forward Pass í…ŒìŠ¤íŠ¸\n# ============================================================\n# ëª¨ë¸ì— ë”ë¯¸ ì…ë ¥ì„ ë„£ì–´ ë™ì‘ì„ í™•ì¸í•©ë‹ˆë‹¤.\n# ============================================================\n\nprint(\"ğŸ”„ Forward Pass í…ŒìŠ¤íŠ¸\")\nprint(\"=\"*60)\n\nbatch_size = 2\ncontext_len = 30\n\n# ì…ë ¥ ìƒì„± (ì •ê·œí™”ëœ ì´ë¯¸ì§€)\nstates = torch.randn(batch_size, context_len, 4 * 84 * 84)     # í‰íƒ„í™”ëœ ì´ë¯¸ì§€\nactions = torch.randint(0, 4, (batch_size, context_len, 1))    # ì´ì‚° action\nrtgs = torch.randn(batch_size, context_len, 1)                  # RTG\ntimesteps = torch.zeros(batch_size, 1, 1, dtype=torch.long)    # ì‹œì‘ timestep\n\nprint(\"\\nğŸ“¥ ì…ë ¥ Shapes:\")\nprint(f\"   states:    {states.shape}\")\nprint(f\"              â””â”€ (batch, context_len, 4Ã—84Ã—84)\")\nprint(f\"   actions:   {actions.shape}\")\nprint(f\"              â””â”€ (batch, context_len, 1)\")\nprint(f\"   rtgs:      {rtgs.shape}\")\nprint(f\"              â””â”€ (batch, context_len, 1)\")\nprint(f\"   timesteps: {timesteps.shape}\")\nprint(f\"              â””â”€ (batch, 1, 1)\")\n\n# Forward (ì†ì‹¤ ê³„ì‚° í¬í•¨)\nmodel.eval()\nwith torch.no_grad():\n    targets = actions.squeeze(-1)  # ì •ë‹µ action\n    logits, loss = model(states, actions, targets, rtgs, timesteps)\n\nprint(f\"\\nğŸ“¤ ì¶œë ¥ Shapes:\")\nprint(f\"   logits: {logits.shape}\")\nprint(f\"           â””â”€ (batch, context_len, vocab_size={4})\")\nprint(f\"   loss:   {loss.item():.4f}\")\n\n# Logits â†’ Action ë³€í™˜ (argmax)\npredicted_actions = logits.argmax(dim=-1)\nprint(f\"\\nğŸ¯ ì˜ˆì¸¡ëœ Action (ë§ˆì§€ë§‰ timestep):\")\nprint(f\"   batch 0: {predicted_actions[0, -1].item()}\")\nprint(f\"   batch 1: {predicted_actions[1, -1].item()}\")\n\n# Softmaxë¡œ í™•ë¥  ë³€í™˜\nprobs = F.softmax(logits[0, -1], dim=-1)\nprint(f\"\\nğŸ“Š Action í™•ë¥  ë¶„í¬ (batch 0, ë§ˆì§€ë§‰ timestep):\")\nfor i, p in enumerate(probs):\n    bar = 'â–ˆ' * int(p * 20)\n    print(f\"   Action {i}: {p:.3f} {bar}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Reward-Conditioned vs Naive ë¹„êµ\n",
    "\n",
    "## ğŸ¯ ëª©í‘œ\n",
    "> Decision Transformerì˜ ë‘ ëª¨ë“œ (reward_conditioned, naive)ì˜ ì°¨ì´ë¥¼ ì´í•´í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“Š ë‘ ëª¨ë“œ ë¹„êµ\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph RC[\"ğŸ¯ Reward-Conditioned (DT)\"]\n",
    "        direction TB\n",
    "        RC_SEQ[\"ì‹œí€€ìŠ¤: [Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚, aâ‚, ...]\"]\n",
    "        RC_LEN[\"ì‹œí€€ìŠ¤ ê¸¸ì´: K Ã— 3\"]\n",
    "        RC_POS[\"ì˜ˆì¸¡ ìœ„ì¹˜: 1::3 (State ìœ„ì¹˜)\"]\n",
    "        RC_COND[\"âœ… RTGë¡œ ëª©í‘œ return ì§€ì •\"]\n",
    "        RC_INF[\"ì¶”ë¡  ì‹œ RTG ë™ì  ì—…ë°ì´íŠ¸\\n(rtg -= reward)\"]\n",
    "\n",
    "        RC_SEQ --- RC_LEN --- RC_POS --- RC_COND --- RC_INF\n",
    "    end\n",
    "\n",
    "    subgraph NV[\"ğŸ“‹ Naive (BC)\"]\n",
    "        direction TB\n",
    "        NV_SEQ[\"ì‹œí€€ìŠ¤: [sâ‚€, aâ‚€, sâ‚, aâ‚, ...]\"]\n",
    "        NV_LEN[\"ì‹œí€€ìŠ¤ ê¸¸ì´: K Ã— 2\"]\n",
    "        NV_POS[\"ì˜ˆì¸¡ ìœ„ì¹˜: 0::2 (State ìœ„ì¹˜)\"]\n",
    "        NV_COND[\"âŒ ì¡°ê±´ ì—†ìŒ\"]\n",
    "        NV_INF[\"RTG ê³ ì •\\n(0ìœ¼ë¡œ ì„¤ì •)\"]\n",
    "\n",
    "        NV_SEQ --- NV_LEN --- NV_POS --- NV_COND --- NV_INF\n",
    "    end\n",
    "\n",
    "    style RC fill:#e8f5e9,stroke:#2e7d32\n",
    "    style NV fill:#fff3e0,stroke:#ef6c00\n",
    "```\n",
    "\n",
    "### ğŸ® ê²Œì„ë³„ ëª©í‘œ RTG\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph TargetRTG[\"ğŸ¯ Reward-Conditioned ëª¨ë“œì˜ ê²Œì„ë³„ ëª©í‘œ\"]\n",
    "        B[\"Breakout\\nRTG=90\"]\n",
    "        S[\"Seaquest\\nRTG=1150\"]\n",
    "        Q[\"Qbert\\nRTG=14000\"]\n",
    "        P[\"Pong\\nRTG=20\"]\n",
    "    end\n",
    "\n",
    "    subgraph NaiveRTG[\"ğŸ“‹ Naive ëª¨ë“œ\"]\n",
    "        N[\"ëª¨ë“  ê²Œì„\\nRTG=0\\n(ì‚¬ìš© ì•ˆ í•¨)\"]\n",
    "    end\n",
    "\n",
    "    style B fill:#e3f2fd,stroke:#1565c0\n",
    "    style S fill:#e3f2fd,stroke:#1565c0\n",
    "    style Q fill:#e3f2fd,stroke:#1565c0\n",
    "    style P fill:#e3f2fd,stroke:#1565c0\n",
    "    style N fill:#ffebee,stroke:#c62828\n",
    "```\n",
    "\n",
    "## ğŸ’¡ í•µì‹¬ ì°¨ì´\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph DTMode[\"ğŸ¯ Reward-Conditioned (DT)\"]\n",
    "        DT1[\"'1000ì ì„ ì–»ê³  ì‹¶ë‹¤'\"]\n",
    "        DT2[\"â†’ 1000ì  ì–»ì„ ë•Œì˜\\ní–‰ë™ ì˜ˆì¸¡\"]\n",
    "        DT3[\"ë‹¤ì–‘í•œ ìˆ˜ì¤€ì˜\\nì •ì±… ìƒì„± ê°€ëŠ¥\"]\n",
    "        DT1 --> DT2 --> DT3\n",
    "    end\n",
    "\n",
    "    subgraph BCMode[\"ğŸ“‹ Naive (BC)\"]\n",
    "        BC1[\"ì¡°ê±´ ì—†ìŒ\"]\n",
    "        BC2[\"â†’ ë°ì´í„°ì˜ í‰ê· ì ì¸\\ní–‰ë™ ëª¨ë°©\"]\n",
    "        BC3[\"ì¡°ì ˆ ë¶ˆê°€\\n(ë°ì´í„° í‰ê·  ì„±ëŠ¥)\"]\n",
    "        BC1 --> BC2 --> BC3\n",
    "    end\n",
    "\n",
    "    subgraph Control[\"ğŸ›ï¸ ì„±ëŠ¥ ì¡°ì ˆ\"]\n",
    "        High[\"RTG ë†’ê²Œ â†’ ë” ë‚˜ì€ ì„±ëŠ¥\"]\n",
    "        Low[\"RTG ë‚®ê²Œ â†’ ë” ì•ˆì „í•œ í–‰ë™\"]\n",
    "        None[\"ì¡°ì ˆ ë¶ˆê°€\"]\n",
    "    end\n",
    "\n",
    "    DTMode --> High\n",
    "    DTMode --> Low\n",
    "    BCMode --> None\n",
    "\n",
    "    style DTMode fill:#e8f5e9,stroke:#2e7d32\n",
    "    style BCMode fill:#fff3e0,stroke:#ef6c00\n",
    "    style High fill:#c8e6c9,stroke:#2e7d32\n",
    "    style Low fill:#e3f2fd,stroke:#1565c0\n",
    "    style None fill:#ffcdd2,stroke:#c62828\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“Š ë‘ ëª¨ë“œ ë¹„êµí‘œ ì¶œë ¥\n# ============================================================\n# ASCII ì•„íŠ¸ë¡œ ë‘ ëª¨ë“œì˜ ì°¨ì´ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.\n# ============================================================\n\ncomparison = \"\"\"\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    Reward-Conditioned vs Naive ë¹„êµ                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                            â”‚\nâ”‚  ğŸ¯ Reward-Conditioned (Decision Transformer)                              â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚\nâ”‚  ì‹œí€€ìŠ¤: [Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚, aâ‚, Râ‚‚, sâ‚‚, aâ‚‚, ...]                        â”‚\nâ”‚           â†‘    â†‘    â†‘                                                      â”‚\nâ”‚          RTG  State Action                                                 â”‚\nâ”‚                                                                            â”‚\nâ”‚  ì˜ˆì¸¡:   Râ‚€ â†’ sâ‚€ â†’ aâ‚€?    (RTGì™€ Stateê°€ ì£¼ì–´ì§€ë©´ Action ì˜ˆì¸¡)             â”‚\nâ”‚               â†‘                                                            â”‚\nâ”‚          1::3 ìœ„ì¹˜ì—ì„œ ì¶”ì¶œ                                                â”‚\nâ”‚                                                                            â”‚\nâ”‚  íŠ¹ì§•:                                                                     â”‚\nâ”‚    â€¢ ëª©í‘œ RTGë¥¼ ì§€ì •í•˜ë©´ ê·¸ì— ë§ëŠ” í–‰ë™ ì˜ˆì¸¡                               â”‚\nâ”‚    â€¢ ë†’ì€ RTG â†’ ê³ ì„±ëŠ¥ ì •ì±…, ë‚®ì€ RTG â†’ ë³´ìˆ˜ì  ì •ì±…                        â”‚\nâ”‚    â€¢ \"ë‚˜ëŠ” 90ì ì„ ì›í•´!\" â†’ 90ì  ë‹¬ì„± ì‹œì˜ í–‰ë™ ì˜ˆì¸¡                        â”‚\nâ”‚                                                                            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                            â”‚\nâ”‚  ğŸ“‹ Naive (Behavior Cloning)                                               â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚\nâ”‚  ì‹œí€€ìŠ¤: [sâ‚€, aâ‚€, sâ‚, aâ‚, sâ‚‚, aâ‚‚, ...]                                     â”‚\nâ”‚           â†‘    â†‘                                                           â”‚\nâ”‚         State Action (RTG ì—†ìŒ!)                                           â”‚\nâ”‚                                                                            â”‚\nâ”‚  ì˜ˆì¸¡:   sâ‚€ â†’ aâ‚€?    (Stateë§Œ ë³´ê³  Action ì˜ˆì¸¡)                            â”‚\nâ”‚          â†‘                                                                 â”‚\nâ”‚     0::2 ìœ„ì¹˜ì—ì„œ ì¶”ì¶œ                                                     â”‚\nâ”‚                                                                            â”‚\nâ”‚  íŠ¹ì§•:                                                                     â”‚\nâ”‚    â€¢ RTG ì¡°ê±´ ì—†ì´ ë‹¨ìˆœ ëª¨ë°©                                               â”‚\nâ”‚    â€¢ ë°ì´í„° í‰ê·  ìˆ˜ì¤€ì˜ ì •ì±…                                               â”‚\nâ”‚    â€¢ \"ê·¸ëƒ¥ ë°ì´í„°ì—ì„œ ë³¸ ëŒ€ë¡œ í–‰ë™í• ê²Œ!\"                                   â”‚\nâ”‚                                                                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\"\"\"\nprint(comparison)\n\n# ê²Œì„ë³„ ëª©í‘œ RTG\nprint(\"\\nğŸ“ˆ ê²Œì„ë³„ ëª©í‘œ RTG (run_dt_atari.pyì—ì„œ ì‚¬ìš©):\")\ngame_rtgs = {\n    'Breakout': 90,\n    'Pong': 20,\n    'Qbert': 14000,\n    'Seaquest': 1150,\n}\nprint(f\"{'ê²Œì„':<15} {'ëª©í‘œ RTG':>10} {'ì„¤ëª…':<30}\")\nprint(\"-\"*60)\nfor game, rtg in game_rtgs.items():\n    if game == 'Breakout':\n        desc = \"ë²½ëŒ 90ê°œ ê¹¨ê¸°\"\n    elif game == 'Pong':\n        desc = \"21ì  ì¤‘ 20ì  íšë“\"\n    elif game == 'Qbert':\n        desc = \"ë†’ì€ ì ìˆ˜ ëª©í‘œ\"\n    else:\n        desc = \"ì ìˆ˜í•¨ ê²Œì„\"\n    print(f\"{game:<15} {rtg:>10} {desc:<30}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“Š ì‹œí€€ìŠ¤ êµ¬ì„± ì‹œê°í™”\n# ============================================================\n# ë‘ ëª¨ë“œì˜ ì‹œí€€ìŠ¤ êµ¬ì„±ì„ ì‹œê°ì ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤.\n# ============================================================\n\nprint(\"ğŸ“Š ì‹œí€€ìŠ¤ êµ¬ì„± ì‹œê°í™”\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 7))\n\n# ===== Reward-Conditioned =====\nrc_seq = ['Râ‚€', 'sâ‚€', 'aâ‚€', 'Râ‚', 'sâ‚', 'aâ‚', 'Râ‚‚', 'sâ‚‚', 'aâ‚‚']\nrc_colors = ['#ff6b6b', '#4ecdc4', '#45b7d1'] * 3  # ë¹¨ê°•, ì²­ë¡, íŒŒë‘\n\nax1 = axes[0]\nfor i, (token, color) in enumerate(zip(rc_seq, rc_colors)):\n    rect = plt.Rectangle((i, 0), 0.9, 1, facecolor=color, edgecolor='black', alpha=0.8)\n    ax1.add_patch(rect)\n    ax1.text(i + 0.45, 0.5, token, ha='center', va='center', fontsize=12, fontweight='bold')\n    \n    # State ìœ„ì¹˜ì—ì„œ ì˜ˆì¸¡ í™”ì‚´í‘œ (1::3)\n    if i % 3 == 1:\n        ax1.annotate('', xy=(i + 0.45, -0.15), xytext=(i + 0.45, -0.4),\n                    arrowprops=dict(arrowstyle='->', color='red', lw=2))\n        ax1.text(i + 0.45, -0.6, f'â†’a{i//3} ì˜ˆì¸¡', ha='center', color='red', fontsize=10, fontweight='bold')\n\nax1.set_xlim(-0.5, len(rc_seq) + 0.5)\nax1.set_ylim(-1, 1.5)\nax1.set_title('ğŸ¯ Reward-Conditioned: [R, s, a] íŠ¸ë¦¬í”Œ Ã— K\\nì˜ˆì¸¡ ìœ„ì¹˜: 1::3 (State ìœ„ì¹˜)', fontsize=14)\nax1.axis('off')\n\n# ===== Naive =====\nnaive_seq = ['sâ‚€', 'aâ‚€', 'sâ‚', 'aâ‚', 'sâ‚‚', 'aâ‚‚']\nnaive_colors = ['#4ecdc4', '#45b7d1'] * 3  # ì²­ë¡, íŒŒë‘ (RTG ì—†ìŒ)\n\nax2 = axes[1]\nfor i, (token, color) in enumerate(zip(naive_seq, naive_colors)):\n    rect = plt.Rectangle((i, 0), 0.9, 1, facecolor=color, edgecolor='black', alpha=0.8)\n    ax2.add_patch(rect)\n    ax2.text(i + 0.45, 0.5, token, ha='center', va='center', fontsize=12, fontweight='bold')\n    \n    # State ìœ„ì¹˜ì—ì„œ ì˜ˆì¸¡ í™”ì‚´í‘œ (0::2)\n    if i % 2 == 0:\n        ax2.annotate('', xy=(i + 0.45, -0.15), xytext=(i + 0.45, -0.4),\n                    arrowprops=dict(arrowstyle='->', color='red', lw=2))\n        ax2.text(i + 0.45, -0.6, f'â†’a{i//2} ì˜ˆì¸¡', ha='center', color='red', fontsize=10, fontweight='bold')\n\nax2.set_xlim(-0.5, len(naive_seq) + 0.5)\nax2.set_ylim(-1, 1.5)\nax2.set_title('ğŸ“‹ Naive (BC): [s, a] í˜ì–´ Ã— K\\nì˜ˆì¸¡ ìœ„ì¹˜: 0::2 (State ìœ„ì¹˜)', fontsize=14)\nax2.axis('off')\n\n# ë²”ë¡€\nfrom matplotlib.patches import Patch\nlegend_elements = [\n    Patch(facecolor='#ff6b6b', edgecolor='black', label='RTG (R)'),\n    Patch(facecolor='#4ecdc4', edgecolor='black', label='State (s)'),\n    Patch(facecolor='#45b7d1', edgecolor='black', label='Action (a)'),\n]\nfig.legend(handles=legend_elements, loc='upper right', ncol=3, fontsize=11)\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.85)\nplt.show()\n\nprint(\"\\nğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:\")\nprint(\"   â€¢ Reward-ConditionedëŠ” RTG í† í°ì´ ì¶”ê°€ë˜ì–´ ì‹œí€€ìŠ¤ê°€ 1.5ë°° ê¸¸ì–´ì§\")\nprint(\"   â€¢ ë‘ ëª¨ë“œ ëª¨ë‘ State ìœ„ì¹˜ì—ì„œ ë‹¤ìŒ Actionì„ ì˜ˆì¸¡\")\nprint(\"   â€¢ ì˜ˆì¸¡ ì¶”ì¶œ ì¸ë±ì‹±: RCëŠ” 1::3, NaiveëŠ” 0::2\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“Š ë‘ ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¹„êµ\n# ============================================================\n# Reward-Conditionedì™€ Naive ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.\n# ============================================================\n\nprint(\"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¹„êµ\")\nprint(\"=\"*60)\n\n# ë‘ ëª¨ë¸ ìƒì„±\nmodel_rc = AtariGPT(vocab_size=4, block_size=90, model_type='reward_conditioned')\nmodel_naive = AtariGPT(vocab_size=4, block_size=60, model_type='naive')  # 2/3 ê¸¸ì´\n\nparams_rc = sum(p.numel() for p in model_rc.parameters())\nparams_naive = sum(p.numel() for p in model_naive.parameters())\n\nprint(f\"\\n{'ëª¨ë¸':<25} {'íŒŒë¼ë¯¸í„° ìˆ˜':>15}\")\nprint(\"-\"*45)\nprint(f\"{'Reward-Conditioned':<25} {params_rc:>15,}\")\nprint(f\"{'Naive (BC)':<25} {params_naive:>15,}\")\nprint(f\"{'ì°¨ì´':<25} {params_rc - params_naive:>15,}\")\n\nprint(f\"\\nğŸ“ ì°¨ì´ ë°œìƒ ì´ìœ :\")\nprint(f\"   â€¢ Block size: RC={90} vs Naive={60}\")\nprint(f\"   â€¢ Position embedding í¬ê¸°ê°€ ë‹¤ë¦„\")\nprint(f\"   â€¢ RTG embedding ë ˆì´ì–´ (Naiveì—ëŠ” ì—†ìŒ)\")\nprint(f\"   â€¢ í•˜ì§€ë§Œ ë©”ì¸ Transformer íŒŒë¼ë¯¸í„°ëŠ” ë™ì¼\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ® ì¶”ë¡  ì‹œ RTG ì—…ë°ì´íŠ¸\n",
    "\n",
    "> Reward-Conditioned ëª¨ë“œì—ì„œëŠ” ì¶”ë¡  ì‹œ RTGë¥¼ ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "### RTG ì—…ë°ì´íŠ¸ ê³¼ì •\n",
    "\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant User as ğŸ¯ ëª©í‘œ ì„¤ì •\n",
    "    participant Model as ğŸ§  AtariGPT\n",
    "    participant Env as ğŸ® Breakout í™˜ê²½\n",
    "\n",
    "    Note over User: ëª©í‘œ: 90ì  (ë²½ëŒ 90ê°œ)\n",
    "\n",
    "    User->>Model: RTG = 90\n",
    "    Model->>Env: Action: ì˜¤ë¥¸ìª½\n",
    "    Env-->>Model: reward = 0 (ë¹—ë‚˜ê°)\n",
    "    Note over Model: RTG = 90 - 0 = 90\n",
    "\n",
    "    Model->>Env: Action: ê³µ ë°œì‚¬\n",
    "    Env-->>Model: reward = 5 (ë²½ëŒ 5ê°œ!)\n",
    "    Note over Model: RTG = 90 - 5 = 85\n",
    "\n",
    "    Model->>Env: Action: ì™¼ìª½\n",
    "    Env-->>Model: reward = 10 (ë²½ëŒ 10ê°œ!)\n",
    "    Note over Model: RTG = 85 - 10 = 75\n",
    "\n",
    "    Model->>Env: Action: ...\n",
    "    Note over Model: ...ê³„ì†...\n",
    "\n",
    "    Note over Model: RTG â‰ˆ 0 â†’ ëª©í‘œ ë‹¬ì„±! ğŸ‰\n",
    "```\n",
    "\n",
    "### ğŸ’¡ í•µì‹¬ ì›ë¦¬\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Principle[\"ğŸ”‘ RTG ì—…ë°ì´íŠ¸ ì›ë¦¬\"]\n",
    "        P1[\"RTG = ë‚¨ì€ ëª©í‘œ\"]\n",
    "        P2[\"rewardë¥¼ ë°›ìœ¼ë©´\\nRTG -= reward\"]\n",
    "        P3[\"ëª¨ë¸ì€ 'ë‚¨ì€ RTGë¥¼\\në‹¬ì„±í•˜ë ¤ë©´ ì–´ë–¤ í–‰ë™?'\\nì„ ì˜ˆì¸¡\"]\n",
    "        P4[\"RTGê°€ 0ì— ê°€ê¹Œì›Œì§€ë©´\\nëª©í‘œ ë‹¬ì„±! ğŸ‰\"]\n",
    "\n",
    "        P1 --> P2 --> P3 --> P4\n",
    "    end\n",
    "\n",
    "    subgraph Flow[\"ğŸ”„ ì¶”ë¡  ë£¨í”„\"]\n",
    "        direction LR\n",
    "        F1[\"ëª©í‘œ RTG\\nì„¤ì •\"] --> F2[\"State\\nê´€ì¸¡\"]\n",
    "        F2 --> F3[\"Action\\nì˜ˆì¸¡\"]\n",
    "        F3 --> F4[\"í™˜ê²½ì—ì„œ\\nì‹¤í–‰\"]\n",
    "        F4 --> F5[\"Reward\\níšë“\"]\n",
    "        F5 --> F6[\"RTG\\nì—…ë°ì´íŠ¸\"]\n",
    "        F6 -->|\"RTG > 0\"| F2\n",
    "        F6 -->|\"RTG â‰ˆ 0\"| Done[\"âœ… ì™„ë£Œ\"]\n",
    "    end\n",
    "\n",
    "    style Principle fill:#e3f2fd,stroke:#1565c0\n",
    "    style Flow fill:#e8f5e9,stroke:#2e7d32\n",
    "    style Done fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ® Breakout ê²Œì„ ì¶”ë¡  ì‹œë®¬ë ˆì´ì…˜\n# ============================================================\n# ì‹¤ì œ í™˜ê²½ ì—†ì´ RTG ì—…ë°ì´íŠ¸ ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.\n# ============================================================\n\ndef simulate_breakout_inference(target_rtg=90, max_steps=30):\n    \"\"\"\n    Breakout ê²Œì„ì—ì„œì˜ RTG ê¸°ë°˜ ì¶”ë¡  ì‹œë®¬ë ˆì´ì…˜\n    \n    Parameters:\n    -----------\n    target_rtg : int\n        ëª©í‘œ ì ìˆ˜ (ë²½ëŒ ê°œìˆ˜)\n    max_steps : int\n        ì‹œë®¬ë ˆì´ì…˜ ìµœëŒ€ ìŠ¤í…\n        \n    Returns:\n    --------\n    list: ê° ìŠ¤í…ì˜ ê¸°ë¡\n    \"\"\"\n    rtg = target_rtg\n    steps = []\n    \n    # Breakout ë³´ìƒ íŒ¨í„´ ì‹œë®¬ë ˆì´ì…˜\n    # ì‹¤ì œë¡œëŠ” ë²½ëŒì„ ê¹¨ë©´ 1~7ì  (ìƒ‰ìƒë³„)\n    reward_pattern = [0, 0, 0, 5, 0, 0, 0, 5, 0, 5,\n                      0, 0, 10, 0, 0, 5, 0, 0, 0, 10,\n                      0, 5, 0, 0, 5, 0, 0, 10, 0, 0]\n    \n    print(f\"ğŸ® Breakout ì¶”ë¡  ì‹œë®¬ë ˆì´ì…˜\")\n    print(f\"   ëª©í‘œ ì ìˆ˜: {target_rtg}\")\n    print(\"=\"*60)\n    \n    total_reward = 0\n    for t in range(min(max_steps, len(reward_pattern))):\n        reward = reward_pattern[t]\n        \n        # ê¸°ë¡\n        steps.append({\n            't': t,\n            'rtg_before': rtg,\n            'reward': reward,\n            'rtg_after': rtg - reward,\n            'total_reward': total_reward + reward\n        })\n        \n        # ë²½ëŒì„ ê¹¼ì„ ë•Œë§Œ ì¶œë ¥\n        if reward > 0:\n            action = np.random.choice(['â†', 'â†’', 'â—‹'])  # ê°€ìƒ ì•¡ì…˜\n            print(f\"Step {t:2d}: RTG={rtg:>3} â†’ Action={action} â†’ ë²½ëŒ íŒŒê´´! reward={reward:>2} â†’ ìƒˆ RTG={rtg-reward:>3}\")\n        \n        # RTG ì—…ë°ì´íŠ¸\n        rtg = rtg - reward\n        total_reward += reward\n        \n        # ëª©í‘œ ë‹¬ì„± ì²´í¬\n        if rtg <= 0:\n            print()\n            print(f\"ğŸ‰ ëª©í‘œ ë‹¬ì„±! (RTG â‰¤ 0)\")\n            break\n    \n    print(\"-\"*60)\n    print(f\"\\nğŸ“Š ê²°ê³¼ ìš”ì•½:\")\n    print(f\"   ì´ ìŠ¤í…: {len(steps)}\")\n    print(f\"   ì´ íšë“ ì ìˆ˜: {total_reward}\")\n    print(f\"   ëª©í‘œ ëŒ€ë¹„: {total_reward/target_rtg*100:.1f}%\")\n    print(f\"   ìµœì¢… RTG: {rtg}\")\n    \n    return steps\n\n# ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰\nsteps = simulate_breakout_inference(target_rtg=90)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“ˆ RTG ë³€í™” ì‹œê°í™”\n# ============================================================\n# ì¶”ë¡  ê³¼ì •ì—ì„œ RTGì™€ ëˆ„ì  ë³´ìƒì´ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ì‹œê°í™”í•©ë‹ˆë‹¤.\n# ============================================================\n\nprint(\"ğŸ“ˆ RTG ë³€í™” ì‹œê°í™”\")\nprint(\"=\"*60)\n\n# ë°ì´í„° ì¶”ì¶œ\ntimesteps_viz = [s['t'] for s in steps]\nrtgs_before = [s['rtg_before'] for s in steps]\nrewards = [s['reward'] for s in steps]\ntotal_rewards = [s['total_reward'] for s in steps]\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 8))\n\n# ===== RTG ë³€í™” =====\nax1 = axes[0]\nax1.plot(timesteps_viz, rtgs_before, 'o-', color='coral', linewidth=2, markersize=6, label='RTG')\nax1.axhline(y=0, color='green', linestyle='--', linewidth=2, label='Goal (RTG=0)')\nax1.fill_between(timesteps_viz, rtgs_before, 0, alpha=0.3, color='coral')\n\n# ë²½ëŒ íŒŒê´´ ì§€ì  í‘œì‹œ\nhit_steps = [s['t'] for s in steps if s['reward'] > 0]\nhit_rtgs = [s['rtg_before'] for s in steps if s['reward'] > 0]\nax1.scatter(hit_steps, hit_rtgs, color='red', s=100, zorder=5, label='Hit!')\n\nax1.set_ylabel('Return-to-Go', fontsize=12)\nax1.set_title('RTG Evolution During Inference (Breakout)', fontsize=14)\nax1.legend(loc='upper right')\nax1.grid(True, alpha=0.3)\nax1.set_xlim(-1, len(steps))\n\n# ===== ëˆ„ì  ë³´ìƒ =====\nax2 = axes[1]\n\n# ë³´ìƒ ë§‰ëŒ€ ê·¸ë˜í”„\ncolors = ['green' if r > 0 else 'lightgray' for r in rewards]\nax2.bar(timesteps_viz, rewards, color=colors, alpha=0.7, label='Instant Reward')\n\n# ëˆ„ì  ë³´ìƒ ì„  ê·¸ë˜í”„\nax2_twin = ax2.twinx()\nax2_twin.plot(timesteps_viz, total_rewards, 'b-', linewidth=2, marker='s', markersize=4, label='Cumulative Reward')\nax2_twin.set_ylabel('Cumulative Reward', fontsize=12, color='blue')\nax2_twin.tick_params(axis='y', labelcolor='blue')\n\nax2.set_xlabel('Step', fontsize=12)\nax2.set_ylabel('Instant Reward', fontsize=12)\nax2.set_title('Reward Accumulation (ë…¹ìƒ‰ = ë²½ëŒ íŒŒê´´)', fontsize=14)\nax2.set_xlim(-1, len(steps))\nax2.legend(loc='upper left')\nax2_twin.legend(loc='upper right')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# í•µì‹¬ ì¸ì‚¬ì´íŠ¸\nprint(\"\\nğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:\")\nprint(f\"   â€¢ RTGëŠ” rewardë¥¼ ë°›ì„ ë•Œë§ˆë‹¤ ê°ì†Œ (= ëª©í‘œì— ê°€ê¹Œì›Œì§)\")\nprint(f\"   â€¢ ëˆ„ì  ë³´ìƒì€ RTGì˜ ì—­ë°©í–¥ìœ¼ë¡œ ì¦ê°€\")\nprint(f\"   â€¢ RTG + ëˆ„ì ë³´ìƒ â‰ˆ ì´ˆê¸° ëª©í‘œ (= {steps[0]['rtg_before']})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# ğŸ“ ì—°ìŠµ ë¬¸ì œ\n\n> ì´í•´ë„ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ì—°ìŠµ ë¬¸ì œì…ë‹ˆë‹¤.\n\n## ì—°ìŠµ 1: CNN ì¶œë ¥ í¬ê¸° ê³„ì‚°\n\n**ë¬¸ì œ**: ì…ë ¥ ì´ë¯¸ì§€ê°€ 64Ã—64 (84Ã—84 ëŒ€ì‹ )ì¼ ë•Œ, CNN ì¶œë ¥ í¬ê¸°ëŠ”?\n\n```\nConv1: kernel=8, stride=4\nConv2: kernel=4, stride=2\nConv3: kernel=3, stride=1\n```\n\n**ê³µì‹**: `output = (input - kernel) / stride + 1`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ============================================================\n# ì—°ìŠµ 1: ì§ì ‘ ê³„ì‚°í•´ë³´ì„¸ìš”!\n# ============================================================\n# TODO: ì•„ë˜ ì½”ë“œë¥¼ ì™„ì„±í•˜ì„¸ìš”\n\ndef calc_output(input_size, kernel, stride):\n    \"\"\"Conv ì¶œë ¥ í¬ê¸° ê³„ì‚°\"\"\"\n    return (input_size - kernel) // stride + 1\n\n# 64x64 ì…ë ¥ì— ëŒ€í•œ ê³„ì‚°\nsize = 64\nprint(f\"ì…ë ¥: {size}x{size}\")\n\n# Conv1\n# size = calc_output(size, ???, ???)\n# print(f\"Conv1: {size}x{size}\")\n\n# Conv2\n# size = calc_output(size, ???, ???)\n# print(f\"Conv2: {size}x{size}\")\n\n# Conv3\n# size = calc_output(size, ???, ???)\n# print(f\"Conv3: {size}x{size}\")\n\n# print(f\"\\nFlatten: 64 Ã— {size} Ã— {size} = {64 * size * size}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“– ì—°ìŠµ 1 ì •ë‹µ\n# ============================================================\n# ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ì •ë‹µì„ í™•ì¸í•˜ì„¸ìš”.\n# ============================================================\n\nprint(\"ğŸ“– ì—°ìŠµ 1 ì •ë‹µ\")\nprint(\"=\"*40)\n\nsize = 64\nprint(f\"ì…ë ¥: {size}x{size}\")\n\n# Conv1: (64-8)/4 + 1 = 15\nsize = calc_output(size, 8, 4)\nprint(f\"Conv1 (k=8, s=4): {size}x{size}\")\n\n# Conv2: (15-4)/2 + 1 = 6\nsize = calc_output(size, 4, 2)\nprint(f\"Conv2 (k=4, s=2): {size}x{size}\")\n\n# Conv3: (6-3)/1 + 1 = 4\nsize = calc_output(size, 3, 1)\nprint(f\"Conv3 (k=3, s=1): {size}x{size}\")\n\nprint(f\"\\nFlatten: 64 Ã— {size} Ã— {size} = {64 * size * size}\")\nprint(f\"\\nğŸ’¡ 84x84ì¼ ë•Œì˜ 3136ê³¼ ë¹„êµí•˜ë©´ {64 * size * size}ì€ ë” ì‘ìŠµë‹ˆë‹¤!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ì—°ìŠµ 2: ëª¨ë“œë³„ ì‹œí€€ìŠ¤ ê¸¸ì´\n\n**ë¬¸ì œ**: `context_length = 50`ì¼ ë•Œ, ê° ëª¨ë“œì˜ ì‹¤ì œ í† í° ì‹œí€€ìŠ¤ ê¸¸ì´ëŠ”?\n\n| ëª¨ë“œ | ì‹œí€€ìŠ¤ êµ¬ì„± | ê¸¸ì´ ê³„ì‚° | ì •ë‹µ |\n|:---|:---|:---|:---|\n| Reward-Conditioned | [R, s, a, ...] | 50 Ã— ? | ? |\n| Naive | [s, a, ...] | 50 Ã— ? | ? |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ============================================================\n# ì—°ìŠµ 2: ì§ì ‘ ê³„ì‚°í•´ë³´ì„¸ìš”!\n# ============================================================\n\ncontext_length = 50\n\n# TODO: ê° ëª¨ë“œì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ê³„ì‚°í•˜ì„¸ìš”\n# reward_conditioned_seq_len = context_length * ???\n# naive_seq_len = context_length * ???\n\n# print(f\"Reward-Conditioned: {reward_conditioned_seq_len}\")\n# print(f\"Naive: {naive_seq_len}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“– ì—°ìŠµ 2 ì •ë‹µ\n# ============================================================\n\nprint(\"ğŸ“– ì—°ìŠµ 2 ì •ë‹µ\")\nprint(\"=\"*40)\n\ncontext_length = 50\n\n# Reward-conditioned: [R, s, a] íŠ¸ë¦¬í”Œ â†’ 3ë°°\nreward_conditioned_seq_len = context_length * 3\n\n# Naive: [s, a] í˜ì–´ â†’ 2ë°°\nnaive_seq_len = context_length * 2\n\nprint(f\"Context length: {context_length}\")\nprint()\nprint(f\"Reward-Conditioned:\")\nprint(f\"   ì‹œí€€ìŠ¤ êµ¬ì„±: [R, s, a, R, s, a, ...]\")\nprint(f\"   ê¸¸ì´: {context_length} Ã— 3 = {reward_conditioned_seq_len}\")\nprint()\nprint(f\"Naive (BC):\")\nprint(f\"   ì‹œí€€ìŠ¤ êµ¬ì„±: [s, a, s, a, ...]\")\nprint(f\"   ê¸¸ì´: {context_length} Ã— 2 = {naive_seq_len}\")\nprint()\nprint(f\"ğŸ’¡ RCê°€ Naiveë³´ë‹¤ {reward_conditioned_seq_len - naive_seq_len}ê°œ í† í° ë” ë§ìŒ!\")\nprint(f\"   (RTG í† í° {context_length}ê°œê°€ ì¶”ê°€ë¨)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ì´ ì…€ì€ ì‚­ì œë¨ (ì—°ìŠµë¬¸ì œ ì •ë‹µê³¼ ë³‘í•©)\npass"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ“ Phase 4 ì™„ë£Œ!\n",
    "\n",
    "## ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! ëª¨ë“  Phaseë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ì´ Phaseì—ì„œ ë°°ìš´ ë‚´ìš©\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Summary[\"ğŸ“š Phase 4 í•™ìŠµ ìš”ì•½\"]\n",
    "        direction TB\n",
    "\n",
    "        subgraph S1[\"1. DQN Replay Buffer\"]\n",
    "            S1A[\"50ê°œ checkpoint\\nì´ˆë³´â†’ì „ë¬¸ê°€ ì •ì±…\"]\n",
    "            S1B[\"ê° 100,000 transitions\"]\n",
    "            S1C[\"4 stacked frames (84Ã—84Ã—4)\"]\n",
    "        end\n",
    "\n",
    "        subgraph S2[\"2. RTG ê³„ì‚°\"]\n",
    "            S2A[\"ì—í”¼ì†Œë“œ ê²½ê³„ ì¡´ì¤‘\"]\n",
    "            S2B[\"done_idxsë¡œ ê²½ê³„ ê´€ë¦¬\"]\n",
    "            S2C[\"êµ¬ê°„ ë‚´ ëˆ„ì í•©\"]\n",
    "        end\n",
    "\n",
    "        subgraph S3[\"3. CNN Encoder\"]\n",
    "            S3A[\"(4,84,84) â†’ ConvÃ—3 â†’ (3136) â†’ (128)\"]\n",
    "        end\n",
    "\n",
    "        subgraph S4[\"4. AtariGPT\"]\n",
    "            S4A[\"CNN + Transformer\"]\n",
    "            S4B[\"RC: [R,s,a]Ã—K / BC: [s,a]Ã—K\"]\n",
    "            S4C[\"Cross-Entropy Loss\"]\n",
    "        end\n",
    "\n",
    "        subgraph S5[\"5. ì¶”ë¡  ì‹œ RTG\"]\n",
    "            S5A[\"ì´ˆê¸°: RTG = ëª©í‘œ ì ìˆ˜\"]\n",
    "            S5B[\"ë§¤ ìŠ¤í…: RTG -= reward\"]\n",
    "            S5C[\"ì¢…ë£Œ: RTG â‰ˆ 0\"]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    style S1 fill:#e3f2fd,stroke:#1565c0\n",
    "    style S2 fill:#fff3e0,stroke:#ef6c00\n",
    "    style S3 fill:#e8f5e9,stroke:#2e7d32\n",
    "    style S4 fill:#f3e5f5,stroke:#7b1fa2\n",
    "    style S5 fill:#fff9c4,stroke:#f9a825\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Phase 4 ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "| ì™„ë£Œ | í•­ëª© |\n",
    "|:---:|:---|\n",
    "| â˜ | DQN replay buffer êµ¬ì¡° ì´í•´ |\n",
    "| â˜ | 4 frame stackì˜ ì˜ë¯¸ ì´í•´ |\n",
    "| â˜ | Atari ìŠ¤íƒ€ì¼ RTG ê³„ì‚° (ì—í”¼ì†Œë“œ ê²½ê³„) ì´í•´ |\n",
    "| â˜ | StateActionReturnDataset ë™ì‘ ì´í•´ |\n",
    "| â˜ | CNN Encoder êµ¬ì¡° ë° ì¶œë ¥ í¬ê¸° ê³„ì‚° |\n",
    "| â˜ | AtariGPT ì „ì²´ êµ¬ì¡° ì´í•´ |\n",
    "| â˜ | Reward-Conditioned vs Naive ì°¨ì´ ì´í•´ |\n",
    "| â˜ | ì¶”ë¡  ì‹œ RTG ë™ì  ì—…ë°ì´íŠ¸ ì´í•´ |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ ë‹¤ìŒ ë‹¨ê³„ (ì„ íƒ)\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Next[\"ğŸš€ ë‹¤ìŒ ë‹¨ê³„\"]\n",
    "        direction TB\n",
    "        N1[\"1ï¸âƒ£ ì‹¤ì œ ë°ì´í„°ë¡œ í•™ìŠµ\\ngsutilë¡œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ\\nrun_dt_atari.py ì‹¤í–‰\"]\n",
    "        N2[\"2ï¸âƒ£ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‹¤í—˜\\ncontext_length: 30â†’50\\nn_layer: 6â†’12\\nn_head: 8â†’4\"]\n",
    "        N3[\"3ï¸âƒ£ ë‹¤ë¥¸ ê²Œì„ ì‹œë„\\nPong, Qbert, Seaquest\\nê²Œì„ë³„ ëª©í‘œ RTG ì¡°ì •\"]\n",
    "        N4[\"4ï¸âƒ£ ê´€ë ¨ ë…¼ë¬¸ ì½ê¸°\\nOnline DT\\nTrajectory Transformer\\nConservative DT\"]\n",
    "    end\n",
    "\n",
    "    style N1 fill:#e3f2fd,stroke:#1565c0\n",
    "    style N2 fill:#fff3e0,stroke:#ef6c00\n",
    "    style N3 fill:#e8f5e9,stroke:#2e7d32\n",
    "    style N4 fill:#f3e5f5,stroke:#7b1fa2\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ºï¸ ì „ì²´ í•™ìŠµ ë¡œë“œë§µ ì™„ë£Œ!\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    P1[\"âœ… Phase 1\\nRL & Transformer\\nê¸°ì´ˆ\"] --> P2[\"âœ… Phase 2\\nRTG & ì‹œí€€ìŠ¤\\nêµ¬ì„±\"]\n",
    "    P2 --> P3[\"âœ… Phase 3\\nGym ì‹¤ìŠµ\\n(ì—°ì†)\"]\n",
    "    P3 --> P4[\"âœ… Phase 4\\nAtari ì‹¤ìŠµ\\n(ì´ë¯¸ì§€)\"]\n",
    "\n",
    "    style P1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px\n",
    "    style P2 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px\n",
    "    style P3 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px\n",
    "    style P4 fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ‰ **ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!**  \n",
    "> Decision Transformerì˜ ì „ì²´ êµ¬ì¡°ì™€ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•˜ì…¨ìŠµë‹ˆë‹¤.  \n",
    "> ì´ì œ ì‹¤ì œ ë°ì´í„°ë¡œ í•™ìŠµí•˜ê³  ì‹¤í—˜í•´ë³´ì„¸ìš”!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}