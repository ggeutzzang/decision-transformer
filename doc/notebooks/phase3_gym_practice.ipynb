{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéÆ Phase 3: Gym ÌôòÍ≤Ω Ïã§Ïäµ\n",
    "\n",
    "> **Î™©Ìëú**: Ïã§Ï†ú Decision Transformer ÏΩîÎìúÎ•º Î∂ÑÏÑùÌïòÍ≥†, Gym ÌôòÍ≤ΩÏóêÏÑú ÌïôÏäµ Î∞è Ï∂îÎ°† Í≥ºÏ†ïÏùÑ Ïã§ÏäµÌï©ÎãàÎã§.\n",
    "\n",
    "---\n",
    "\n",
    "## üó∫Ô∏è Ïù¥ PhaseÏóêÏÑú Î∞∞Ïö∏ ÎÇ¥Ïö©\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Phase3[\"üéÆ Phase 3: Gym ÌôòÍ≤Ω Ïã§Ïäµ\"]\n",
    "        direction LR\n",
    "        subgraph S1[\"üìÇ Îç∞Ïù¥ÌÑ∞ÏÖã ÌÉêÏÉâ\"]\n",
    "            D1[\"Í∂§Ï†Å Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞\"]\n",
    "            D2[\"State / Action / Reward\"]\n",
    "            D3[\"RTG Í≥ÑÏÇ∞\"]\n",
    "        end\n",
    "\n",
    "        subgraph S2[\"üîç Î™®Îç∏ ÏΩîÎìú Î∂ÑÏÑù\"]\n",
    "            M1[\"Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò\"]\n",
    "            M2[\"ÏûÑÎ≤†Îî© Î†àÏù¥Ïñ¥\"]\n",
    "            M3[\"Forward Pass\"]\n",
    "        end\n",
    "\n",
    "        subgraph S3[\"üöÄ ÌïôÏäµ & Ï∂îÎ°†\"]\n",
    "            T1[\"Î∞∞Ïπò ÏÉùÏÑ±\"]\n",
    "            T2[\"ÏÜêÏã§ Ìï®Ïàò MSE\"]\n",
    "            T3[\"RTG Ï°∞Í±¥Î∂Ä Ï∂îÎ°†\"]\n",
    "        end\n",
    "\n",
    "        S1 --> S2 --> S3\n",
    "    end\n",
    "\n",
    "    style S1 fill:#e3f2fd\n",
    "    style S2 fill:#fff3e0\n",
    "    style S3 fill:#e8f5e9\n",
    "```\n",
    "\n",
    "## üìã Î™©Ï∞®\n",
    "\n",
    "| ÏÑπÏÖò | Ï£ºÏ†ú | ÌïôÏäµ ÎÇ¥Ïö© |\n",
    "|:---:|:---|:---|\n",
    "| **1** | [ÌôòÍ≤Ω ÏÑ§Ï†ï](#1-ÌôòÍ≤Ω-ÏÑ§Ï†ï) | Python Ìå®ÌÇ§ÏßÄ ÌôïÏù∏ Î∞è Í≤ΩÎ°ú ÏÑ§Ï†ï |\n",
    "| **2** | [Îç∞Ïù¥ÌÑ∞ÏÖã ÌÉêÏÉâ](#2-Îç∞Ïù¥ÌÑ∞ÏÖã-ÌÉêÏÉâ) | D4RL Îç∞Ïù¥ÌÑ∞ÏÖã Íµ¨Ï°∞ÏôÄ ÌÜµÍ≥Ñ Î∂ÑÏÑù |\n",
    "| **3** | [Î™®Îç∏ ÏΩîÎìú Î∂ÑÏÑù](#3-Î™®Îç∏-ÏΩîÎìú-Î∂ÑÏÑù) | DecisionTransformer ÎÇ¥Î∂Ä Íµ¨Ï°∞ Ïù¥Ìï¥ |\n",
    "| **4** | [ÌïôÏäµ Ïã§Ìñâ](#4-ÌïôÏäµ-Ïã§Ìñâ) | Î∞∞Ïπò ÏÉùÏÑ±Í≥º ÌïôÏäµ Î£®ÌîÑ Íµ¨ÌòÑ |\n",
    "| **5** | [ÌèâÍ∞Ä Í≥ºÏ†ï Ïù¥Ìï¥](#5-ÌèâÍ∞Ä-Í≥ºÏ†ï-Ïù¥Ìï¥) | RTG Ï°∞Í±¥Î∂Ä Ï∂îÎ°† Î©îÏª§ÎãàÏ¶ò |\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Phase Ïó∞Í≤∞\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    P1[\"Phase 1\\nüìñ Î∞∞Í≤ΩÏßÄÏãù\\nRL/Transformer\\nÍ∏∞Ï¥à Í∞úÎÖê\"] --> P2[\"Phase 2\\nüß† ÌïµÏã¨Í∞úÎÖê\\nRTG/ÏãúÌÄÄÏä§ Íµ¨ÏÑ±\\nDTÏùò ÏûëÎèô ÏõêÎ¶¨\"]\n",
    "    P2 --> P3[\"Phase 3\\nüéÆ Gym Ïã§Ïäµ\\nÏã§Ï†ú ÏΩîÎìú Ïã§Ìñâ\\nÌïôÏäµ & Ï∂îÎ°† Ïã§Ïäµ\"]\n",
    "    P3 --> P4[\"Phase 4\\nüëæ Atari\\nÏù¥ÎØ∏ÏßÄ ÌôòÍ≤Ω\\nÍ≥†Í∏â ÏùëÏö©\"]\n",
    "\n",
    "    style P3 fill:#ffeb3b,stroke:#f57f17,stroke-width:3px\n",
    "    style P1 fill:#e3f2fd\n",
    "    style P2 fill:#e8f5e9\n",
    "    style P4 fill:#fce4ec\n",
    "```\n",
    "\n",
    "> üí° **Gym ÌôòÍ≤ΩÏù¥ÎûÄ?**  \n",
    "> OpenAI GymÏùÄ Í∞ïÌôîÌïôÏäµ Ïó∞Íµ¨Î•º ÏúÑÌïú ÌëúÏ§Ä ÌôòÍ≤Ω ÎùºÏù¥Î∏åÎü¨Î¶¨ÏûÖÎãàÎã§.  \n",
    "> **Hopper**, **HalfCheetah**, **Walker2d** Îì± Î¨ºÎ¶¨ ÏãúÎÆ¨Î†àÏù¥ÏÖò Í∏∞Î∞ò Î°úÎ¥á Ï†úÏñ¥ ÌÉúÏä§ÌÅ¨Î•º Ï†úÍ≥µÌï©ÎãàÎã§.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ ÏÇ¨Ï†Ñ ÏöîÍµ¨ÏÇ¨Ìï≠\n",
    "\n",
    "```python\n",
    "# ÌïÑÏöîÌïú Ìå®ÌÇ§ÏßÄ\n",
    "- Python 3.7+\n",
    "- PyTorch 1.8+\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "- transformers (Hugging Face)\n",
    "\n",
    "# ÏÑ†ÌÉùÏ†Å (Ïã§Ï†ú ÌôòÍ≤Ω Ïã§Ìñâ Ïãú)\n",
    "- MuJoCo\n",
    "- d4rl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. ÌôòÍ≤Ω ÏÑ§Ï†ï\n",
    "\n",
    "## üéØ Î™©Ìëú\n",
    "> Python ÌôòÍ≤ΩÏùÑ ÌôïÏù∏ÌïòÍ≥†, Decision Transformer ÏΩîÎìúÏóê Ï†ëÍ∑ºÌï† Ïàò ÏûàÎèÑÎ°ù Í≤ΩÎ°úÎ•º ÏÑ§Ï†ïÌï©ÎãàÎã§.\n",
    "\n",
    "## üì¶ ÌïÑÏöîÌïú Ìå®ÌÇ§ÏßÄ Íµ¨Ï°∞\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Project[\"üìÅ decision-transformer/\"]\n",
    "        direction TB\n",
    "        subgraph Gym[\"üìÇ gym/ ‚Üê Ïù¥Î≤à PhaseÏóêÏÑú ÏÇ¨Ïö©\"]\n",
    "            direction TB\n",
    "            subgraph DT_Mod[\"decision_transformer/\"]\n",
    "                direction TB\n",
    "                Models[\"models/\\n‚îú decision_transformer.py ‚Üê ÌïµÏã¨ Î™®Îç∏\\n‚îî trajectory_gpt2.py ‚Üê GPT-2 Í∏∞Î∞ò\"]\n",
    "                Training[\"training/\\n‚îî seq_trainer.py ‚Üê ÌïôÏäµ ÏΩîÎìú\"]\n",
    "                Eval[\"evaluation/\\n‚îî evaluate_episodes.py ‚Üê ÌèâÍ∞Ä ÏΩîÎìú\"]\n",
    "            end\n",
    "            Data[\"data/\\n‚îî hopper-medium-v2.pkl ‚Üê D4RL Îç∞Ïù¥ÌÑ∞ÏÖã\"]\n",
    "            Exp[\"experiment.py ‚Üê Î©îÏù∏ Ïã§Ìóò Ïä§ÌÅ¨Î¶ΩÌä∏\"]\n",
    "        end\n",
    "        Notebooks[\"doc/notebooks/ ‚Üê ÌòÑÏû¨ ÏúÑÏπò\"]\n",
    "    end\n",
    "\n",
    "    style Gym fill:#e8f5e9,stroke:#4caf50,stroke-width:2px\n",
    "    style DT_Mod fill:#fff3e0\n",
    "    style Notebooks fill:#ffeb3b\n",
    "```\n",
    "\n",
    "## üîß Ìå®ÌÇ§ÏßÄ Ïó≠Ìï†\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Packages[\"ÌïµÏã¨ Ìå®ÌÇ§ÏßÄ\"]\n",
    "        direction TB\n",
    "        PT[\"üî• PyTorch\\nÎ™®Îç∏ Ï†ïÏùò & ÌïôÏäµ\"] --> TF[\"ü§ó Transformers\\nGPT-2 Î™®Îç∏ ÏÇ¨Ïö©\"]\n",
    "        NP[\"üìä NumPy\\nÎç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨\"] --> MPL[\"üìà Matplotlib\\nÏãúÍ∞ÅÌôî\"]\n",
    "    end\n",
    "\n",
    "    subgraph Optional[\"ÏÑ†ÌÉù Ìå®ÌÇ§ÏßÄ\"]\n",
    "        MJ[\"ü§ñ MuJoCo\\nÎ¨ºÎ¶¨ ÏãúÎÆ¨Î†àÏù¥ÏÖò\"]\n",
    "        D4[\"üì¶ D4RL\\nÎç∞Ïù¥ÌÑ∞ÏÖã Îã§Ïö¥Î°úÎìú\"]\n",
    "    end\n",
    "\n",
    "    style Packages fill:#e3f2fd\n",
    "    style Optional fill:#fce4ec\n",
    "```\n",
    "\n",
    "ÏïÑÎûò ÏÖÄÏóêÏÑú ÌïÑÏöîÌïú Ìå®ÌÇ§ÏßÄÍ∞Ä ÏÑ§ÏπòÎêòÏñ¥ ÏûàÎäîÏßÄ ÌôïÏù∏Ìï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêç Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "\n",
      "‚úÖ PyTorch: 2.9.0+cpu\n",
      "   ‚îî‚îÄ CUDA available: False\n",
      "‚úÖ NumPy: 2.0.2\n",
      "‚úÖ Matplotlib: 3.10.0\n",
      "‚úÖ Transformers: 5.0.0\n",
      "\n",
      "==================================================\n",
      "Î™®Îì† ÌïÑÏàò Ìå®ÌÇ§ÏßÄÍ∞Ä ÏÑ§ÏπòÎêòÏóàÎã§Î©¥ Îã§Ïùå ÏÖÄÎ°ú ÏßÑÌñâÌïòÏÑ∏Ïöî!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üì¶ Ìå®ÌÇ§ÏßÄ Î≤ÑÏ†Ñ ÌôïÏù∏\n",
    "# ============================================================\n",
    "# Decision Transformer Ïã§ÌñâÏóê ÌïÑÏöîÌïú ÌïµÏã¨ Ìå®ÌÇ§ÏßÄÎì§ÏûÖÎãàÎã§.\n",
    "# Í∞Å Ìå®ÌÇ§ÏßÄÏùò Ïó≠Ìï†:\n",
    "#   - PyTorch: Îî•Îü¨Îãù ÌîÑÎ†àÏûÑÏõåÌÅ¨ (Î™®Îç∏ Ï†ïÏùò Î∞è ÌïôÏäµ)\n",
    "#   - NumPy: ÏàòÏπò Í≥ÑÏÇ∞ (Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨)\n",
    "#   - Transformers: Hugging FaceÏùò GPT-2 Î™®Îç∏ ÏÇ¨Ïö©\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "print(f\"üêç Python: {sys.version}\")\n",
    "print()\n",
    "\n",
    "# PyTorch ÌôïÏù∏\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "    print(f\"   ‚îî‚îÄ CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   ‚îî‚îÄ CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå PyTorch not installed!\")\n",
    "    print(\"   ÏÑ§Ïπò: pip install torch\")\n",
    "\n",
    "# NumPy ÌôïÏù∏\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"‚úÖ NumPy: {np.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå NumPy not installed!\")\n",
    "    print(\"   ÏÑ§Ïπò: pip install numpy\")\n",
    "\n",
    "# Matplotlib ÌôïÏù∏\n",
    "try:\n",
    "    import matplotlib\n",
    "    print(f\"‚úÖ Matplotlib: {matplotlib.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Matplotlib not installed!\")\n",
    "    print(\"   ÏÑ§Ïπò: pip install matplotlib\")\n",
    "\n",
    "# Transformers ÌôïÏù∏\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"‚úÖ Transformers: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Transformers not installed!\")\n",
    "    print(\"   ÏÑ§Ïπò: pip install transformers\")\n",
    "    print(\"   (ÏóÜÏñ¥ÎèÑ SimpleDTÎ°ú Ïã§Ïäµ Í∞ÄÎä•)\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*50)\n",
    "print(\"Î™®Îì† ÌïÑÏàò Ìå®ÌÇ§ÏßÄÍ∞Ä ÏÑ§ÏπòÎêòÏóàÎã§Î©¥ Îã§Ïùå ÏÖÄÎ°ú ÏßÑÌñâÌïòÏÑ∏Ïöî!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Í≤ΩÎ°ú ÏÑ§Ï†ï Í≤∞Í≥º:\n",
      "   Project root: /content/decision-transformer\n",
      "   Gym path:     /content/decision-transformer/gym\n",
      "\n",
      "‚úÖ Gym Í≤ΩÎ°ú Ï°¥Ïû¨ ÌôïÏù∏!\n",
      "‚úÖ decision_transformer Î™®Îìà Ï°¥Ïû¨ ÌôïÏù∏!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìÇ ÌîÑÎ°úÏ†ùÌä∏ Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "# ============================================================\n",
    "# PythonÏù¥ gym/ Ìè¥ÎçîÏùò Î™®ÎìàÏùÑ Ï∞æÏùÑ Ïàò ÏûàÎèÑÎ°ù Í≤ΩÎ°úÎ•º Ï∂îÍ∞ÄÌï©ÎãàÎã§.\n",
    "#\n",
    "# ÌôòÍ≤Ω ÏûêÎèô Í∞êÏßÄ:\n",
    "#   - Google Colab: git clone ÌõÑ /content/decision-transformer ÏÇ¨Ïö©\n",
    "#   - Î°úÏª¨ ÌôòÍ≤Ω: ÎÖ∏Ìä∏Î∂Å ÏúÑÏπòÏóêÏÑú ÏÉÅÎåÄ Í≤ΩÎ°úÎ°ú ÏûêÎèô ÌÉêÏÉâ\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def find_project_root():\n",
    "    \"\"\"ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏Î•º ÏûêÎèôÏúºÎ°ú ÌÉêÏÉâÌï©ÎãàÎã§.\"\"\"\n",
    "\n",
    "    # 1) Colab ÌôòÍ≤Ω Í∞êÏßÄ\n",
    "    if 'google.colab' in sys.modules:\n",
    "        colab_path = '/content/decision-transformer'\n",
    "        if not os.path.exists(colab_path):\n",
    "            print(\"üì• Colab ÌôòÍ≤Ω Í∞êÏßÄ! Î†àÌè¨Î•º ÌÅ¥Î°†Ìï©ÎãàÎã§...\")\n",
    "            os.system('git clone https://github.com/ggeutzzang/decision-transformer.git /content/decision-transformer')\n",
    "        return colab_path\n",
    "\n",
    "    # 2) Î°úÏª¨ ÌôòÍ≤Ω: ÌòÑÏû¨ ÎîîÎ†âÌÜ†Î¶¨ÏóêÏÑú ÏÉÅÏúÑÎ°ú Ïò¨ÎùºÍ∞ÄÎ©∞ ÌÉêÏÉâ\n",
    "    #    'gym/' Í≥º 'atari/' Ìè¥ÎçîÍ∞Ä Î™®Îëê ÏûàÎäî ÎîîÎ†âÌÜ†Î¶¨Î•º ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏Î°ú ÌåêÎã®\n",
    "    search_dir = os.path.abspath(os.getcwd())\n",
    "    for _ in range(5):  # ÏµúÎåÄ 5Îã®Í≥Ñ ÏÉÅÏúÑÍπåÏßÄ ÌÉêÏÉâ\n",
    "        if os.path.isdir(os.path.join(search_dir, 'gym')) and \\\n",
    "           os.path.isdir(os.path.join(search_dir, 'atari')):\n",
    "            return search_dir\n",
    "        search_dir = os.path.dirname(search_dir)\n",
    "\n",
    "    # 3) Î™ª Ï∞æÏúºÎ©¥ Í∏∞Î≥∏Í∞í\n",
    "    return os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "GYM_PATH = os.path.join(PROJECT_ROOT, 'gym')\n",
    "\n",
    "# sys.pathÏóê Í≤ΩÎ°ú Ï∂îÍ∞Ä (Ï§ëÎ≥µ Î∞©ÏßÄ)\n",
    "# Ïù¥Î†áÍ≤å ÌïòÎ©¥ `from decision_transformer.models import ...` Í∞Ä Í∞ÄÎä•Ìï¥Ïßê\n",
    "if GYM_PATH not in sys.path:\n",
    "    sys.path.insert(0, GYM_PATH)\n",
    "\n",
    "print(\"üìÇ Í≤ΩÎ°ú ÏÑ§Ï†ï Í≤∞Í≥º:\")\n",
    "print(f\"   Project root: {PROJECT_ROOT}\")\n",
    "print(f\"   Gym path:     {GYM_PATH}\")\n",
    "print()\n",
    "\n",
    "# Í≤ΩÎ°ú Í≤ÄÏ¶ù\n",
    "if os.path.exists(GYM_PATH):\n",
    "    print(\"‚úÖ Gym Í≤ΩÎ°ú Ï°¥Ïû¨ ÌôïÏù∏!\")\n",
    "    dt_path = os.path.join(GYM_PATH, 'decision_transformer')\n",
    "    if os.path.exists(dt_path):\n",
    "        print(\"‚úÖ decision_transformer Î™®Îìà Ï°¥Ïû¨ ÌôïÏù∏!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è decision_transformer Î™®ÎìàÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\")\n",
    "else:\n",
    "    print(\"‚ùå Gym Í≤ΩÎ°úÍ∞Ä Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§!\")\n",
    "    print(\"   ÎÖ∏Ìä∏Î∂Å ÏúÑÏπòÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Îç∞Ïù¥ÌÑ∞ÏÖã ÌÉêÏÉâ\n",
    "\n",
    "## üéØ Î™©Ìëú\n",
    "> D4RL Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Íµ¨Ï°∞Î•º Ïù¥Ìï¥ÌïòÍ≥†, Í∞ïÌôîÌïôÏäµ Í∂§Ï†Å(trajectory) Îç∞Ïù¥ÌÑ∞Í∞Ä Ïñ¥ÎñªÍ≤å Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÎäîÏßÄ ÌååÏïÖÌï©ÎãàÎã§.\n",
    "\n",
    "## üìö D4RLÏù¥ÎûÄ?\n",
    "\n",
    "**D4RL** (Datasets for Deep Data-Driven Reinforcement Learning)ÏùÄ Ïò§ÌîÑÎùºÏù∏ Í∞ïÌôîÌïôÏäµ Ïó∞Íµ¨Î•º ÏúÑÌïú ÌëúÏ§Ä Îç∞Ïù¥ÌÑ∞ÏÖãÏûÖÎãàÎã§.\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph D4RL[\"üì¶ D4RL Îç∞Ïù¥ÌÑ∞ÏÖã\"]\n",
    "        direction LR\n",
    "        subgraph Envs[\"üèãÔ∏è ÌôòÍ≤Ω\"]\n",
    "            E1[\"hopper\"]\n",
    "            E2[\"halfcheetah\"]\n",
    "            E3[\"walker2d\"]\n",
    "        end\n",
    "\n",
    "        subgraph Quality[\"üìä Îç∞Ïù¥ÌÑ∞ ÌíàÏßà\"]\n",
    "            Q1[\"medium\\nÏ§ëÍ∞Ñ ÏàòÏ§Ä Ï†ïÏ±Ö\"]\n",
    "            Q2[\"medium-replay\\nÌïôÏäµ Ï§ë replay buffer\"]\n",
    "            Q3[\"medium-expert\\nmedium + expert ÌòºÌï©\"]\n",
    "            Q4[\"expert\\nÏ†ÑÎ¨∏Í∞Ä Ï†ïÏ±Ö\"]\n",
    "        end\n",
    "\n",
    "        Envs --> Quality\n",
    "    end\n",
    "\n",
    "    subgraph Naming[\"üìù ÌååÏùºÎ™Ö Í∑úÏπô\"]\n",
    "        direction LR\n",
    "        N1[\"hopper\"] --- N2[\"-\"] --- N3[\"medium\"] --- N4[\"-v2\"] --- N5[\".pkl\"]\n",
    "        N1b[\"ÌôòÍ≤Ω\"] ~~~ N3b[\"Îç∞Ïù¥ÌÑ∞ÏÖã Ï¢ÖÎ•ò\"] ~~~ N4b[\"Î≤ÑÏ†Ñ\"]\n",
    "    end\n",
    "\n",
    "    style D4RL fill:#e3f2fd\n",
    "    style Envs fill:#fff3e0\n",
    "    style Quality fill:#e8f5e9\n",
    "    style Naming fill:#f3e5f5\n",
    "```\n",
    "\n",
    "## ü¶ø Hopper ÌôòÍ≤Ω\n",
    "\n",
    "Ïù¥Î≤à Ïã§ÏäµÏóêÏÑú ÏÇ¨Ïö©Ìï† **Hopper**Îäî Ìïú Îã§Î¶¨Î°ú Îõ∞Îäî Î°úÎ¥áÏûÖÎãàÎã§.\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Hopper[\"ü¶ø Hopper Î°úÎ¥á\"]\n",
    "        direction TB\n",
    "        Torso[\"‚óè Î™∏ÌÜµ torso\"] --> Thigh[\"‚îÇ ÌóàÎ≤ÖÏßÄ thigh\"]\n",
    "        Thigh --> Leg[\"‚îÇ Ï†ïÍ∞ïÏù¥ leg\"]\n",
    "        Leg --> Foot[\"‚îÄ‚îÄ Î∞ú foot\"]\n",
    "    end\n",
    "\n",
    "    subgraph Specs[\"üìã ÌôòÍ≤Ω Ïä§Ìéô\"]\n",
    "        direction TB\n",
    "        SP1[\"State Ï∞®Ïõê: 11\\nÏúÑÏπò, ÏÜçÎèÑ, Í∞ÅÎèÑ Îì±\"]\n",
    "        SP2[\"Action Ï∞®Ïõê: 3\\nÍ∞Å Í¥ÄÏ†àÏùò ÌÜ†ÌÅ¨\"]\n",
    "        SP3[\"Î™©Ìëú: ÏµúÎåÄÌïú Î©ÄÎ¶¨ Îõ∞Í∏∞\\nÎÑòÏñ¥ÏßÄÏßÄ ÏïäÍ≥† Ï†ÑÏßÑ\"]\n",
    "    end\n",
    "\n",
    "    Hopper --- Specs\n",
    "\n",
    "    style Hopper fill:#fff3e0\n",
    "    style Specs fill:#e8f5e9\n",
    "```\n",
    "\n",
    "## üìä Í∂§Ï†Å(Trajectory) Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞\n",
    "\n",
    "D4RL Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ Ïó¨Îü¨ Í∞úÏùò **Í∂§Ï†Å(trajectory)**ÏúºÎ°ú Íµ¨ÏÑ±Îê©ÎãàÎã§.  \n",
    "Í∞Å Í∂§Ï†ÅÏùÄ ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÌôòÍ≤ΩÏóêÏÑú Ìïú ÏóêÌîºÏÜåÎìú ÎèôÏïà Í≤ΩÌóòÌïú Îç∞Ïù¥ÌÑ∞Î•º Îã¥Í≥† ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Dataset[\"üìÇ trajectories Î¶¨Ïä§Ìä∏\"]\n",
    "        direction TB\n",
    "        T1[\"traj_0\"] \n",
    "        T2[\"traj_1\"]\n",
    "        TN[\"traj_N\"]\n",
    "        T1 ~~~ T2 ~~~ TN\n",
    "    end\n",
    "\n",
    "    subgraph OneTraj[\"üìã Í∞Å trajectory Íµ¨Ï°∞\"]\n",
    "        direction TB\n",
    "        OBS[\"observations: (T, 11)\\ns‚ÇÄ, s‚ÇÅ, ..., s_T\"]\n",
    "        ACT[\"actions: (T, 3)\\na‚ÇÄ, a‚ÇÅ, ..., a_T\"]\n",
    "        REW[\"rewards: (T,)\\nr‚ÇÄ, r‚ÇÅ, ..., r_T\"]\n",
    "    end\n",
    "\n",
    "    Dataset --> OneTraj\n",
    "\n",
    "    style Dataset fill:#e3f2fd\n",
    "    style OBS fill:#c8e6c9\n",
    "    style ACT fill:#bbdefb\n",
    "    style REW fill:#ffcdd2\n",
    "```\n",
    "\n",
    "> üí° **TÎäî ÏóêÌîºÏÜåÎìú Í∏∏Ïù¥**Î°ú, Í∂§Ï†ÅÎßàÎã§ Îã§Î•º Ïàò ÏûàÏäµÎãàÎã§.  \n",
    "> HopperÏùò Í≤ΩÏö∞ Î≥¥ÌÜµ 100~1000 Ïä§ÌÖù Ï†ïÎèÑÏûÖÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Îç∞Ïù¥ÌÑ∞ÏÖã ÎîîÎ†âÌÜ†Î¶¨ ÌÉêÏÉâ\n",
      "============================================================\n",
      "\n",
      "‚ö†Ô∏è .pkl ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§.\n",
      "\n",
      "Îç∞Ïù¥ÌÑ∞ÏÖã Îã§Ïö¥Î°úÎìú Î∞©Î≤ï:\n",
      "  cd gym\n",
      "  python data/download_d4rl_datasets.py\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìÇ Îç∞Ïù¥ÌÑ∞ÏÖã ÌååÏùº ÌôïÏù∏\n",
    "# ============================================================\n",
    "# D4RL Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ pickle ÌòïÏãùÏúºÎ°ú Ï†ÄÏû•ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\n",
    "# Í∞Å ÌååÏùºÏóêÎäî Ïó¨Îü¨ Í∞úÏùò ÏóêÌîºÏÜåÎìú(Í∂§Ï†Å)Í∞Ä Îã¥Í≤® ÏûàÏäµÎãàÎã§.\n",
    "# ============================================================\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú\n",
    "data_dir = os.path.join(GYM_PATH, 'data')\n",
    "\n",
    "print(\"üìÇ Îç∞Ïù¥ÌÑ∞ÏÖã ÎîîÎ†âÌÜ†Î¶¨ ÌÉêÏÉâ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if os.path.exists(data_dir):\n",
    "    files = os.listdir(data_dir)\n",
    "    pkl_files = [f for f in files if f.endswith('.pkl')]\n",
    "    \n",
    "    if pkl_files:\n",
    "        print(f\"\\n‚úÖ ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Îç∞Ïù¥ÌÑ∞ÏÖã ({len(pkl_files)}Í∞ú):\\n\")\n",
    "        print(f\"{'ÌååÏùºÎ™Ö':<35} {'ÌÅ¨Í∏∞':>10}\")\n",
    "        print(\"-\"*50)\n",
    "        for f in sorted(pkl_files):\n",
    "            size = os.path.getsize(os.path.join(data_dir, f)) / (1024*1024)\n",
    "            print(f\"  {f:<33} {size:>8.1f} MB\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è .pkl ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§.\")\n",
    "        print(\"\\nÎç∞Ïù¥ÌÑ∞ÏÖã Îã§Ïö¥Î°úÎìú Î∞©Î≤ï:\")\n",
    "        print(\"  cd gym\")\n",
    "        print(\"  python data/download_d4rl_datasets.py\")\n",
    "else:\n",
    "    print(f\"‚ùå Îç∞Ïù¥ÌÑ∞ ÎîîÎ†âÌÜ†Î¶¨Í∞Ä ÏóÜÏäµÎãàÎã§: {data_dir}\")\n",
    "    print(\"\\nÎç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Î®ºÏ†Ä Îã§Ïö¥Î°úÎìúÌïòÏÑ∏Ïöî:\")\n",
    "    print(\"  cd gym && python data/download_d4rl_datasets.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú Ï§ë...\n",
      "============================================================\n",
      "‚ö†Ô∏è ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§: /content/decision-transformer/gym/data/hopper-medium-v2.pkl\n",
      "\n",
      "üîß ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§...\n",
      "   (Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ÏôÄ ÎèôÏùºÌïú Íµ¨Ï°∞Î°ú ÏÉùÏÑ±)\n",
      "‚úÖ ÎçîÎØ∏ Í∂§Ï†Å 100Í∞ú ÏÉùÏÑ± ÏôÑÎ£å!\n",
      "\n",
      "üìå Îç∞Ïù¥ÌÑ∞ Ï∂úÏ≤ò: Dummy\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üì• Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
    "# ============================================================\n",
    "# hopper-medium-v2 Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Î°úÎìúÌï©ÎãàÎã§.\n",
    "# ÌååÏùºÏù¥ ÏóÜÏúºÎ©¥ ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.\n",
    "#\n",
    "# Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞:\n",
    "#   trajectories = [traj_1, traj_2, ..., traj_N]\n",
    "#   Í∞Å trajÎäî ÌïòÎÇòÏùò ÏóêÌîºÏÜåÎìú(Í≤åÏûÑ Ìïú Ìåê)Î•º ÏùòÎØ∏\n",
    "# ============================================================\n",
    "\n",
    "dataset_path = os.path.join(data_dir, 'hopper-medium-v2.pkl')\n",
    "\n",
    "print(\"üì• Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú Ï§ë...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    # Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
    "    with open(dataset_path, 'rb') as f:\n",
    "        trajectories = pickle.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú ÏôÑÎ£å!\")\n",
    "    print(f\"\\nüìä Í∏∞Î≥∏ Ï†ïÎ≥¥:\")\n",
    "    print(f\"   Ï¥ù Í∂§Ï†Å(ÏóêÌîºÏÜåÎìú) Ïàò: {len(trajectories)}\")\n",
    "    print(f\"   Ï≤´ Î≤àÏß∏ Í∂§Ï†ÅÏùò ÌÇ§: {list(trajectories[0].keys())}\")\n",
    "    \n",
    "    DATA_SOURCE = \"D4RL\"\n",
    "else:\n",
    "    # ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±\n",
    "    print(f\"‚ö†Ô∏è ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§: {dataset_path}\")\n",
    "    print(\"\\nüîß ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§...\")\n",
    "    print(\"   (Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ÏôÄ ÎèôÏùºÌïú Íµ¨Ï°∞Î°ú ÏÉùÏÑ±)\")\n",
    "    \n",
    "    np.random.seed(42)  # Ïû¨ÌòÑÏÑ±ÏùÑ ÏúÑÌïú ÏãúÎìú\n",
    "    trajectories = []\n",
    "    \n",
    "    for i in range(100):  # 100Í∞úÏùò ÏóêÌîºÏÜåÎìú\n",
    "        ep_len = np.random.randint(100, 500)  # ÏóêÌîºÏÜåÎìú Í∏∏Ïù¥\n",
    "        traj = {\n",
    "            'observations': np.random.randn(ep_len, 11).astype(np.float32),  # 11Ï∞®Ïõê ÏÉÅÌÉú\n",
    "            'actions': np.clip(np.random.randn(ep_len, 3), -1, 1).astype(np.float32),  # 3Ï∞®Ïõê ÌñâÎèô\n",
    "            'rewards': np.random.uniform(0, 3, ep_len).astype(np.float32)  # Î≥¥ÏÉÅ\n",
    "        }\n",
    "        trajectories.append(traj)\n",
    "    \n",
    "    print(f\"‚úÖ ÎçîÎØ∏ Í∂§Ï†Å {len(trajectories)}Í∞ú ÏÉùÏÑ± ÏôÑÎ£å!\")\n",
    "    DATA_SOURCE = \"Dummy\"\n",
    "\n",
    "print(f\"\\nüìå Îç∞Ïù¥ÌÑ∞ Ï∂úÏ≤ò: {DATA_SOURCE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Ï≤´ Î≤àÏß∏ Í∂§Ï†Å ÏÉÅÏÑ∏ Î∂ÑÏÑù\n",
      "============================================================\n",
      "\n",
      "üìê Shape Ï†ïÎ≥¥:\n",
      "   observations: (202, 11)\n",
      "                 ‚îî‚îÄ (ÏóêÌîºÏÜåÎìú Í∏∏Ïù¥, state Ï∞®Ïõê)\n",
      "   actions:      (202, 3)\n",
      "                 ‚îî‚îÄ (ÏóêÌîºÏÜåÎìú Í∏∏Ïù¥, action Ï∞®Ïõê)\n",
      "   rewards:      (202,)\n",
      "                 ‚îî‚îÄ (ÏóêÌîºÏÜåÎìú Í∏∏Ïù¥,)\n",
      "\n",
      "üìä ÏóêÌîºÏÜåÎìú ÌÜµÍ≥Ñ:\n",
      "   ÏóêÌîºÏÜåÎìú Í∏∏Ïù¥: 202 Ïä§ÌÖù\n",
      "   Ï¥ù Return: 317.95\n",
      "   ÌèâÍ∑† Reward: 1.5740\n",
      "\n",
      "üìù Îç∞Ïù¥ÌÑ∞ ÏÉòÌîå (Ï≤òÏùå 3 Ïä§ÌÖù):\n",
      "\n",
      "   Timestep 0:\n",
      "     State:  [-0.5502345   0.5154331   0.47386083  1.3684502  -0.91682684]... (Ïïû 5Í∞úÎßå ÌëúÏãú)\n",
      "     Action: [0.5623166  0.41726345 1.        ]\n",
      "     Reward: 2.8213\n",
      "\n",
      "   Timestep 1:\n",
      "     State:  [-0.1595165   0.02222183 -0.4277929  -0.53181744 -0.1174755 ]...\n",
      "     Action: [ 0.349595   -0.54914695  1.        ]\n",
      "     Reward: 0.0475\n",
      "\n",
      "   Timestep 2:\n",
      "     State:  [-0.5251228  1.9127712 -2.0267196  1.1194236  0.7791926]...\n",
      "     Action: [1.         0.23320596 0.06032192]\n",
      "     Reward: 1.5683\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üîç Í∂§Ï†Å Îç∞Ïù¥ÌÑ∞ ÏÉÅÏÑ∏ Î∂ÑÏÑù\n",
    "# ============================================================\n",
    "# ÌïòÎÇòÏùò Í∂§Ï†Å(trajectory)Ïù¥ Ïñ¥ÎñªÍ≤å Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÎäîÏßÄ ÏÇ¥Ìé¥Î¥ÖÎãàÎã§.\n",
    "#\n",
    "# Í∂§Ï†Å Íµ¨Ï°∞:\n",
    "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "# ‚îÇ  traj = {                                               ‚îÇ\n",
    "# ‚îÇ    'observations': [s_0, s_1, s_2, ..., s_T]           ‚îÇ\n",
    "# ‚îÇ    'actions':      [a_0, a_1, a_2, ..., a_T]           ‚îÇ\n",
    "# ‚îÇ    'rewards':      [r_0, r_1, r_2, ..., r_T]           ‚îÇ\n",
    "# ‚îÇ  }                                                     ‚îÇ\n",
    "# ‚îÇ                                                        ‚îÇ\n",
    "# ‚îÇ  T = ÏóêÌîºÏÜåÎìú Í∏∏Ïù¥ (ÌôòÍ≤ΩÎßàÎã§ Îã§Î¶Ñ)                        ‚îÇ\n",
    "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "# ============================================================\n",
    "\n",
    "# Ï≤´ Î≤àÏß∏ Í∂§Ï†Å ÏÑ†ÌÉù\n",
    "traj = trajectories[0]\n",
    "\n",
    "print(\"üîç Ï≤´ Î≤àÏß∏ Í∂§Ï†Å ÏÉÅÏÑ∏ Î∂ÑÏÑù\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Shape Ï†ïÎ≥¥\n",
    "print(\"\\nüìê Shape Ï†ïÎ≥¥:\")\n",
    "print(f\"   observations: {traj['observations'].shape}\")\n",
    "print(f\"                 ‚îî‚îÄ (ÏóêÌîºÏÜåÎìú Í∏∏Ïù¥, state Ï∞®Ïõê)\")\n",
    "print(f\"   actions:      {traj['actions'].shape}\")\n",
    "print(f\"                 ‚îî‚îÄ (ÏóêÌîºÏÜåÎìú Í∏∏Ïù¥, action Ï∞®Ïõê)\")\n",
    "print(f\"   rewards:      {traj['rewards'].shape}\")\n",
    "print(f\"                 ‚îî‚îÄ (ÏóêÌîºÏÜåÎìú Í∏∏Ïù¥,)\")\n",
    "\n",
    "# ÏóêÌîºÏÜåÎìú ÌÜµÍ≥Ñ\n",
    "ep_len = len(traj['rewards'])\n",
    "total_return = sum(traj['rewards'])\n",
    "\n",
    "print(f\"\\nüìä ÏóêÌîºÏÜåÎìú ÌÜµÍ≥Ñ:\")\n",
    "print(f\"   ÏóêÌîºÏÜåÎìú Í∏∏Ïù¥: {ep_len} Ïä§ÌÖù\")\n",
    "print(f\"   Ï¥ù Return: {total_return:.2f}\")\n",
    "print(f\"   ÌèâÍ∑† Reward: {total_return / ep_len:.4f}\")\n",
    "\n",
    "# Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ ÏÉòÌîå\n",
    "print(f\"\\nüìù Îç∞Ïù¥ÌÑ∞ ÏÉòÌîå (Ï≤òÏùå 3 Ïä§ÌÖù):\")\n",
    "print()\n",
    "print(\"   Timestep 0:\")\n",
    "print(f\"     State:  {traj['observations'][0][:5]}... (Ïïû 5Í∞úÎßå ÌëúÏãú)\")\n",
    "print(f\"     Action: {traj['actions'][0]}\")\n",
    "print(f\"     Reward: {traj['rewards'][0]:.4f}\")\n",
    "print()\n",
    "print(\"   Timestep 1:\")\n",
    "print(f\"     State:  {traj['observations'][1][:5]}...\")\n",
    "print(f\"     Action: {traj['actions'][1]}\")\n",
    "print(f\"     Reward: {traj['rewards'][1]:.4f}\")\n",
    "print()\n",
    "print(\"   Timestep 2:\")\n",
    "print(f\"     State:  {traj['observations'][2][:5]}...\")\n",
    "print(f\"     Action: {traj['actions'][2]}\")\n",
    "print(f\"     Reward: {traj['rewards'][2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üêº PandasÎ°ú Îç∞Ïù¥ÌÑ∞ÏÖã Î∂ÑÏÑù\n",
    "# ============================================================\n",
    "# Í∂§Ï†Å Îç∞Ïù¥ÌÑ∞Î•º DataFrameÏúºÎ°ú Î≥ÄÌôòÌïòÎ©¥ Ìõ®Ïî¨ ÏßÅÍ¥ÄÏ†ÅÏúºÎ°ú ÌååÏïÖÌï† Ïàò ÏûàÏäµÎãàÎã§.\n",
    "# - Í∞Å ÏÉÅÌÉú Ï∞®ÏõêÏùò Î∂ÑÌè¨Î•º ÌïúÎààÏóê ÌôïÏù∏\n",
    "# - actionÍ≥º rewardÏùò ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌååÏïÖ\n",
    "# - Ïù¥ÏÉÅÏπò(outlier) ÌÉêÏßÄ\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ‚îÄ‚îÄ ÌïòÎÇòÏùò Í∂§Ï†ÅÏùÑ DataFrameÏúºÎ°ú Î≥ÄÌôò ‚îÄ‚îÄ\n",
    "traj = trajectories[0]\n",
    "\n",
    "# Hopper ÏÉÅÌÉú Ï∞®Ïõê Ïù¥Î¶Ñ\n",
    "state_cols = [\n",
    "    'z_pos', 'y_angle', 'thigh_angle', 'leg_angle', 'foot_angle',\n",
    "    'z_vel', 'y_ang_vel', 'thigh_vel', 'leg_vel', 'foot_vel', 'x_vel'\n",
    "]\n",
    "action_cols = ['act_thigh', 'act_leg', 'act_foot']\n",
    "\n",
    "# ÏÉÅÌÉú + ÌñâÎèô + Î≥¥ÏÉÅÏùÑ ÌïòÎÇòÏùò DataFrameÏúºÎ°ú\n",
    "df = pd.DataFrame(traj['observations'], columns=state_cols)\n",
    "df[action_cols] = traj['actions']\n",
    "df['reward'] = traj['rewards']\n",
    "df.index.name = 'timestep'\n",
    "\n",
    "print(\"üêº Í∂§Ï†Å #0ÏùÑ DataFrameÏúºÎ°ú Î≥ÄÌôò\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {df.shape} (Ìñâ=ÌÉÄÏûÑÏä§ÌÖù, Ïó¥=state+action+reward)\")\n",
    "print()\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìä Í∏∞Ïà† ÌÜµÍ≥Ñ (describe)\n",
    "# ============================================================\n",
    "# Í∞Å Ï∞®ÏõêÏùò ÌèâÍ∑†, ÌëúÏ§ÄÌé∏Ï∞®, ÏµúÏÜå/ÏµúÎåÄÍ∞íÏùÑ ÌïúÎààÏóê ÌôïÏù∏Ìï©ÎãàÎã§.\n",
    "# Ïä§ÏºÄÏùºÏù¥ Ï∞®ÏõêÎßàÎã§ ÌÅ¨Í≤å Îã§Î•∏ Í≤ÉÏùÑ ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n",
    "# ‚Üí Ïù¥Í≤ÉÏù¥ ÏÉÅÌÉú Ï†ïÍ∑úÌôîÍ∞Ä ÌïÑÏöîÌïú Ïù¥Ïú†ÏûÖÎãàÎã§!\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìä Í∏∞Ïà† ÌÜµÍ≥Ñ (Í∂§Ï†Å #0)\")\n",
    "print(\"=\"*60)\n",
    "df.describe().round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üîó Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖã ÏöîÏïΩ (Î™®Îì† Í∂§Ï†Å Ìï©ÏÇ∞)\n",
    "# ============================================================\n",
    "# Í∞úÎ≥Ñ Í∂§Ï†ÅÏù¥ ÏïÑÎãå Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖã ÏàòÏ§ÄÏùò ÌÜµÍ≥ÑÎ•º ÌôïÏù∏Ìï©ÎãàÎã§.\n",
    "# ============================================================\n",
    "\n",
    "# ÏóêÌîºÏÜåÎìú ÏàòÏ§Ä ÏöîÏïΩ DataFrame\n",
    "episode_df = pd.DataFrame({\n",
    "    'length': [len(t['rewards']) for t in trajectories],\n",
    "    'return': [t['rewards'].sum() for t in trajectories],\n",
    "    'mean_reward': [t['rewards'].mean() for t in trajectories],\n",
    "    'std_reward': [t['rewards'].std() for t in trajectories],\n",
    "    'max_reward': [t['rewards'].max() for t in trajectories],\n",
    "    'min_reward': [t['rewards'].min() for t in trajectories],\n",
    "})\n",
    "episode_df.index.name = 'episode'\n",
    "\n",
    "print(\"üîó Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖã ÏóêÌîºÏÜåÎìú ÏöîÏïΩ\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Ï¥ù ÏóêÌîºÏÜåÎìú Ïàò: {len(episode_df)}\")\n",
    "print(f\"Ï¥ù ÌÉÄÏûÑÏä§ÌÖù Ïàò: {episode_df['length'].sum():,}\")\n",
    "print()\n",
    "episode_df.describe().round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìà ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∂ÑÏÑù\n",
    "# ============================================================\n",
    "# actionÍ≥º state/reward Í∞ÑÏùò ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÎ•º ÌôïÏù∏Ìï©ÎãàÎã§.\n",
    "# Ïñ¥Îñ§ ÏÉÅÌÉú Î≥ÄÏàòÍ∞Ä ÌñâÎèô ÏÑ†ÌÉùÏóê ÏòÅÌñ•ÏùÑ ÎØ∏ÏπòÎäîÏßÄ ÌååÏïÖÌï† Ïàò ÏûàÏäµÎãàÎã§.\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ï†ÑÏ≤¥ Í∂§Ï†ÅÏùÑ Ìï©Ï≥êÏÑú ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Í≥ÑÏÇ∞\n",
    "all_data = []\n",
    "for t in trajectories[:50]:  # Ï≤òÏùå 50Í∞ú Í∂§Ï†Å ÏÇ¨Ïö© (ÏÜçÎèÑ)\n",
    "    tdf = pd.DataFrame(t['observations'], columns=state_cols)\n",
    "    tdf[action_cols] = t['actions']\n",
    "    tdf['reward'] = t['rewards']\n",
    "    all_data.append(tdf)\n",
    "\n",
    "full_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "print(f\"üìà ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∂ÑÏÑù (Í∂§Ï†Å 50Í∞ú, {len(full_df):,} ÌÉÄÏûÑÏä§ÌÖù)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌûàÌä∏Îßµ\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "corr = full_df.corr()\n",
    "im = ax.imshow(corr, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')\n",
    "\n",
    "# Ï∂ï Î†àÏù¥Î∏î\n",
    "labels = list(full_df.columns)\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_yticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_yticklabels(labels, fontsize=9)\n",
    "ax.set_title('State / Action / Reward Correlation', fontsize=14)\n",
    "plt.colorbar(im, ax=ax, label='Correlation', shrink=0.8)\n",
    "\n",
    "# ÏÉÅÍ¥ÄÍ≥ÑÏàò ÌÖçÏä§Ìä∏ ÌëúÏãú (Ï£ºÏöî Í∞íÎßå)\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        val = corr.iloc[i, j]\n",
    "        if abs(val) > 0.3 and i != j:\n",
    "            ax.text(j, i, f'{val:.2f}', ha='center', va='center', fontsize=7,\n",
    "                    color='white' if abs(val) > 0.6 else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# rewardÏôÄ Í∞ÄÏû• ÏÉÅÍ¥ÄÏù¥ ÎÜíÏùÄ Î≥ÄÏàò top 5\n",
    "print(\"\\nüèÜ rewardÏôÄ ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÍ∞Ä ÎÜíÏùÄ Î≥ÄÏàò (Ï†àÎåÄÍ∞í Í∏∞Ï§Ä):\")\n",
    "reward_corr = corr['reward'].drop('reward').abs().sort_values(ascending=False)\n",
    "for name, val in reward_corr.head(5).items():\n",
    "    direction = 'Ïñë(+)' if corr.loc[name, 'reward'] > 0 else 'Ïùå(-)'\n",
    "    print(f\"   {name:<15} : {corr.loc[name, 'reward']:>+.3f} ({direction} ÏÉÅÍ¥Ä)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Îç∞Ïù¥ÌÑ∞ÏÖã ÌÜµÍ≥Ñ ÏãúÍ∞ÅÌôî\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà ÏàòÏπò ÌÜµÍ≥Ñ:\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ Ìï≠Î™©                                           Í∞í ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ Ï¥ù ÏóêÌîºÏÜåÎìú Ïàò                                   100 ‚îÇ\n",
      "‚îÇ ÌèâÍ∑† ÏóêÌîºÏÜåÎìú Í∏∏Ïù¥                            278.6 ¬± 117.9 ‚îÇ\n",
      "‚îÇ ÏµúÏÜå/ÏµúÎåÄ Í∏∏Ïù¥                     103 / 496         ‚îÇ\n",
      "‚îÇ ÌèâÍ∑† Return                             419.1 ¬± 174.7 ‚îÇ\n",
      "‚îÇ ÏµúÏÜå/ÏµúÎåÄ Return               161.4 / 758.0       ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "üí° ÌåÅ: ÌïôÏäµ Ïãú Î™©Ìëú ReturnÏùÑ ÌèâÍ∑†(419) Ïù¥ÏÉÅÏúºÎ°ú ÏÑ§Ï†ïÌïòÎ©¥\n",
      "       Îçî ÎÇòÏùÄ Ï†ïÏ±ÖÏùÑ Ïú†ÎèÑÌï† Ïàò ÏûàÏäµÎãàÎã§!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üìä Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ÑÏ≤¥ ÌÜµÍ≥Ñ ÏãúÍ∞ÅÌôî\n",
    "# ============================================================\n",
    "# Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Î∂ÑÌè¨Î•º ÏãúÍ∞ÅÌôîÌïòÏó¨ Îç∞Ïù¥ÌÑ∞ ÌäπÏÑ±ÏùÑ ÌååÏïÖÌï©ÎãàÎã§.\n",
    "# - ÏóêÌîºÏÜåÎìú Í∏∏Ïù¥ Î∂ÑÌè¨: ÏóêÌîºÏÜåÎìúÍ∞Ä ÏñºÎßàÎÇò Í∏∏Í≤å ÏßÄÏÜçÎêòÎäîÍ∞Ä?\n",
    "# - Return Î∂ÑÌè¨: Ï†ïÏ±ÖÏùò ÌíàÏßàÏù¥ Ïñ¥Îñ†ÌïúÍ∞Ä?\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï (utils.pyÏóêÏÑú Í∞ÄÏ†∏Ïò¥)\n",
    "try:\n",
    "    from utils import setup_matplotlib\n",
    "except:\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ÌÜµÍ≥Ñ Í≥ÑÏÇ∞\n",
    "lengths = [len(t['rewards']) for t in trajectories]\n",
    "returns = [sum(t['rewards']) for t in trajectories]\n",
    "\n",
    "print(\"üìä Îç∞Ïù¥ÌÑ∞ÏÖã ÌÜµÍ≥Ñ ÏãúÍ∞ÅÌôî\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Í∑∏ÎûòÌîÑ ÏÉùÏÑ±\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. ÏóêÌîºÏÜåÎìú Í∏∏Ïù¥ Î∂ÑÌè¨\n",
    "ax1 = axes[0]\n",
    "ax1.hist(lengths, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax1.axvline(np.mean(lengths), color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {np.mean(lengths):.0f}')\n",
    "ax1.axvline(np.median(lengths), color='orange', linestyle=':', linewidth=2,\n",
    "            label=f'Median: {np.median(lengths):.0f}')\n",
    "ax1.set_xlabel('Episode Length (steps)', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_title('Episode Length Distribution', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Return Î∂ÑÌè¨\n",
    "ax2 = axes[1]\n",
    "ax2.hist(returns, bins=30, edgecolor='black', alpha=0.7, color='forestgreen')\n",
    "ax2.axvline(np.mean(returns), color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Mean: {np.mean(returns):.0f}')\n",
    "ax2.axvline(np.median(returns), color='orange', linestyle=':', linewidth=2,\n",
    "            label=f'Median: {np.median(returns):.0f}')\n",
    "ax2.set_xlabel('Episode Return', fontsize=12)\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.set_title('Episode Return Distribution', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ÏàòÏπò ÌÜµÍ≥Ñ Ï∂úÎ†•\n",
    "print(f\"\\nüìà ÏàòÏπò ÌÜµÍ≥Ñ:\")\n",
    "print(f\"‚îå{'‚îÄ'*50}‚îê\")\n",
    "print(f\"‚îÇ {'Ìï≠Î™©':<20} {'Í∞í':>25} ‚îÇ\")\n",
    "print(f\"‚îú{'‚îÄ'*50}‚î§\")\n",
    "print(f\"‚îÇ {'Ï¥ù ÏóêÌîºÏÜåÎìú Ïàò':<20} {len(trajectories):>25,} ‚îÇ\")\n",
    "print(f\"‚îÇ {'ÌèâÍ∑† ÏóêÌîºÏÜåÎìú Í∏∏Ïù¥':<20} {np.mean(lengths):>22.1f} ¬± {np.std(lengths):.1f} ‚îÇ\")\n",
    "print(f\"‚îÇ {'ÏµúÏÜå/ÏµúÎåÄ Í∏∏Ïù¥':<20} {min(lengths):>11} / {max(lengths):<11} ‚îÇ\")\n",
    "print(f\"‚îÇ {'ÌèâÍ∑† Return':<20} {np.mean(returns):>22.1f} ¬± {np.std(returns):.1f} ‚îÇ\")\n",
    "print(f\"‚îÇ {'ÏµúÏÜå/ÏµúÎåÄ Return':<20} {min(returns):>11.1f} / {max(returns):<11.1f} ‚îÇ\")\n",
    "print(f\"‚îî{'‚îÄ'*50}‚îò\")\n",
    "\n",
    "# Decision TransformerÏóêÏÑú ÏÇ¨Ïö©Ìï† Î™©Ìëú Return Ï∞∏Í≥†\n",
    "print(f\"\\nüí° ÌåÅ: ÌïôÏäµ Ïãú Î™©Ìëú ReturnÏùÑ ÌèâÍ∑†({np.mean(returns):.0f}) Ïù¥ÏÉÅÏúºÎ°ú ÏÑ§Ï†ïÌïòÎ©¥\")\n",
    "print(f\"       Îçî ÎÇòÏùÄ Ï†ïÏ±ÖÏùÑ Ïú†ÎèÑÌï† Ïàò ÏûàÏäµÎãàÎã§!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà RTG (Return-to-Go) Í≥ÑÏÇ∞ Ïã§Ïäµ\n",
    "\n",
    "> **RTG**Îäî Decision TransformerÏùò ÌïµÏã¨ Í∞úÎÖêÏûÖÎãàÎã§.  \n",
    "> \"ÌòÑÏû¨ ÏãúÏ†êÎ∂ÄÌÑ∞ ÏóêÌîºÏÜåÎìú ÎÅùÍπåÏßÄ ÏñªÏùÑ Ïàò ÏûàÎäî Ï¥ù Î≥¥ÏÉÅ\"ÏùÑ ÏùòÎØ∏Ìï©ÎãàÎã§.\n",
    "\n",
    "### RTG Í≥ÑÏÇ∞ Í≥µÏãù\n",
    "\n",
    "$$\\text{RTG}_t = r_t + r_{t+1} + r_{t+2} + \\cdots + r_T$$\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Example[\"üìù RTG Í≥ÑÏÇ∞ ÏòàÏãú: rewards = 1, 2, 3, 4, 5\"]\n",
    "        direction TB\n",
    "        subgraph T0[\"t=0\"]\n",
    "            R0[\"ÎÇ®ÏùÄ Î≥¥ÏÉÅ: 1+2+3+4+5\\nRTG = 15\"]\n",
    "        end\n",
    "        subgraph T1[\"t=1\"]\n",
    "            R1[\"ÎÇ®ÏùÄ Î≥¥ÏÉÅ: 2+3+4+5\\nRTG = 14\"]\n",
    "        end\n",
    "        subgraph T2[\"t=2\"]\n",
    "            R2[\"ÎÇ®ÏùÄ Î≥¥ÏÉÅ: 3+4+5\\nRTG = 12\"]\n",
    "        end\n",
    "        subgraph T3[\"t=3\"]\n",
    "            R3[\"ÎÇ®ÏùÄ Î≥¥ÏÉÅ: 4+5\\nRTG = 9\"]\n",
    "        end\n",
    "        subgraph T4[\"t=4\"]\n",
    "            R4[\"ÎÇ®ÏùÄ Î≥¥ÏÉÅ: 5\\nRTG = 5\"]\n",
    "        end\n",
    "        T0 --> T1 --> T2 --> T3 --> T4\n",
    "    end\n",
    "\n",
    "    style T0 fill:#ffcdd2\n",
    "    style T1 fill:#ffccbc\n",
    "    style T2 fill:#fff9c4\n",
    "    style T3 fill:#dcedc8\n",
    "    style T4 fill:#c8e6c9\n",
    "```\n",
    "\n",
    "### Ìï†Ïù∏Ïú®(Œ≥)ÏùÑ Ï†ÅÏö©Ìïú RTG\n",
    "\n",
    "Ïã§Ï†úÎ°úÎäî **Ìï†Ïù∏Ïú®(discount factor, Œ≥)**ÏùÑ Ï†ÅÏö©ÌïòÍ∏∞ÎèÑ Ìï©ÎãàÎã§:\n",
    "\n",
    "$$\\text{RTG}_t = r_t + \\gamma \\cdot r_{t+1} + \\gamma^2 \\cdot r_{t+2} + \\cdots$$\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Gamma[\"‚öôÔ∏è Ìï†Ïù∏Ïú®Ïóê Îî∞Î•∏ Ï∞®Ïù¥\"]\n",
    "        direction TB\n",
    "        G1[\"Œ≥ = 1.0\\nÎ™®Îì† ÎØ∏Îûò Î≥¥ÏÉÅÏùÑ ÎèôÎì±ÌïòÍ≤å\\n‚Üí Decision Transformer ÎÖºÎ¨∏ÏóêÏÑú ÏÇ¨Ïö©\"]\n",
    "        G2[\"Œ≥ = 0.99\\nÎ®º ÎØ∏Îûò Î≥¥ÏÉÅÏùÑ ÏïΩÍ∞Ñ Ìï†Ïù∏\"]\n",
    "        G3[\"Œ≥ = 0.9\\nÎ®º ÎØ∏Îûò Î≥¥ÏÉÅÏùÑ ÎßéÏù¥ Ìï†Ïù∏\"]\n",
    "    end\n",
    "\n",
    "    style G1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px\n",
    "    style G2 fill:#fff9c4\n",
    "    style G3 fill:#ffcdd2\n",
    "```\n",
    "\n",
    "### üßÆ RTG Ïó≠Ïàú Í≥ÑÏÇ∞ ÏïåÍ≥†Î¶¨Ï¶ò\n",
    "\n",
    "RTGÎäî **Îí§ÏóêÏÑú ÏïûÏúºÎ°ú(Ïó≠Ïàú)** Í≥ÑÏÇ∞ÌïòÎ©¥ Ìö®Ïú®Ï†ÅÏûÖÎãàÎã§:\n",
    "\n",
    "```mermaid\n",
    "flowchart RL\n",
    "    subgraph Algorithm[\"üîÑ Ïó≠Ïàú Í≥ÑÏÇ∞\"]\n",
    "        direction RL\n",
    "        S4[\"t=4: RTG=r‚ÇÑ=5\"] --> S3[\"t=3: RTG=r‚ÇÉ+Œ≥¬∑5=9\"]\n",
    "        S3 --> S2[\"t=2: RTG=r‚ÇÇ+Œ≥¬∑9=12\"]\n",
    "        S2 --> S1[\"t=1: RTG=r‚ÇÅ+Œ≥¬∑12=14\"]\n",
    "        S1 --> S0[\"t=0: RTG=r‚ÇÄ+Œ≥¬∑14=15\"]\n",
    "    end\n",
    "\n",
    "    style Algorithm fill:#e3f2fd\n",
    "    style S0 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px\n",
    "```\n",
    "\n",
    "> üí° **Decision Transformer ÎÖºÎ¨∏ÏóêÏÑúÎäî Œ≥=1.0 (Ìï†Ïù∏ ÏóÜÏùå)ÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.**  \n",
    "> Ïù¥Îäî Î™®Îì† ÎØ∏Îûò Î≥¥ÏÉÅÏùÑ ÎèôÎì±ÌïòÍ≤å Ï∑®Í∏âÌïòÍ≤†Îã§Îäî ÏùòÎØ∏ÏûÖÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üî¢ RTG Í≥ÑÏÇ∞ Ìï®Ïàò Íµ¨ÌòÑ\n",
    "# ============================================================\n",
    "# experiment.pyÏóêÏÑú ÏÇ¨Ïö©ÌïòÎäî discount_cumsum Ìï®ÏàòÎ•º Íµ¨ÌòÑÌï©ÎãàÎã§.\n",
    "# \n",
    "# ÏïåÍ≥†Î¶¨Ï¶ò:\n",
    "#   1. ÎßàÏßÄÎßâ ÏãúÏ†êÏùò RTG = ÎßàÏßÄÎßâ reward\n",
    "#   2. Ïó≠ÏàúÏúºÎ°ú ÏßÑÌñâÌïòÎ©∞ RTG[t] = reward[t] + Œ≥ * RTG[t+1]\n",
    "# ============================================================\n",
    "\n",
    "def discount_cumsum(x, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Ìï†Ïù∏Îêú ÎàÑÏ†ÅÌï©(RTG) Í≥ÑÏÇ∞\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : numpy.ndarray\n",
    "        reward ÏãúÌÄÄÏä§ [r_0, r_1, ..., r_T]\n",
    "    gamma : float\n",
    "        Ìï†Ïù∏Ïú® (default: 1.0, Ï¶â Ìï†Ïù∏ ÏóÜÏùå)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        RTG ÏãúÌÄÄÏä§ [RTG_0, RTG_1, ..., RTG_T]\n",
    "    \n",
    "    ÏòàÏãú:\n",
    "    -----\n",
    "    >>> rewards = [1, 2, 3]\n",
    "    >>> discount_cumsum(rewards, gamma=1.0)\n",
    "    array([6, 5, 3])  # [1+2+3, 2+3, 3]\n",
    "    \"\"\"\n",
    "    # Í≤∞Í≥º Î∞∞Ïó¥ Ï¥àÍ∏∞Ìôî\n",
    "    discount_cumsum_arr = np.zeros_like(x)\n",
    "    \n",
    "    # ÎßàÏßÄÎßâ ÏãúÏ†ê: RTG[-1] = reward[-1]\n",
    "    discount_cumsum_arr[-1] = x[-1]\n",
    "    \n",
    "    # Ïó≠ÏàúÏúºÎ°ú Í≥ÑÏÇ∞: RTG[t] = reward[t] + Œ≥ * RTG[t+1]\n",
    "    for t in reversed(range(x.shape[0] - 1)):\n",
    "        discount_cumsum_arr[t] = x[t] + gamma * discount_cumsum_arr[t + 1]\n",
    "    \n",
    "    return discount_cumsum_arr\n",
    "\n",
    "# ============================================================\n",
    "# üìù RTG Í≥ÑÏÇ∞ ÏòàÏãú\n",
    "# ============================================================\n",
    "print(\"üî¢ RTG Í≥ÑÏÇ∞ ÏòàÏãú\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Í∞ÑÎã®Ìïú ÏòàÏãú\n",
    "rewards = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "# Ìï†Ïù∏Ïú®Ïóê Îî∞Î•∏ RTG ÎπÑÍµê\n",
    "rtg_gamma1 = discount_cumsum(rewards, gamma=1.0)\n",
    "rtg_gamma099 = discount_cumsum(rewards, gamma=0.99)\n",
    "rtg_gamma09 = discount_cumsum(rewards, gamma=0.9)\n",
    "\n",
    "print(f\"\\nRewards: {rewards}\")\n",
    "print()\n",
    "print(f\"{'ÏãúÏ†ê t':<8} {'Œ≥=1.0':>10} {'Œ≥=0.99':>12} {'Œ≥=0.9':>12}\")\n",
    "print(\"-\"*45)\n",
    "for t in range(len(rewards)):\n",
    "    print(f\"  {t:<6} {rtg_gamma1[t]:>10.2f} {rtg_gamma099[t]:>12.2f} {rtg_gamma09[t]:>12.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"üí° Œ≥=1.0: Î™®Îì† ÎØ∏Îûò Î≥¥ÏÉÅÏùÑ ÎèôÎì±ÌïòÍ≤å Ìï©ÏÇ∞\")\n",
    "print(\"   Œ≥<1.0: Î®º ÎØ∏Îûò Î≥¥ÏÉÅÏùºÏàòÎ°ù Îçî Ï†ÅÍ≤å Î∞òÏòÅ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìà Ïã§Ï†ú Í∂§Ï†ÅÏóêÏÑú RTG ÏãúÍ∞ÅÌôî\n",
    "# ============================================================\n",
    "# Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ÏóêÏÑú RewardÏôÄ RTGÍ∞Ä Ïñ¥ÎñªÍ≤å Î≥ÄÌôîÌïòÎäîÏßÄ ÏãúÍ∞ÅÌôîÌï©ÎãàÎã§.\n",
    "#\n",
    "# Í¥ÄÏ∞∞ Ìè¨Ïù∏Ìä∏:\n",
    "#   - Reward: ÏàúÍ∞ÑÏ†ÅÏù∏ Î≥¥ÏÉÅ (ÎÖ∏Ïù¥Ï¶àÍ∞Ä ÎßéÏùå)\n",
    "#   - RTG: ÎØ∏Îûò Î≥¥ÏÉÅÏùò Ìï© (ÏãúÍ∞ÑÏù¥ ÏßÄÎÇ†ÏàòÎ°ù Í∞êÏÜå)\n",
    "# ============================================================\n",
    "\n",
    "# Ï≤´ Î≤àÏß∏ Í∂§Ï†ÅÏùò Îç∞Ïù¥ÌÑ∞\n",
    "traj = trajectories[0]\n",
    "rewards_traj = traj['rewards']\n",
    "rtg = discount_cumsum(rewards_traj, gamma=1.0)\n",
    "\n",
    "print(\"üìà Ïã§Ï†ú Í∂§Ï†ÅÏóêÏÑú Reward vs RTG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Í∑∏ÎûòÌîÑ (Ï≤òÏùå 100 Ïä§ÌÖùÎßå)\n",
    "num_steps = min(100, len(rewards_traj))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Reward over time\n",
    "ax1 = axes[0]\n",
    "ax1.plot(rewards_traj[:num_steps], alpha=0.8, linewidth=1.5, color='blue')\n",
    "ax1.fill_between(range(num_steps), rewards_traj[:num_steps], alpha=0.3, color='blue')\n",
    "ax1.axhline(y=np.mean(rewards_traj[:num_steps]), color='red', linestyle='--', \n",
    "            label=f'Mean: {np.mean(rewards_traj[:num_steps]):.2f}')\n",
    "ax1.set_xlabel('Timestep', fontsize=12)\n",
    "ax1.set_ylabel('Reward', fontsize=12)\n",
    "ax1.set_title('Instantaneous Reward Over Time', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. RTG over time\n",
    "ax2 = axes[1]\n",
    "ax2.plot(rtg[:num_steps], alpha=0.8, linewidth=2, color='orange')\n",
    "ax2.fill_between(range(num_steps), rtg[:num_steps], alpha=0.3, color='orange')\n",
    "ax2.set_xlabel('Timestep', fontsize=12)\n",
    "ax2.set_ylabel('Return-to-Go', fontsize=12)\n",
    "ax2.set_title('Return-to-Go Over Time', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# RTG Í∞êÏÜå Ï∂îÏÑ∏ ÌëúÏãú\n",
    "ax2.annotate(f'ÏãúÏûë: {rtg[0]:.0f}', xy=(0, rtg[0]), fontsize=10,\n",
    "             xytext=(10, rtg[0] - (rtg[0] - rtg[num_steps-1])*0.2),\n",
    "             arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "ax2.annotate(f't={num_steps-1}: {rtg[num_steps-1]:.0f}', \n",
    "             xy=(num_steps-1, rtg[num_steps-1]), fontsize=10,\n",
    "             xytext=(num_steps-20, rtg[num_steps-1] + (rtg[0] - rtg[num_steps-1])*0.2),\n",
    "             arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ÌïµÏã¨ Ìè¨Ïù∏Ìä∏ Ï∂úÎ†•\n",
    "print(f\"\\nüìå ÌïµÏã¨ Í¥ÄÏ∞∞:\")\n",
    "print(f\"   ‚Ä¢ Ï¥àÍ∏∞ RTG (t=0): {rtg[0]:.2f}\")\n",
    "print(f\"     ‚îî‚îÄ ÏóêÌîºÏÜåÎìú Ï†ÑÏ≤¥ÏóêÏÑú ÏñªÏùÑ Ï¥ù Î≥¥ÏÉÅ\")\n",
    "print(f\"   ‚Ä¢ Ï§ëÍ∞Ñ RTG (t={num_steps//2}): {rtg[num_steps//2]:.2f}\")\n",
    "print(f\"     ‚îî‚îÄ ÎÇ®ÏùÄ Ï†àÎ∞ò ÎèôÏïà ÏñªÏùÑ Î≥¥ÏÉÅ\")\n",
    "print(f\"   ‚Ä¢ ÏµúÏ¢Ö RTG (t={num_steps-1}): {rtg[num_steps-1]:.2f}\")\n",
    "print(f\"     ‚îî‚îÄ Í±∞Ïùò ÏóêÌîºÏÜåÎìú ÎÅù, ÎÇ®ÏùÄ Î≥¥ÏÉÅ Ï†ÅÏùå\")\n",
    "print()\n",
    "print(\"üí° RTGÎäî ÏãúÍ∞ÑÏù¥ ÏßÄÎÇ†ÏàòÎ°ù Îã®Ï°∞ Í∞êÏÜåÌï©ÎãàÎã§!\")\n",
    "print(\"   (Îß§ Ïä§ÌÖù rewardÎ•º Î∞õÏúºÎ©¥ÏÑú 'ÎÇ®ÏùÄ Î≥¥ÏÉÅ'Ïù¥ Ï§ÑÏñ¥Îì§Í∏∞ ÎïåÎ¨∏)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Î™®Îç∏ ÏΩîÎìú Î∂ÑÏÑù\n",
    "\n",
    "## üéØ Î™©Ìëú\n",
    "> Decision Transformer Î™®Îç∏Ïùò ÎÇ¥Î∂Ä Íµ¨Ï°∞Î•º Ïù¥Ìï¥ÌïòÍ≥†, Í∞Å Ïª¥Ìè¨ÎÑåÌä∏Ïùò Ïó≠Ìï†ÏùÑ ÌååÏïÖÌï©ÎãàÎã§.\n",
    "\n",
    "## üèóÔ∏è DecisionTransformer Ï†ÑÏ≤¥ ÏïÑÌÇ§ÌÖçÏ≤ò\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Inputs[\"üì• ÏûÖÎ†•\"]\n",
    "        RTG[\"RTG\\n(batch, K, 1)\"]\n",
    "        State[\"State\\n(batch, K, 11)\"]\n",
    "        Action[\"Action\\n(batch, K, 3)\"]\n",
    "        Time[\"Timestep\\n(batch, K)\"]\n",
    "    end\n",
    "\n",
    "    subgraph Embedding[\"1Ô∏è‚É£ ÏûÑÎ≤†Îî© Î†àÏù¥Ïñ¥\"]\n",
    "        RTG -->|\"Linear(1‚Üí128)\"| RE[\"RTG Emb\"]\n",
    "        State -->|\"Linear(11‚Üí128)\"| SE[\"State Emb\"]\n",
    "        Action -->|\"Linear(3‚Üí128)\"| AE[\"Action Emb\"]\n",
    "        Time -->|\"Embedding(1000, 128)\"| TE[\"Time Emb\"]\n",
    "\n",
    "        RE --- |\"+ Time\"| REF[\"R + T\"]\n",
    "        SE --- |\"+ Time\"| SEF[\"S + T\"]\n",
    "        AE --- |\"+ Time\"| AEF[\"A + T\"]\n",
    "    end\n",
    "\n",
    "    subgraph Interleave[\"2Ô∏è‚É£ ÏãúÌÄÄÏä§ Íµ¨ÏÑ± (Interleave)\"]\n",
    "        REF --> Stack\n",
    "        SEF --> Stack\n",
    "        AEF --> Stack\n",
    "        Stack[\"[R‚ÇÄ, s‚ÇÄ, a‚ÇÄ, R‚ÇÅ, s‚ÇÅ, a‚ÇÅ, ...]\\nShape: (batch, K√ó3, 128)\"] --> LN[\"LayerNorm\"]\n",
    "    end\n",
    "\n",
    "    subgraph Transformer[\"3Ô∏è‚É£ GPT-2 Transformer\"]\n",
    "        LN --> L1[\"Layer 1: Attention ‚Üí FFN ‚Üí LN\"]\n",
    "        L1 --> L2[\"Layer 2: Attention ‚Üí FFN ‚Üí LN\"]\n",
    "        L2 --> L3[\"Layer 3: Attention ‚Üí FFN ‚Üí LN\"]\n",
    "        L3 --> Out[\"(batch, K√ó3, 128)\"]\n",
    "    end\n",
    "\n",
    "    subgraph Heads[\"4Ô∏è‚É£ ÏòàÏ∏° Ìó§Îìú\"]\n",
    "        Out -->|\"[:, 1::3, :]\\nstate ÏúÑÏπò\"| PA[\"‚≠ê predict_action\\nLinear(128‚Üí3)\"]\n",
    "        Out -->|\"[:, 2::3, :]\"| PS[\"predict_state\\n(ÎØ∏ÏÇ¨Ïö©)\"]\n",
    "        Out -->|\"[:, 2::3, :]\"| PR[\"predict_return\\n(ÎØ∏ÏÇ¨Ïö©)\"]\n",
    "    end\n",
    "\n",
    "    style Inputs fill:#e1f5fe\n",
    "    style Embedding fill:#fff3e0\n",
    "    style Interleave fill:#f3e5f5\n",
    "    style Transformer fill:#e8f5e9\n",
    "    style Heads fill:#ffebee\n",
    "    style PA fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px\n",
    "    style PS fill:#eeeeee,stroke:#bdbdbd,stroke-dasharray: 5 5\n",
    "    style PR fill:#eeeeee,stroke:#bdbdbd,stroke-dasharray: 5 5\n",
    "```\n",
    "\n",
    "## üîë ÌïµÏã¨ ÏïÑÏù¥ÎîîÏñ¥: ÏãúÌÄÄÏä§ Íµ¨ÏÑ± (Interleaving)\n",
    "\n",
    "Decision TransformerÏùò Í∞ÄÏû• ÎèÖÌäπÌïú ÏÑ§Í≥ÑÎäî **ÏÑ∏ Ï¢ÖÎ•òÏùò ÌÜ†ÌÅ∞ÏùÑ Ïù∏ÌÑ∞Î¶¨Îπô**ÌïòÎäî Í≤ÉÏûÖÎãàÎã§:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Sequence[\"üìù ÌÜ†ÌÅ∞ ÏãúÌÄÄÏä§ Íµ¨ÏÑ±\"]\n",
    "        direction LR\n",
    "        R0[\"RÃÇ‚ÇÄ\"] --> S0[\"s‚ÇÄ\"] --> A0[\"a‚ÇÄ\"]\n",
    "        A0 --> R1[\"RÃÇ‚ÇÅ\"] --> S1[\"s‚ÇÅ\"] --> A1[\"a‚ÇÅ\"]\n",
    "        A1 --> R2[\"RÃÇ‚ÇÇ\"] --> S2[\"s‚ÇÇ\"] --> A2[\"a‚ÇÇ\"]\n",
    "    end\n",
    "\n",
    "    subgraph Predict[\"üéØ ÏòàÏ∏° Î∞©Ìñ•\"]\n",
    "        direction TB\n",
    "        P1[\"State ÌÜ†ÌÅ∞ ÏúÑÏπòÏóêÏÑú\\nActionÏùÑ ÏòàÏ∏°\"]\n",
    "        P2[\"Ï¶â, s‚ÇÇ ÏúÑÏπòÏùò Ï∂úÎ†•ÏúºÎ°ú\\na‚ÇÇÎ•º ÏòàÏ∏°\"]\n",
    "    end\n",
    "\n",
    "    Sequence --> Predict\n",
    "\n",
    "    style R0 fill:#ffcdd2\n",
    "    style R1 fill:#ffcdd2\n",
    "    style R2 fill:#ffcdd2\n",
    "    style S0 fill:#c8e6c9\n",
    "    style S1 fill:#c8e6c9\n",
    "    style S2 fill:#c8e6c9\n",
    "    style A0 fill:#bbdefb\n",
    "    style A1 fill:#bbdefb\n",
    "    style A2 fill:#bbdefb\n",
    "```\n",
    "\n",
    "## üîç GPT-2 Transformer Î∏îÎ°ùÏùò Attention ÎèôÏûë\n",
    "\n",
    "### Self-AttentionÏù¥ÎûÄ?\n",
    "\n",
    "Í∞Å ÌÜ†ÌÅ∞Ïù¥ **\"Îã§Î•∏ ÌÜ†ÌÅ∞Îì§ Ï§ë Ïñ¥ÎîîÎ•º ÏñºÎßàÎÇò Ï∞∏Í≥†Ìï†ÏßÄ\"** Í∞ÄÏ§ëÏπòÎ•º Í≥ÑÏÇ∞ÌïòÎäî Í≤ÉÏûÖÎãàÎã§.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph SA[\"Self-Attention ÌïµÏã¨ Ïó∞ÏÇ∞\"]\n",
    "        direction TB\n",
    "        Input[\"ÏûÖÎ†• ÌÜ†ÌÅ∞ x\"] --> Q[\"Q = x √ó Wq\n",
    "(Query: ÎÇ¥Í∞Ä Ï∞æÎäî Í≤É)\"]\n",
    "        Input --> K[\"K = x √ó Wk\n",
    "(Key: ÎÇ¥Í∞Ä Ï†úÍ≥µÌïòÎäî Í≤É)\"]\n",
    "        Input --> V[\"V = x √ó Wv\n",
    "(Value: Ïã§Ï†ú ÎÇ¥Ïö©)\"]\n",
    "\n",
    "        Q --> Score[\"Score = Q √ó K·µÄ / ‚àöd\n",
    "(Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞)\"]\n",
    "        K --> Score\n",
    "        Score --> Softmax[\"Softmax\n",
    "(Í∞ÄÏ§ëÏπò Ï†ïÍ∑úÌôî)\"]\n",
    "        Softmax --> Output[\"Output = Í∞ÄÏ§ëÏπò √ó V\n",
    "(Í∞ÄÏ§ë Ìï©ÏÇ∞)\"]\n",
    "        V --> Output\n",
    "    end\n",
    "\n",
    "    style Q fill:#ffcdd2\n",
    "    style K fill:#c8e6c9\n",
    "    style V fill:#bbdefb\n",
    "```\n",
    "\n",
    "### DTÏóêÏÑúÏùò Attention ÏòàÏãú (K=3, Ï¥ù 9ÌÜ†ÌÅ∞)\n",
    "\n",
    "ÏãúÌÄÄÏä§: `[RÃÇ‚ÇÄ, s‚ÇÄ, a‚ÇÄ, RÃÇ‚ÇÅ, s‚ÇÅ, a‚ÇÅ, RÃÇ‚ÇÇ, s‚ÇÇ, a‚ÇÇ]`\n",
    "\n",
    "s‚ÇÅ ÏúÑÏπòÏóêÏÑú actionÏùÑ ÏòàÏ∏°Ìï† Îïå, attentionÏù¥ ÌïòÎäî Ïùº:\n",
    "\n",
    "```\n",
    "s‚ÇÅÏùò Query: \"actionÏùÑ Í≤∞Ï†ïÌïòÎ†§Î©¥ Ïñ¥Îñ§ Ï†ïÎ≥¥Í∞Ä ÌïÑÏöîÌïòÏßÄ?\"\n",
    "          ‚Üì\n",
    "Í∞Å Í≥ºÍ±∞ ÌÜ†ÌÅ∞Ïùò KeyÏôÄ ÎπÑÍµê:\n",
    "   RÃÇ‚ÇÄ: 0.05  (Î®º Í≥ºÍ±∞Ïùò Î™©Ìëú ‚Üí ÏïΩÌïú Í¥ÄÏã¨)\n",
    "   s‚ÇÄ: 0.10  (Ïù¥Ï†Ñ ÏÉÅÌÉú ‚Üí Î≥¥ÌÜµ Í¥ÄÏã¨)\n",
    "   a‚ÇÄ: 0.08  (Ïù¥Ï†Ñ ÌñâÎèô ‚Üí Î≥¥ÌÜµ Í¥ÄÏã¨)\n",
    "   RÃÇ‚ÇÅ: 0.35  (ÌòÑÏû¨ Î™©Ìëú! ‚Üí Í∞ïÌïú Í¥ÄÏã¨ ‚≠ê)\n",
    "   s‚ÇÅ: 0.42  (ÌòÑÏû¨ ÏÉÅÌÉú! ‚Üí Í∞ÄÏû• Í∞ïÌïú Í¥ÄÏã¨ ‚≠ê)\n",
    "   a‚ÇÅ: ‚ùå    (ÎØ∏Îûò ‚Üí Î≥º Ïàò ÏóÜÏùå, Causal Mask)\n",
    "   RÃÇ‚ÇÇ: ‚ùå\n",
    "   s‚ÇÇ: ‚ùå\n",
    "   a‚ÇÇ: ‚ùå\n",
    "          ‚Üì\n",
    "Output = 0.05√óV(RÃÇ‚ÇÄ) + 0.10√óV(s‚ÇÄ) + 0.08√óV(a‚ÇÄ) + 0.35√óV(RÃÇ‚ÇÅ) + 0.42√óV(s‚ÇÅ)\n",
    "```\n",
    "\n",
    "### Causal Attention Mask\n",
    "\n",
    "GPT-2Ïùò **Causal Mask**Îäî ÎØ∏Îûò ÌÜ†ÌÅ∞ÏùÑ Î≥º Ïàò ÏóÜÍ≤å Ï∞®Îã®Ìï©ÎãàÎã§:\n",
    "\n",
    "```\n",
    "              RÃÇ‚ÇÄ  s‚ÇÄ  a‚ÇÄ  RÃÇ‚ÇÅ  s‚ÇÅ  a‚ÇÅ  RÃÇ‚ÇÇ  s‚ÇÇ  a‚ÇÇ\n",
    "         RÃÇ‚ÇÄ [ ‚úÖ  ‚ùå  ‚ùå  ‚ùå  ‚ùå  ‚ùå  ‚ùå  ‚ùå  ‚ùå ]\n",
    "         s‚ÇÄ [ ‚úÖ  ‚úÖ  ‚ùå  ‚ùå  ‚ùå  ‚ùå  ‚ùå  ‚ùå  ‚ùå ]\n",
    "         a‚ÇÄ [ ‚úÖ  ‚úÖ  ‚úÖ  ‚ùå  ‚ùå  ‚ùå  ‚ùå  ‚ùå  ‚ùå ]\n",
    "         RÃÇ‚ÇÅ [ ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚ùå  ‚ùå  ‚ùå  ‚ùå  ‚ùå ]\n",
    "    ‚≠ê   s‚ÇÅ [ ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚ùå  ‚ùå  ‚ùå  ‚ùå ]  ‚Üê Ïó¨Í∏∞ÏÑú a‚ÇÅ ÏòàÏ∏°\n",
    "         a‚ÇÅ [ ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚ùå  ‚ùå  ‚ùå ]\n",
    "         RÃÇ‚ÇÇ [ ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚ùå  ‚ùå ]\n",
    "         s‚ÇÇ [ ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚ùå ]\n",
    "         a‚ÇÇ [ ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ  ‚úÖ ]\n",
    "\n",
    "‚úÖ = attention Í∞ÄÎä• (Ï∞∏Í≥†Ìï®)\n",
    "‚ùå = attention Î∂àÍ∞Ä (ÎØ∏ÎûòÎùºÏÑú Ï∞®Îã®)\n",
    "```\n",
    "\n",
    "> üí° s‚ÇÅ ÏúÑÏπòÏóêÏÑú a‚ÇÅÏùÑ ÏòàÏ∏°Ìï† Îïå, **RÃÇ‚ÇÄ, s‚ÇÄ, a‚ÇÄ, RÃÇ‚ÇÅ, s‚ÇÅ** ÍπåÏßÄÎßå Î≥º Ïàò ÏûàÏäµÎãàÎã§.  \n",
    "> Ï¶â \"ÌòÑÏû¨ Î™©Ìëú(RÃÇ‚ÇÅ)ÏôÄ ÌòÑÏû¨ ÏÉÅÌÉú(s‚ÇÅ)Î•º Î≥¥Í≥† Ïñ¥Îñ§ ÌñâÎèôÏùÑ Ìï¥Ïïº Ìï†ÏßÄ\" Í≤∞Ï†ïÌï©ÎãàÎã§.  \n",
    "> Ï∂îÎ°† ÏãúÏóêÎèÑ ÎØ∏ÎûòÎ•º Î™®Î•¥Í∏∞ ÎïåÎ¨∏Ïóê, ÌïôÏäµ ÏãúÏóêÎèÑ ÎèôÏùºÌïú Ï°∞Í±¥ÏùÑ ÎßåÎì§Ïñ¥Ï£ºÎäî Í≤ÉÏûÖÎãàÎã§.\n",
    "\n",
    "## üí° Ïª¥Ìè¨ÎÑåÌä∏Î≥Ñ ÏöîÏïΩ\n",
    "\n",
    "| Íµ¨ÏÑ± ÏöîÏÜå | Ïó≠Ìï† | ÌÅ¨Í∏∞ ÏòàÏãú |\n",
    "|:---|:---|:---|\n",
    "| `embed_state` | ÏÉÅÌÉú Î≤°ÌÑ∞Î•º hidden Ï∞®ÏõêÏúºÎ°ú Î≥ÄÌôò | 11 ‚Üí 128 |\n",
    "| `embed_action` | ÌñâÎèô Î≤°ÌÑ∞Î•º hidden Ï∞®ÏõêÏúºÎ°ú Î≥ÄÌôò | 3 ‚Üí 128 |\n",
    "| `embed_return` | RTG Ïä§ÏπºÎùºÎ•º hidden Ï∞®ÏõêÏúºÎ°ú Î≥ÄÌôò | 1 ‚Üí 128 |\n",
    "| `embed_timestep` | ÏãúÍ∞Ñ Ï†ïÎ≥¥ Ï∂îÍ∞Ä (Ï†àÎåÄ ÏúÑÏπò) | 1000 √ó 128 |\n",
    "| `transformer` | ÏãúÌÄÄÏä§ Ìå®ÌÑ¥ ÌïôÏäµ (GPT-2) | 3 layers |\n",
    "| `predict_action` | state ÌÜ†ÌÅ∞ ‚Üí action ÏòàÏ∏° | 128 ‚Üí 3 |\n",
    "\n",
    "> üìå **ÏΩîÎìú Ï∞∏Ï°∞**: `gym/decision_transformer/models/decision_transformer.py`Ïùò  \n",
    "> `forward()` Î©îÏÑúÎìú (line 55-99)ÏóêÏÑú Ïù¥ ÌùêÎ¶ÑÏùÑ ÏßÅÏ†ë ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üì¶ DecisionTransformer ÌÅ¥ÎûòÏä§ ÏûÑÌè¨Ìä∏\n",
    "# ============================================================\n",
    "# gym/decision_transformer/models/decision_transformer.pyÏóê Ï†ïÏùòÎêú\n",
    "# DecisionTransformer ÌÅ¥ÎûòÏä§Î•º ÏûÑÌè¨Ìä∏Ìï©ÎãàÎã§.\n",
    "#\n",
    "# ÏûÑÌè¨Ìä∏ Ïã§Ìå® Ïãú SimpleDT(Ïö∞Î¶¨Í∞Ä ÏßÅÏ†ë Íµ¨ÌòÑÌïú Î≤ÑÏ†Ñ)Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    from decision_transformer.models.decision_transformer import DecisionTransformer\n",
    "    print(\"‚úÖ DecisionTransformer ÌÅ¥ÎûòÏä§ Î°úÎìú ÏÑ±Í≥µ!\")\n",
    "    print(\"   ‚îî‚îÄ Í≤ΩÎ°ú: gym/decision_transformer/models/decision_transformer.py\")\n",
    "    DT_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è DecisionTransformer ÏûÑÌè¨Ìä∏ Ïã§Ìå®\")\n",
    "    print(f\"   ‚îî‚îÄ Ïò§Î•ò: {e}\")\n",
    "    print()\n",
    "    print(\"üîß ÎåÄÏïà: SimpleDT (ÏßÅÏ†ë Íµ¨ÌòÑ Î≤ÑÏ†Ñ)Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\")\n",
    "    print(\"   ‚îî‚îÄ ÌïµÏã¨ Íµ¨Ï°∞Îäî ÎèôÏùºÌïòÎ©∞, ÌïôÏäµÏö©ÏúºÎ°ú Ï∂©Î∂ÑÌï©ÎãàÎã§.\")\n",
    "    DT_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üèóÔ∏è Î™®Îç∏ ÏÉùÏÑ±\n",
    "# ============================================================\n",
    "# DecisionTransformer ÎòêÎäî SimpleDT Î™®Îç∏ÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.\n",
    "#\n",
    "# Ï£ºÏöî ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞:\n",
    "#   state_dim (11): HopperÏùò ÏÉÅÌÉú Ï∞®Ïõê\n",
    "#   act_dim (3): HopperÏùò ÌñâÎèô Ï∞®Ïõê\n",
    "#   max_length (K=20): Ïª®ÌÖçÏä§Ìä∏ Í∏∏Ïù¥ (Ìïú Î≤àÏóê Î≥¥Îäî Í≥ºÍ±∞ Ïä§ÌÖù Ïàò)\n",
    "#   hidden_size (128): Ìä∏ÎûúÏä§Ìè¨Î®∏Ïùò hidden dimension\n",
    "#   n_layer (3): Ìä∏ÎûúÏä§Ìè¨Î®∏ Î†àÏù¥Ïñ¥ Ïàò\n",
    "#   n_head (1): Attention head Ïàò\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Hopper ÌôòÍ≤Ω ÏÑ§Ï†ïÍ∞í\n",
    "STATE_DIM = 11   # Hopper state dimension\n",
    "ACT_DIM = 3      # Hopper action dimension\n",
    "CONTEXT_LEN = 20 # K: Ìïú Î≤àÏóê Î≥¥Îäî Í≥ºÍ±∞ Ïä§ÌÖù Ïàò\n",
    "\n",
    "print(\"üèóÔ∏è Î™®Îç∏ ÏÉùÏÑ±\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if DT_AVAILABLE:\n",
    "    # ============================\n",
    "    # Ïã§Ï†ú DecisionTransformer ÏÇ¨Ïö©\n",
    "    # ============================\n",
    "    model = DecisionTransformer(\n",
    "        state_dim=STATE_DIM,      # ÏÉÅÌÉú Ï∞®Ïõê\n",
    "        act_dim=ACT_DIM,          # ÌñâÎèô Ï∞®Ïõê\n",
    "        max_length=CONTEXT_LEN,   # Ïª®ÌÖçÏä§Ìä∏ Í∏∏Ïù¥ (K)\n",
    "        max_ep_len=1000,          # ÏµúÎåÄ ÏóêÌîºÏÜåÎìú Í∏∏Ïù¥\n",
    "        hidden_size=128,          # hidden dimension\n",
    "        n_layer=3,                # Ìä∏ÎûúÏä§Ìè¨Î®∏ Î†àÏù¥Ïñ¥ Ïàò\n",
    "        n_head=1,                 # Attention head Ïàò\n",
    "        n_inner=128*4,            # FFN ÎÇ¥Î∂Ä Ï∞®Ïõê (Î≥¥ÌÜµ hidden*4)\n",
    "        activation_function='relu',\n",
    "        n_positions=1024,         # ÏúÑÏπò ÏûÑÎ≤†Îî© ÏµúÎåÄ Í∏∏Ïù¥\n",
    "        resid_pdrop=0.1,          # Residual dropout\n",
    "        attn_pdrop=0.1,           # Attention dropout\n",
    "    )\n",
    "    print(\"‚úÖ DecisionTransformer Î™®Îç∏ ÏÉùÏÑ± ÏôÑÎ£å\")\n",
    "    \n",
    "else:\n",
    "    # ============================\n",
    "    # SimpleDT: ÏßÅÏ†ë Íµ¨ÌòÑÌïú Í≤ΩÎüâ Î≤ÑÏ†Ñ\n",
    "    # ============================\n",
    "    # ÌïµÏã¨ Íµ¨Ï°∞Îäî DecisionTransformerÏôÄ ÎèôÏùºÌï©ÎãàÎã§.\n",
    "    # PyTorchÏùò Í∏∞Î≥∏ TransformerEncoderÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
    "    \n",
    "    class SimpleEmbeddings(nn.Module):\n",
    "        \"\"\"\n",
    "        ÏûÑÎ≤†Îî© Î†àÏù¥Ïñ¥: RTG, State, ActionÏùÑ hidden spaceÎ°ú Î≥ÄÌôò\n",
    "        \n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ  RTG (1)    ‚îÇ     ‚îÇ State (11)  ‚îÇ     ‚îÇ Action (3)  ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "               ‚îÇ Linear            ‚îÇ Linear           ‚îÇ Linear\n",
    "               ‚ñº                   ‚ñº                   ‚ñº\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ   (128)     ‚îÇ     ‚îÇ   (128)     ‚îÇ     ‚îÇ   (128)     ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "               ‚îÇ    + timestep embedding    ‚îÇ\n",
    "               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                               ‚ñº\n",
    "                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                        ‚îÇ  LayerNorm  ‚îÇ\n",
    "                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        \"\"\"\n",
    "        def __init__(self, state_dim, action_dim, hidden_size, max_timestep=1000):\n",
    "            super().__init__()\n",
    "            # Í∞Å Î™®Îã¨Î¶¨Ìã∞Î≥Ñ ÏûÑÎ≤†Îî© Î†àÏù¥Ïñ¥\n",
    "            self.embed_state = nn.Linear(state_dim, hidden_size)\n",
    "            self.embed_action = nn.Linear(action_dim, hidden_size)\n",
    "            self.embed_return = nn.Linear(1, hidden_size)\n",
    "            \n",
    "            # ÏãúÍ∞Ñ Ï†ïÎ≥¥Î•º ÏúÑÌïú ÏûÑÎ≤†Îî© (Î£©ÏóÖ ÌÖåÏù¥Î∏î)\n",
    "            self.embed_timestep = nn.Embedding(max_timestep, hidden_size)\n",
    "            \n",
    "            # Ï†ïÍ∑úÌôî Î†àÏù¥Ïñ¥\n",
    "            self.embed_ln = nn.LayerNorm(hidden_size)\n",
    "            \n",
    "        def forward(self, states, actions, returns_to_go, timesteps):\n",
    "            \"\"\"\n",
    "            ÏûÖÎ†•ÏùÑ ÏûÑÎ≤†Îî©ÌïòÍ≥† ÏãúÌÄÄÏä§Î°ú Íµ¨ÏÑ±\n",
    "            \n",
    "            Args:\n",
    "                states: (batch, seq_len, state_dim)\n",
    "                actions: (batch, seq_len, action_dim)  \n",
    "                returns_to_go: (batch, seq_len, 1)\n",
    "                timesteps: (batch, seq_len)\n",
    "                \n",
    "            Returns:\n",
    "                (batch, seq_len*3, hidden_size)\n",
    "            \"\"\"\n",
    "            batch_size, seq_len = states.shape[0], states.shape[1]\n",
    "            \n",
    "            # Step 1: Í∞Å Î™®Îã¨Î¶¨Ìã∞ ÏûÑÎ≤†Îî©\n",
    "            state_emb = self.embed_state(states)      # (B, K, H)\n",
    "            action_emb = self.embed_action(actions)   # (B, K, H)\n",
    "            return_emb = self.embed_return(returns_to_go)  # (B, K, H)\n",
    "            \n",
    "            # Step 2: ÏãúÍ∞Ñ ÏûÑÎ≤†Îî© Ï∂îÍ∞Ä\n",
    "            time_emb = self.embed_timestep(timesteps)  # (B, K, H)\n",
    "            state_emb = state_emb + time_emb\n",
    "            action_emb = action_emb + time_emb\n",
    "            return_emb = return_emb + time_emb\n",
    "            \n",
    "            # Step 3: (RTG, s, a) ÏàúÏÑúÎ°ú interleave\n",
    "            # [R_0, s_0, a_0, R_1, s_1, a_1, ...]\n",
    "            stacked = torch.stack([return_emb, state_emb, action_emb], dim=2)\n",
    "            token_embeddings = stacked.reshape(batch_size, seq_len * 3, -1)\n",
    "            \n",
    "            # Step 4: LayerNorm Ï†ÅÏö©\n",
    "            return self.embed_ln(token_embeddings)\n",
    "\n",
    "    class SimpleDT(nn.Module):\n",
    "        \"\"\"\n",
    "        Í∞ÑÏÜåÌôîÎêú Decision Transformer\n",
    "        \n",
    "        PyTorchÏùò TransformerEncoderÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Íµ¨ÌòÑ\n",
    "        \"\"\"\n",
    "        def __init__(self, state_dim, action_dim, hidden_size=128, n_layer=3, n_head=1):\n",
    "            super().__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            \n",
    "            # ÏûÑÎ≤†Îî© Î†àÏù¥Ïñ¥\n",
    "            self.embeddings = SimpleEmbeddings(state_dim, action_dim, hidden_size)\n",
    "            \n",
    "            # Ìä∏ÎûúÏä§Ìè¨Î®∏ Ïù∏ÏΩîÎçî\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_size, \n",
    "                nhead=n_head,\n",
    "                dim_feedforward=hidden_size * 4,  # FFN Ï∞®Ïõê\n",
    "                dropout=0.1, \n",
    "                batch_first=True  # (batch, seq, feature) ÏàúÏÑú ÏÇ¨Ïö©\n",
    "            )\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layer)\n",
    "            \n",
    "            # Action ÏòàÏ∏° Ìó§Îìú\n",
    "            # state ÌÜ†ÌÅ∞Ïùò Ï∂úÎ†•ÏóêÏÑú actionÏùÑ ÏòàÏ∏°\n",
    "            self.predict_action = nn.Sequential(\n",
    "                nn.Linear(hidden_size, action_dim),\n",
    "                nn.Tanh()  # ActionÏùÑ [-1, 1] Î≤îÏúÑÎ°ú Ï†úÌïú\n",
    "            )\n",
    "            \n",
    "        def forward(self, states, actions, returns_to_go, timesteps, attention_mask=None):\n",
    "            \"\"\"\n",
    "            Forward pass\n",
    "            \n",
    "            Returns:\n",
    "                (state_preds, action_preds, return_preds)\n",
    "                - action_predsÎßå ÏÇ¨Ïö© (ÎÇòÎ®∏ÏßÄÎäî None)\n",
    "            \"\"\"\n",
    "            batch_size, seq_len = states.shape[0], states.shape[1]\n",
    "            \n",
    "            # Step 1: ÏûÑÎ≤†Îî©\n",
    "            token_embeddings = self.embeddings(states, actions, returns_to_go, timesteps)\n",
    "            # Shape: (batch, seq_len*3, hidden_size)\n",
    "            \n",
    "            # Step 2: Causal mask ÏÉùÏÑ± (ÎØ∏Îûò Ï†ïÎ≥¥ Ï∞®Îã®)\n",
    "            seq_len_3 = seq_len * 3\n",
    "            causal_mask = torch.triu(\n",
    "                torch.ones(seq_len_3, seq_len_3) * float('-inf'), \n",
    "                diagonal=1\n",
    "            ).to(states.device)\n",
    "            \n",
    "            # Step 3: Ìä∏ÎûúÏä§Ìè¨Î®∏ ÌÜµÍ≥º\n",
    "            hidden_states = self.transformer(token_embeddings, mask=causal_mask)\n",
    "            \n",
    "            # Step 4: Action ÏòàÏ∏° (state ÌÜ†ÌÅ∞ ÏúÑÏπòÏóêÏÑú)\n",
    "            # ÌÜ†ÌÅ∞ ÏàúÏÑú: [R_0, s_0, a_0, R_1, s_1, a_1, ...]\n",
    "            # state ÏúÑÏπò: 1, 4, 7, ... (1::3)\n",
    "            action_hidden = hidden_states[:, 1::3, :]  # state ÌÜ†ÌÅ∞Îßå ÏÑ†ÌÉù\n",
    "            action_preds = self.predict_action(action_hidden)\n",
    "            \n",
    "            return None, action_preds, None\n",
    "    \n",
    "    model = SimpleDT(state_dim=STATE_DIM, action_dim=ACT_DIM)\n",
    "    print(\"‚úÖ SimpleDT Î™®Îç∏ ÏÉùÏÑ± ÏôÑÎ£å\")\n",
    "\n",
    "# Î™®Îç∏ Ï†ïÎ≥¥ Ï∂úÎ†•\n",
    "print(f\"\\nüìê Î™®Îç∏ ÏÑ§Ï†ï:\")\n",
    "print(f\"   State dimension:  {STATE_DIM}\")\n",
    "print(f\"   Action dimension: {ACT_DIM}\")\n",
    "print(f\"   Context length:   {CONTEXT_LEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìä Î™®Îç∏ Íµ¨Ï°∞ Î∞è ÌååÎùºÎØ∏ÌÑ∞ Î∂ÑÏÑù\n",
    "# ============================================================\n",
    "# Î™®Îç∏Ïùò Íµ¨Ï°∞ÏôÄ ÌïôÏäµ Í∞ÄÎä•Ìïú ÌååÎùºÎØ∏ÌÑ∞ ÏàòÎ•º ÌôïÏù∏Ìï©ÎãàÎã§.\n",
    "# Ïù¥Î•º ÌÜµÌï¥ Î™®Îç∏Ïùò Î≥µÏû°ÎèÑÎ•º Ïù¥Ìï¥Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìä Î™®Îç∏ Íµ¨Ï°∞ Î∂ÑÏÑù\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Î™®Îç∏ Íµ¨Ï°∞ Ï∂úÎ†•\n",
    "print(\"\\nüèóÔ∏è Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò:\")\n",
    "print(\"-\"*60)\n",
    "print(model)\n",
    "print(\"-\"*60)\n",
    "\n",
    "# ÌååÎùºÎØ∏ÌÑ∞ Ïàò Í≥ÑÏÇ∞\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìà ÌååÎùºÎØ∏ÌÑ∞ ÌÜµÍ≥Ñ:\")\n",
    "print(f\"   Ï¥ù ÌååÎùºÎØ∏ÌÑ∞ Ïàò:     {total_params:>12,}\")\n",
    "print(f\"   ÌïôÏäµ Í∞ÄÎä• ÌååÎùºÎØ∏ÌÑ∞: {trainable_params:>12,}\")\n",
    "\n",
    "# Î†àÏù¥Ïñ¥Î≥Ñ ÌååÎùºÎØ∏ÌÑ∞ Î∂ÑÏÑù\n",
    "print(f\"\\nüìã Î†àÏù¥Ïñ¥Î≥Ñ ÌååÎùºÎØ∏ÌÑ∞ Ïàò:\")\n",
    "print(f\"{'Î†àÏù¥Ïñ¥':<40} {'ÌååÎùºÎØ∏ÌÑ∞':>15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # Ïù¥Î¶Ñ Ï∂ïÏïΩ\n",
    "    short_name = name if len(name) <= 38 else \"...\" + name[-35:]\n",
    "    print(f\"{short_name:<40} {param.numel():>15,}\")\n",
    "\n",
    "# Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ Ï∂îÏ†ï\n",
    "param_memory = total_params * 4 / (1024 * 1024)  # float32 = 4 bytes\n",
    "print(f\"\\nüíæ Ï∂îÏ†ï Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ:\")\n",
    "print(f\"   ÌååÎùºÎØ∏ÌÑ∞: ~{param_memory:.2f} MB\")\n",
    "print(f\"   (Ïã§Ï†ú ÌïôÏäµ Ïãú gradient Îì±ÏúºÎ°ú ÏïΩ 3-4Î∞∞ ÌïÑÏöî)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üîÑ Forward Pass ÌÖåÏä§Ìä∏\n",
    "# ============================================================\n",
    "# Î™®Îç∏Ïóê ÎçîÎØ∏ ÏûÖÎ†•ÏùÑ ÎÑ£Ïñ¥ Ï∂úÎ†• shapeÏùÑ ÌôïÏù∏Ìï©ÎãàÎã§.\n",
    "# Ïù¥Î•º ÌÜµÌï¥ Îç∞Ïù¥ÌÑ∞ ÌùêÎ¶ÑÏùÑ Ïù¥Ìï¥Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n",
    "#\n",
    "# ÏûÖÎ†• ‚Üí Ï∂úÎ†• ÌùêÎ¶Ñ:\n",
    "#   (states, actions, rtg, timesteps)\n",
    "#       ‚Üì\n",
    "#   DecisionTransformer\n",
    "#       ‚Üì\n",
    "#   (state_preds, action_preds, return_preds)\n",
    "#       ‚îî‚îÄ action_predsÎßå ÏÇ¨Ïö©\n",
    "# ============================================================\n",
    "\n",
    "batch_size = 4\n",
    "seq_len = CONTEXT_LEN  # K = 20\n",
    "\n",
    "print(\"üîÑ Forward Pass ÌÖåÏä§Ìä∏\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ÎçîÎØ∏ ÏûÖÎ†• ÏÉùÏÑ±\n",
    "states = torch.randn(batch_size, seq_len, STATE_DIM)     # (B, K, 11)\n",
    "actions = torch.randn(batch_size, seq_len, ACT_DIM)      # (B, K, 3)\n",
    "returns_to_go = torch.randn(batch_size, seq_len, 1)      # (B, K, 1)\n",
    "timesteps = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)  # (B, K)\n",
    "attention_mask = torch.ones(batch_size, seq_len)         # (B, K)\n",
    "\n",
    "print(\"\\nüì• ÏûÖÎ†• Shapes:\")\n",
    "print(f\"   states:        {states.shape}\")\n",
    "print(f\"                  ‚îî‚îÄ (batch, context_len, state_dim)\")\n",
    "print(f\"   actions:       {actions.shape}\")\n",
    "print(f\"                  ‚îî‚îÄ (batch, context_len, action_dim)\")\n",
    "print(f\"   returns_to_go: {returns_to_go.shape}\")\n",
    "print(f\"                  ‚îî‚îÄ (batch, context_len, 1)\")\n",
    "print(f\"   timesteps:     {timesteps.shape}\")\n",
    "print(f\"                  ‚îî‚îÄ (batch, context_len)\")\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    if DT_AVAILABLE:\n",
    "        # Ïã§Ï†ú DecisionTransformer API\n",
    "        state_preds, action_preds, return_preds = model(\n",
    "            states, actions, None, returns_to_go, timesteps, attention_mask\n",
    "        )\n",
    "    else:\n",
    "        # SimpleDT API\n",
    "        state_preds, action_preds, return_preds = model(\n",
    "            states, actions, returns_to_go, timesteps\n",
    "        )\n",
    "\n",
    "print(f\"\\nüì§ Ï∂úÎ†• Shapes:\")\n",
    "print(f\"   action_preds:  {action_preds.shape}\")\n",
    "print(f\"                  ‚îî‚îÄ (batch, context_len, action_dim)\")\n",
    "\n",
    "# ÎßàÏßÄÎßâ timestepÏùò action ÏòàÏ∏° (Ïã§Ï†ú ÏÇ¨Ïö©ÎêòÎäî Í∞í)\n",
    "print(f\"\\nüéØ ÎßàÏßÄÎßâ timestepÏùò action ÏòàÏ∏°:\")\n",
    "print(f\"   action_preds[0, -1, :] = {action_preds[0, -1, :]}\")\n",
    "print(f\"   ‚îî‚îÄ Ï≤´ Î≤àÏß∏ Î∞∞Ïπò, ÎßàÏßÄÎßâ timestepÏùò ÏòàÏ∏° action\")\n",
    "print()\n",
    "print(\"üí° Ï∂îÎ°† ÏãúÏóêÎäî ÎßàÏßÄÎßâ timestepÏùò actionÎßå ÏÇ¨Ïö©Ìï©ÎãàÎã§!\")\n",
    "print(\"   (Í∞ÄÏû• ÏµúÏã† Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú Ìïú ÏòàÏ∏°Ïù¥Í∏∞ ÎïåÎ¨∏)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.5 üî¨ 10Ïä§ÌÖù Îç∞Ïù¥ÌÑ∞Î°ú Î≥¥Îäî ÌïôÏäµ Í≥ºÏ†ï\n",
    "\n",
    "> Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ **10Ïä§ÌÖù**Îßå Í∫ºÎÇ¥ÏÑú, ÌïôÏäµÏù¥ Ïñ¥ÎñªÍ≤å ÏßÑÌñâÎêòÎäîÏßÄ Ï≤òÏùåÎ∂ÄÌÑ∞ ÎÅùÍπåÏßÄ Îî∞ÎùºÍ∞ëÎãàÎã§.  \n",
    "> Ï∂îÏÉÅÏ†ÅÏù∏ ÏÑ§Î™Ö ÎåÄÏã† **Ïã§Ï†ú Ïà´Ïûê**Í∞Ä Ïñ¥ÎñªÍ≤å Î≥ÄÌïòÎäîÏßÄ ÏßÅÏ†ë ÌôïÏù∏Ìï©ÎãàÎã§.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 1: Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ 10Ïä§ÌÖù Í∫ºÎÇ¥Í∏∞\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "traj = trajectories[0]\n",
    "T = 10  # 10Ïä§ÌÖùÎßå ÏÇ¨Ïö©\n",
    "\n",
    "states_10  = traj['observations'][:T]  # (10, 11)\n",
    "actions_10 = traj['actions'][:T]        # (10, 3)\n",
    "rewards_10 = traj['rewards'][:T]        # (10,)\n",
    "\n",
    "# Î≥¥Í∏∞ ÏâΩÍ≤å DataFrameÏúºÎ°ú\n",
    "state_cols = ['z_pos', 'y_angle', 'thigh_ang', 'leg_ang', 'foot_ang',\n",
    "              'z_vel', 'y_ang_vel', 'thigh_vel', 'leg_vel', 'foot_vel', 'x_vel']\n",
    "action_cols = ['act_thigh', 'act_leg', 'act_foot']\n",
    "\n",
    "df_raw = pd.DataFrame(states_10, columns=state_cols)\n",
    "df_raw[action_cols] = actions_10\n",
    "df_raw['reward'] = rewards_10\n",
    "df_raw.index.name = 't'\n",
    "\n",
    "print('üìã ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ (10Ïä§ÌÖù)')\n",
    "print('Ïù¥Í≤ÉÏù¥ Hopper Î°úÎ¥áÏù¥ 10Î≤à ÏõÄÏßÅÏù∏ Í∏∞Î°ùÏûÖÎãàÎã§.')\n",
    "print()\n",
    "df_raw.round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 2: RTG Í≥ÑÏÇ∞ - \"ÏïûÏúºÎ°ú ÏñºÎßàÎÇò Îçî Î∞õÏùÑ Ïàò ÏûàÎÇò?\"\n",
    "# ============================================================\n",
    "\n",
    "# RTG = ÌòÑÏû¨ ÏãúÏ†êÎ∂ÄÌÑ∞ ÎÅùÍπåÏßÄÏùò Î≥¥ÏÉÅ Ìï©\n",
    "rtg_10 = discount_cumsum(rewards_10, gamma=1.0)\n",
    "\n",
    "df_rtg = pd.DataFrame({\n",
    "    'reward': rewards_10,\n",
    "    'RTG': rtg_10,\n",
    "    'RTG Í≥ÑÏÇ∞': [f'{rtg_10[t]:.3f} = ' + ' + '.join(f'{rewards_10[j]:.3f}' for j in range(t, T))\n",
    "                 for t in range(T)]\n",
    "})\n",
    "df_rtg.index.name = 't'\n",
    "\n",
    "print('üìà RTG Í≥ÑÏÇ∞ Í≥ºÏ†ï')\n",
    "print('Í∞Å ÏãúÏ†êÏóêÏÑú \"ÏïûÏúºÎ°ú ÎÇ®ÏùÄ Î≥¥ÏÉÅÏùò Ìï©\"ÏùÑ Íµ¨Ìï©ÎãàÎã§.')\n",
    "print()\n",
    "df_rtg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 3: Ï†ïÍ∑úÌôî - Î™®Îç∏Ïù¥ Îã§Î£®Í∏∞ Ïâ¨Ïö¥ Î≤îÏúÑÎ°ú Î≥ÄÌôò\n",
    "# ============================================================\n",
    "\n",
    "# Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏóêÏÑú Í≥ÑÏÇ∞Ìïú ÌèâÍ∑†/ÌëúÏ§ÄÌé∏Ï∞®Î°ú Ï†ïÍ∑úÌôî\n",
    "all_states = np.concatenate([t['observations'] for t in trajectories], axis=0)\n",
    "state_mean = np.mean(all_states, axis=0)\n",
    "state_std  = np.std(all_states, axis=0) + 1e-6\n",
    "\n",
    "states_norm = (states_10 - state_mean) / state_std\n",
    "scale = 1000.0\n",
    "rtg_scaled = rtg_10 / scale\n",
    "\n",
    "# ÎπÑÍµê ÌÖåÏù¥Î∏î (Ï≤òÏùå 3Ï∞®ÏõêÎßå)\n",
    "df_norm = pd.DataFrame({\n",
    "    'z_pos (ÏõêÎ≥∏)':      states_10[:, 0].round(4),\n",
    "    'z_pos (Ï†ïÍ∑úÌôî)':    states_norm[:, 0].round(4),\n",
    "    'y_angle (ÏõêÎ≥∏)':    states_10[:, 1].round(4),\n",
    "    'y_angle (Ï†ïÍ∑úÌôî)':  states_norm[:, 1].round(4),\n",
    "    'RTG (ÏõêÎ≥∏)':        rtg_10.round(2),\n",
    "    'RTG (√∑1000)':      rtg_scaled.round(5),\n",
    "})\n",
    "df_norm.index.name = 't'\n",
    "\n",
    "print('üîÑ Ï†ïÍ∑úÌôî Ï†ÑÌõÑ ÎπÑÍµê')\n",
    "print(f'ÏÉÅÌÉú: (x - mean) / std  |  RTG: x / {scale:.0f}')\n",
    "print()\n",
    "df_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 4: Î™®Îç∏Ïóê ÎÑ£Ïñ¥Î≥¥Í∏∞ (ÌïôÏäµ Ï†Ñ)\n",
    "# ============================================================\n",
    "# ÏïÑÏßÅ ÌïôÏäµÌïòÏßÄ ÏïäÏùÄ Î™®Îç∏Ïóê 10Ïä§ÌÖùÏùÑ ÎÑ£ÏúºÎ©¥ Ïñ¥Îñ§ actionÏùÑ ÏòàÏ∏°Ìï†Íπå?\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ÌÖêÏÑú Î≥ÄÌôò (batch=1)\n",
    "s  = torch.tensor(states_norm, dtype=torch.float32).unsqueeze(0).to(device)     # (1, 10, 11)\n",
    "a  = torch.tensor(actions_10, dtype=torch.float32).unsqueeze(0).to(device)      # (1, 10, 3)\n",
    "r  = torch.tensor(rtg_scaled, dtype=torch.float32).reshape(1, T, 1).to(device)  # (1, 10, 1)\n",
    "ts = torch.arange(T, dtype=torch.long).unsqueeze(0).to(device)                  # (1, 10)\n",
    "m  = torch.ones(1, T, dtype=torch.float32).to(device)                           # (1, 10) Ìå®Îî© ÏóÜÏùå\n",
    "\n",
    "with torch.no_grad():\n",
    "    if DT_AVAILABLE:\n",
    "        _, action_preds, _ = model(s, a, None, r, ts, m)\n",
    "    else:\n",
    "        _, action_preds, _ = model(s, a, r, ts)\n",
    "\n",
    "pred_before = action_preds[0].cpu().numpy()  # (10, 3)\n",
    "\n",
    "# ÏòàÏ∏° vs Ïã§Ï†ú ÎπÑÍµê\n",
    "df_pred = pd.DataFrame({\n",
    "    'actual_thigh':  actions_10[:, 0].round(4),\n",
    "    'pred_thigh':    pred_before[:, 0].round(4),\n",
    "    'actual_leg':    actions_10[:, 1].round(4),\n",
    "    'pred_leg':      pred_before[:, 1].round(4),\n",
    "    'actual_foot':   actions_10[:, 2].round(4),\n",
    "    'pred_foot':     pred_before[:, 2].round(4),\n",
    "})\n",
    "df_pred.index.name = 't'\n",
    "\n",
    "mse_before = np.mean((pred_before - actions_10) ** 2)\n",
    "print(f'ü§ñ ÌïôÏäµ Ï†Ñ Î™®Îç∏Ïùò ÏòàÏ∏° (MSE: {mse_before:.4f})')\n",
    "print('ÏòàÏ∏°Ïù¥ Ïã§Ï†úÏôÄ Ï†ÑÌòÄ Îã§Î¶ÖÎãàÎã§ - ÏïÑÏßÅ ÏïÑÎ¨¥Í≤ÉÎèÑ Î∞∞Ïö∞ÏßÄ ÏïäÏïòÏúºÎãàÍπåÏöî!')\n",
    "print()\n",
    "df_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 5: Ïù¥ 10Ïä§ÌÖù Îç∞Ïù¥ÌÑ∞Î°ú ÌïôÏäµÏãúÌÇ§Í∏∞\n",
    "# ============================================================\n",
    "# Í∞ôÏùÄ 10Ïä§ÌÖù Îç∞Ïù¥ÌÑ∞Î•º Î∞òÎ≥µÌï¥ÏÑú Î™®Îç∏Ïóê Î≥¥Ïó¨Ï£ºÎ©¥,\n",
    "# Î™®Îç∏Ïù¥ Ï†êÏ†ê Ïò¨Î∞îÎ•∏ actionÏùÑ ÏòàÏ∏°ÌïòÍ≤å Îê©ÎãàÎã§.\n",
    "# ============================================================\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "model.train()\n",
    "\n",
    "losses = []\n",
    "print('üöÄ ÌïôÏäµ ÏãúÏûë (Í∞ôÏùÄ 10Ïä§ÌÖùÏùÑ 100Î≤à Î∞òÎ≥µ)')\n",
    "print('='*60)\n",
    "\n",
    "for step in range(100):\n",
    "    # Forward pass\n",
    "    if DT_AVAILABLE:\n",
    "        _, action_preds_train, _ = model(s, a, None, r, ts, m)\n",
    "    else:\n",
    "        _, action_preds_train, _ = model(s, a, r, ts)\n",
    "\n",
    "    # Loss = ÏòàÏ∏° actionÍ≥º Ïã§Ï†ú actionÏùò Ï∞®Ïù¥\n",
    "    loss = torch.mean((action_preds_train - a) ** 2)\n",
    "\n",
    "    # Ïó≠Ï†ÑÌåå\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if step % 20 == 0 or step == 99:\n",
    "        print(f'  Step {step:>3}: Loss = {loss.item():.6f}')\n",
    "\n",
    "print(f'\\nüìâ Loss Î≥ÄÌôî: {losses[0]:.4f} ‚Üí {losses[-1]:.4f}')\n",
    "print(f'   {(1 - losses[-1]/losses[0]) * 100:.1f}% Í∞êÏÜå!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 6: ÌïôÏäµ ÌõÑ Í∞ôÏùÄ Îç∞Ïù¥ÌÑ∞Î°ú Îã§Ïãú ÏòàÏ∏°\n",
    "# ============================================================\n",
    "# ÌïôÏäµ Ï†ÑÍ≥º ÌõÑÎ•º ÎÇòÎûÄÌûà ÎπÑÍµêÌï©ÎãàÎã§.\n",
    "# ============================================================\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    if DT_AVAILABLE:\n",
    "        _, action_preds_after, _ = model(s, a, None, r, ts, m)\n",
    "    else:\n",
    "        _, action_preds_after, _ = model(s, a, r, ts)\n",
    "\n",
    "pred_after = action_preds_after[0].cpu().numpy()\n",
    "mse_after = np.mean((pred_after - actions_10) ** 2)\n",
    "\n",
    "# 3Í∞ú action Í∞ÅÍ∞Å ÎπÑÍµê\n",
    "df_compare = pd.DataFrame({\n",
    "    'Ïã§Ï†ú thigh':    actions_10[:, 0].round(4),\n",
    "    'ÌïôÏäµÏ†Ñ ÏòàÏ∏°':    pred_before[:, 0].round(4),\n",
    "    'ÌïôÏäµÌõÑ ÏòàÏ∏°':    pred_after[:, 0].round(4),\n",
    "    'Ïã§Ï†ú leg':      actions_10[:, 1].round(4),\n",
    "    'ÌïôÏäµÏ†Ñ ÏòàÏ∏° ':   pred_before[:, 1].round(4),\n",
    "    'ÌïôÏäµÌõÑ ÏòàÏ∏° ':   pred_after[:, 1].round(4),\n",
    "})\n",
    "df_compare.index.name = 't'\n",
    "\n",
    "print(f'üìä ÌïôÏäµ Ï†ÑÌõÑ ÎπÑÍµê')\n",
    "print(f'   ÌïôÏäµ Ï†Ñ MSE: {mse_before:.6f}')\n",
    "print(f'   ÌïôÏäµ ÌõÑ MSE: {mse_after:.6f}')\n",
    "print()\n",
    "df_compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 7: ÌïôÏäµ Í≥ºÏ†ï ÏãúÍ∞ÅÌôî\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# 1. Loss Î≥ÄÌôî\n",
    "axes[0].plot(losses, linewidth=2)\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Loss Curve')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2~3. Í∞Å action Ï∞®ÏõêÎ≥Ñ ÏòàÏ∏° ÎπÑÍµê\n",
    "for dim, name in enumerate(['thigh', 'leg']):\n",
    "    ax = axes[dim + 1]\n",
    "    ax.plot(actions_10[:, dim], 'ko-', label='actual', linewidth=2, markersize=6)\n",
    "    ax.plot(pred_before[:, dim], 'rx--', label='before', alpha=0.6)\n",
    "    ax.plot(pred_after[:, dim], 'b^-', label='after', alpha=0.8)\n",
    "    ax.set_xlabel('Timestep')\n",
    "    ax.set_ylabel(f'Action ({name})')\n",
    "    ax.set_title(f'{name} action: before vs after')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('üí° Í≤ÄÏùÄ Ï†ê(actual)Ïóê ÌååÎûÄ ÏÇºÍ∞ÅÌòï(after)Ïù¥ Í∞ÄÍπåÏõåÏßÑ Í≤ÉÏùÑ ÌôïÏù∏ÌïòÏÑ∏Ïöî!')\n",
    "print('   Ïù¥Í≤ÉÏù¥ \"ÌïôÏäµ\"ÏûÖÎãàÎã§. Í∞ôÏùÄ ÏÉÅÌô©ÏóêÏÑú Îçî Ï†ïÌôïÌïú ÌñâÎèôÏùÑ ÏòàÏ∏°ÌïòÍ≤å ÎêòÏóàÏäµÎãàÎã§.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîë Ïó¨Í∏∞ÏÑú Î∞∞Ïö¥ Í≤É\n",
    "\n",
    "| Îã®Í≥Ñ | Î¨¥ÏóáÏùÑ ÌñàÎÇò | Ïôú ÌñàÎÇò |\n",
    "|------|-----------|--------|\n",
    "| **Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú** | Í∂§Ï†ÅÏóêÏÑú 10Ïä§ÌÖù Í∫ºÎÉÑ | Î™®Îç∏Ïóê Î≥¥Ïó¨Ï§Ñ \"Í≤ΩÌóò\" Ï§ÄÎπÑ |\n",
    "| **RTG Í≥ÑÏÇ∞** | Í∞Å ÏãúÏ†êÏùò ÎØ∏Îûò Î≥¥ÏÉÅ Ìï©ÏÇ∞ | \"Ïù¥ Ï†ïÎèÑ ÏÑ±Í≥ºÎ•º ÏõêÌï¥\" ÎùºÎäî Ï°∞Í±¥ |\n",
    "| **Ï†ïÍ∑úÌôî** | state √∑ std, RTG √∑ 1000 | Î™®Îç∏Ïù¥ Îã§Î£®Í∏∞ Ïâ¨Ïö¥ Î≤îÏúÑÎ°ú |\n",
    "| **Forward** | Î™®Îç∏Ïóê (RTG, state) ÏûÖÎ†• | Ïñ¥Îñ§ actionÏùÑ Ìï¥Ïïº Ìï†ÏßÄ ÏòàÏ∏° |\n",
    "| **Loss** | MSE(ÏòàÏ∏° action, Ïã§Ï†ú action) | ÏñºÎßàÎÇò ÌãÄÎ†∏ÎäîÏßÄ Ï∏°Ï†ï |\n",
    "| **Backward** | Í∑∏ÎûòÎîîÏñ∏Ìä∏ Í≥ÑÏÇ∞ & ÌååÎùºÎØ∏ÌÑ∞ ÏàòÏ†ï | Îã§ÏùåÏóî Îçú ÌãÄÎ¶¨ÎèÑÎ°ù |\n",
    "\n",
    "> Ïù¥ Í≥ºÏ†ïÏùÑ **ÏàòÎßå Î≤à** Î∞òÎ≥µÌïòÍ≥†, **Îß§Î≤à Îã§Î•∏ Í∂§Ï†ÅÏùò Îã§Î•∏ Íµ¨Í∞Ñ**ÏùÑ ÏÉòÌîåÎßÅÌïòÎ©¥  \n",
    "> ‚Üí Î™®Îç∏Ïù¥ Îã§ÏñëÌïú ÏÉÅÌô©ÏóêÏÑú Ïò¨Î∞îÎ•∏ ÌñâÎèôÏùÑ ÏòàÏ∏°Ìï† Ïàò ÏûàÍ≤å Îê©ÎãàÎã§.  \n",
    "> ‚Üí Ïù¥Í≤ÉÏù¥ Section 4ÏóêÏÑú ÌïòÎäî Î≥∏Í≤©Ï†ÅÏù∏ ÌïôÏäµÏûÖÎãàÎã§.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. ÌïôÏäµ Ïã§Ìñâ\n",
    "\n",
    "## üéØ Î™©Ìëú\n",
    "> Decision Transformer ÌïôÏäµ ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ Íµ¨ÌòÑÌïòÍ≥† Ïã§ÌñâÌï©ÎãàÎã§.\n",
    "\n",
    "## üìä ÌïôÏäµ ÌååÏù¥ÌîÑÎùºÏù∏ Í∞úÏöî\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Pipeline[\"üìä ÌïôÏäµ ÌååÏù¥ÌîÑÎùºÏù∏\"]\n",
    "        direction TB\n",
    "        subgraph Step1[\"1Ô∏è‚É£ Î∞∞Ïπò ÏÉùÏÑ± get_batch\"]\n",
    "            B1[\"trajectoriesÏóêÏÑú\\nÎ¨¥ÏûëÏúÑÎ°ú K Í∏∏Ïù¥Ïùò\\nÏÑúÎ∏åÏãúÌÄÄÏä§ ÏÉòÌîåÎßÅ\"]\n",
    "            B2[\"‚Üí states, actions,\\n  rtg, timesteps, mask\"]\n",
    "            B1 --> B2\n",
    "        end\n",
    "\n",
    "        subgraph Step2[\"2Ô∏è‚É£ Forward Pass\"]\n",
    "            F1[\"Î™®Îç∏Ïóê ÏûÖÎ†•\"]\n",
    "            F2[\"action_preds Ï∂úÎ†•\"]\n",
    "            F1 --> F2\n",
    "        end\n",
    "\n",
    "        subgraph Step3[\"3Ô∏è‚É£ ÏÜêÏã§ Í≥ÑÏÇ∞\"]\n",
    "            L1[\"MSE Loss\"]\n",
    "            L2[\"= mean(action_preds - action_target)¬≤\"]\n",
    "            L1 --> L2\n",
    "        end\n",
    "\n",
    "        subgraph Step4[\"4Ô∏è‚É£ Ïó≠Ï†ÑÌåå & ÏµúÏ†ÅÌôî\"]\n",
    "            O1[\"loss.backward()\"]\n",
    "            O2[\"gradient clipping (0.25)\"]\n",
    "            O3[\"optimizer.step()\"]\n",
    "            O1 --> O2 --> O3\n",
    "        end\n",
    "\n",
    "        Step1 --> Step2 --> Step3 --> Step4\n",
    "        Step4 -.->|\"Î∞òÎ≥µ\"| Step1\n",
    "    end\n",
    "\n",
    "    style Step1 fill:#e3f2fd\n",
    "    style Step2 fill:#fff3e0\n",
    "    style Step3 fill:#ffebee\n",
    "    style Step4 fill:#e8f5e9\n",
    "```\n",
    "\n",
    "## üîë Î∞∞Ïπò ÏÉùÏÑ± Í≥ºÏ†ï ÏÉÅÏÑ∏\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Sampling[\"üì¶ Î∞∞Ïπò ÏÉùÏÑ± ÏÉÅÏÑ∏\"]\n",
    "        direction TB\n",
    "        S1[\"1. Í∂§Ï†Å ÏÑ†ÌÉù\\nÍ∏¥ Í∂§Ï†ÅÏùºÏàòÎ°ù ÏûêÏ£º ÏÑ†ÌÉù\"] --> S2[\"2. ÏãúÏûëÏ†ê Î¨¥ÏûëÏúÑ ÏÑ†ÌÉù\\nsi = random(0, T)\"]\n",
    "        S2 --> S3[\"3. K Í∏∏Ïù¥ ÏÑúÎ∏åÏãúÌÄÄÏä§ Ï∂îÏ∂ú\\ntraj[si : si+K]\"]\n",
    "        S3 --> S4[\"4. RTG Í≥ÑÏÇ∞\\nsiÎ∂ÄÌÑ∞ ÏóêÌîºÏÜåÎìú ÎÅùÍπåÏßÄ\"]\n",
    "        S4 --> S5[\"5. Ìå®Îî© (ÏïûÏ™ΩÏóê 0 Ï∂îÍ∞Ä)\\nKÎ≥¥Îã§ ÏßßÏúºÎ©¥ ÏïûÏóê Ìå®Îî©\"]\n",
    "        S5 --> S6[\"6. TensorÎ°ú Î≥ÄÌôò\"]\n",
    "    end\n",
    "\n",
    "    style Sampling fill:#e3f2fd\n",
    "```\n",
    "\n",
    "## üéØ Ìå®Îî© ÏãúÍ∞ÅÌôî\n",
    "\n",
    "ÏÑúÎ∏åÏãúÌÄÄÏä§ Í∏∏Ïù¥Í∞Ä KÎ≥¥Îã§ ÏßßÏùÄ Í≤ΩÏö∞, **ÏïûÏ™Ω(left)**Ïóê 0ÏùÑ Ï±ÑÏõÅÎãàÎã§:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Before[\"ÏõêÎ≥∏ (Í∏∏Ïù¥ 3)\"]\n",
    "        direction LR\n",
    "        B1[\"s‚ÇÉ\"] --> B2[\"s‚ÇÑ\"] --> B3[\"s‚ÇÖ\"]\n",
    "    end\n",
    "\n",
    "    subgraph After[\"Ìå®Îî© ÌõÑ (Í∏∏Ïù¥ K=20)\"]\n",
    "        direction LR\n",
    "        P1[\"0\"] --> P2[\"0\"] --> P3[\"...\"] --> P4[\"0\"]\n",
    "        P4 --> A1[\"s‚ÇÉ\"] --> A2[\"s‚ÇÑ\"] --> A3[\"s‚ÇÖ\"]\n",
    "    end\n",
    "\n",
    "    Before -->|\"ÏïûÏ™Ω Ìå®Îî©\"| After\n",
    "\n",
    "    style P1 fill:#eeeeee\n",
    "    style P2 fill:#eeeeee\n",
    "    style P3 fill:#eeeeee\n",
    "    style P4 fill:#eeeeee\n",
    "    style A1 fill:#c8e6c9\n",
    "    style A2 fill:#c8e6c9\n",
    "    style A3 fill:#c8e6c9\n",
    "```\n",
    "\n",
    "> üí° **Ïôú ÏïûÏ™ΩÏóê Ìå®Îî©Ìï†ÍπåÏöî?**  \n",
    "> Causal attentionÏóêÏÑú Í∞ÄÏû• ÏµúÍ∑º(Ïò§Î•∏Ï™Ω) ÌÜ†ÌÅ∞Ïù¥ ÏòàÏ∏°Ïóê Í∞ÄÏû• Ï§ëÏöîÌï©ÎãàÎã§.  \n",
    "> ÏïûÏ™Ω Ìå®Îî©ÏùÄ attention maskÎ°ú Î¨¥ÏãúÎêòÎØÄÎ°ú Î™®Îç∏ ÏÑ±Îä•Ïóê ÏòÅÌñ•ÏùÑ Ï£ºÏßÄ ÏïäÏäµÎãàÎã§.\n",
    "\n",
    "## ‚ö° DT ÌïôÏäµ = Supervised Learning!\n",
    "\n",
    "Decision TransformerÏùò ÌïôÏäµÏùÄ Í∏∞Ï°¥ RLÍ≥º ÏôÑÏ†ÑÌûà Îã§Î¶ÖÎãàÎã§:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph TraditionalRL[\"üîÑ Í∏∞Ï°¥ RL ÌïôÏäµ\"]\n",
    "        direction TB\n",
    "        TR1[\"ÌôòÍ≤ΩÍ≥º ÏÉÅÌò∏ÏûëÏö©\"]\n",
    "        TR2[\"Î≥¥ÏÉÅ ÏµúÎåÄÌôî\\n(Bellman Î∞©Ï†ïÏãù)\"]\n",
    "        TR3[\"Online ÌïôÏäµ\"]\n",
    "    end\n",
    "\n",
    "    subgraph DTSL[\"ü§ñ DT ÌïôÏäµ = Supervised Learning\"]\n",
    "        direction TB\n",
    "        DT1[\"Ïò§ÌîÑÎùºÏù∏ Îç∞Ïù¥ÌÑ∞ÏÖã\\n(ÎØ∏Î¶¨ ÏàòÏßëÎêú Í∂§Ï†Å)\"]\n",
    "        DT2[\"Action ÏòàÏ∏°\\n(MSE ÏÜêÏã§)\"]\n",
    "        DT3[\"Offline ÌïôÏäµ\\n(ÌôòÍ≤Ω Î∂àÌïÑÏöî)\"]\n",
    "    end\n",
    "\n",
    "    style TraditionalRL fill:#ffebee\n",
    "    style DTSL fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px\n",
    "```\n",
    "\n",
    "## üí° ÌïµÏã¨ Ìè¨Ïù∏Ìä∏\n",
    "\n",
    "| Îã®Í≥Ñ | ÎÇ¥Ïö© | ÎπÑÍ≥† |\n",
    "|:---|:---|:---|\n",
    "| **Î∞∞Ïπò ÏÉùÏÑ±** | Í∏¥ Í∂§Ï†ÅÏóêÏÑú K Í∏∏Ïù¥ ÏÑúÎ∏åÏãúÌÄÄÏä§ Ï∂îÏ∂ú | Ìå®Îî© ÌïÑÏöî Ïãú ÏïûÏóê Ï∂îÍ∞Ä |\n",
    "| **ÏÜêÏã§ Ìï®Ïàò** | MSE (Mean Squared Error) | Ïó∞ÏÜç action ÏòàÏ∏°Ïóê Ï†ÅÌï© |\n",
    "| **Ï†ïÍ∑úÌôî** | RTGÎ•º scaleÎ°ú ÎÇòÎàî (Ïòà: 1000) | ÌïôÏäµ ÏïàÏ†ïÌôî |\n",
    "| **Gradient Clipping** | max_norm = 0.25 | ÌïôÏäµ ÏïàÏ†ïÌôî |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üì¶ Î∞∞Ïπò ÏÉùÏÑ± Ìï®Ïàò (get_batch)\n",
    "# ============================================================\n",
    "# experiment.pyÏùò get_batch Ìï®ÏàòÎ•º Í∞ÑÏÜåÌôîÌïú Î≤ÑÏ†ÑÏûÖÎãàÎã§.\n",
    "#\n",
    "# Î∞∞Ïπò ÏÉùÏÑ± Í≥ºÏ†ï:\n",
    "#   1. Í∂§Ï†Å ÏÑ†ÌÉù (Í∏¥ Í∂§Ï†ÅÏù¥ Îçî ÏûêÏ£º ÏÑ†ÌÉùÎê®)\n",
    "#   2. ÏãúÏûëÏ†ê Î¨¥ÏûëÏúÑ ÏÑ†ÌÉù\n",
    "#   3. K Í∏∏Ïù¥Ïùò ÏÑúÎ∏åÏãúÌÄÄÏä§ Ï∂îÏ∂ú\n",
    "#   4. RTG Í≥ÑÏÇ∞ (Ìï¥Îãπ ÏãúÏ†êÎ∂ÄÌÑ∞ ÎÅùÍπåÏßÄ)\n",
    "#   5. Ìå®Îî© (ÏïûÏ™ΩÏóê 0 Ï∂îÍ∞Ä)\n",
    "#   6. TensorÎ°ú Î≥ÄÌôò\n",
    "#\n",
    "# Ìå®Îî© ÏãúÍ∞ÅÌôî:\n",
    "#   ÏõêÎ≥∏: [s3, s4, s5]  (Í∏∏Ïù¥ 3)\n",
    "#   Ìå®Îî©: [0, 0, 0, ..., 0, s3, s4, s5]  (Í∏∏Ïù¥ K)\n",
    "#         ‚îú‚îÄ padding ‚îÄ‚î§  ‚îú‚îÄ Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ ‚îÄ‚î§\n",
    "# ============================================================\n",
    "\n",
    "def get_batch(trajectories, batch_size=64, max_len=20, state_dim=11, act_dim=3, scale=1000):\n",
    "    \"\"\"\n",
    "    ÌïôÏäµÏö© Î∞∞Ïπò Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    trajectories : list\n",
    "        Í∂§Ï†Å Îç∞Ïù¥ÌÑ∞ Î¶¨Ïä§Ìä∏\n",
    "    batch_size : int\n",
    "        Î∞∞Ïπò ÌÅ¨Í∏∞\n",
    "    max_len : int\n",
    "        Ïª®ÌÖçÏä§Ìä∏ Í∏∏Ïù¥ (K)\n",
    "    state_dim, act_dim : int\n",
    "        ÏÉÅÌÉú/ÌñâÎèô Ï∞®Ïõê\n",
    "    scale : float\n",
    "        RTG Ï†ïÍ∑úÌôî Ïä§ÏºÄÏùº (Î≥¥ÌÜµ ÌôòÍ≤ΩÏùò ÏµúÎåÄ return)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (states, actions, returns_to_go, timesteps, mask)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Í∂§Ï†Å ÏÑ†ÌÉù (Í∏¥ Í∂§Ï†ÅÏùºÏàòÎ°ù Îçî ÏûêÏ£º ÏÑ†ÌÉù)\n",
    "    # Ïù¥Ïú†: Í∏¥ Í∂§Ï†ÅÏùÄ Îçî ÎßéÏùÄ transitionÏùÑ Ìè¨Ìï®ÌïòÎØÄÎ°ú Í≥µÏ†ïÌïú ÏÉòÌîåÎßÅÏùÑ ÏúÑÌï®\n",
    "    traj_lengths = [len(t['rewards']) for t in trajectories]\n",
    "    total_transitions = sum(traj_lengths)\n",
    "    probabilities = [l / total_transitions for l in traj_lengths]\n",
    "    \n",
    "    batch_inds = np.random.choice(\n",
    "        len(trajectories),\n",
    "        size=batch_size,\n",
    "        replace=True,  # Î≥µÏõê Ï∂îÏ∂ú (Í∞ôÏùÄ Í∂§Ï†ÅÏùÑ Ïó¨Îü¨ Î≤à ÏÑ†ÌÉù Í∞ÄÎä•)\n",
    "        p=probabilities\n",
    "    )\n",
    "    \n",
    "    # Í≤∞Í≥º Ï†ÄÏû•Ïö© Î¶¨Ïä§Ìä∏\n",
    "    states_list, actions_list, rtg_list, timesteps_list, mask_list = [], [], [], [], []\n",
    "    \n",
    "    for i in batch_inds:\n",
    "        traj = trajectories[i]\n",
    "        traj_len = len(traj['rewards'])\n",
    "        \n",
    "        # Step 2: ÏãúÏûëÏ†ê Î¨¥ÏûëÏúÑ ÏÑ†ÌÉù\n",
    "        si = np.random.randint(0, traj_len)\n",
    "        \n",
    "        # Step 3: ÏÑúÎ∏åÏãúÌÄÄÏä§ Ï∂îÏ∂ú (siÎ∂ÄÌÑ∞ ÏµúÎåÄ max_len Í∏∏Ïù¥)\n",
    "        s = traj['observations'][si:si + max_len]\n",
    "        a = traj['actions'][si:si + max_len]\n",
    "        r = traj['rewards'][si:si + max_len]\n",
    "        \n",
    "        # Step 4: RTG Í≥ÑÏÇ∞ (siÎ∂ÄÌÑ∞ ÏóêÌîºÏÜåÎìú ÎÅùÍπåÏßÄ)\n",
    "        # Ï£ºÏùò: si Ïù¥Ï†ÑÏùò rewardÎäî Ïù¥ÎØ∏ Î∞õÏùÄ Í≤ÉÏù¥ÎØÄÎ°ú Ï†úÏô∏\n",
    "        full_rtg = discount_cumsum(traj['rewards'][si:], gamma=1.0)\n",
    "        rtg = full_rtg[:max_len]\n",
    "        \n",
    "        # Timesteps (Ï†àÎåÄ ÏãúÍ∞Ñ)\n",
    "        t = np.arange(si, min(si + max_len, traj_len))\n",
    "        \n",
    "        # Step 5: Ïã§Ï†ú Ï∂îÏ∂úÎêú Í∏∏Ïù¥\n",
    "        actual_len = len(s)\n",
    "        \n",
    "        # Step 6: ÏïûÏ™Ω Ìå®Îî© (max_lenÎ≥¥Îã§ ÏßßÏùÄ Í≤ΩÏö∞)\n",
    "        padding_len = max_len - actual_len\n",
    "        \n",
    "        s = np.concatenate([np.zeros((padding_len, state_dim)), s], axis=0)\n",
    "        a = np.concatenate([np.zeros((padding_len, act_dim)), a], axis=0)\n",
    "        rtg = np.concatenate([np.zeros((padding_len,)), rtg], axis=0)\n",
    "        t = np.concatenate([np.zeros((padding_len,)), t], axis=0)\n",
    "        \n",
    "        # Attention mask: Ìå®Îî©=0, Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞=1\n",
    "        m = np.concatenate([np.zeros((padding_len,)), np.ones((actual_len,))], axis=0)\n",
    "        \n",
    "        states_list.append(s)\n",
    "        actions_list.append(a)\n",
    "        rtg_list.append(rtg)\n",
    "        timesteps_list.append(t)\n",
    "        mask_list.append(m)\n",
    "    \n",
    "    # Step 7: Numpy ‚Üí Tensor Î≥ÄÌôò\n",
    "    states = torch.tensor(np.array(states_list), dtype=torch.float32)\n",
    "    actions = torch.tensor(np.array(actions_list), dtype=torch.float32)\n",
    "    returns_to_go = torch.tensor(np.array(rtg_list), dtype=torch.float32).unsqueeze(-1) / scale\n",
    "    timesteps = torch.tensor(np.array(timesteps_list), dtype=torch.long)\n",
    "    mask = torch.tensor(np.array(mask_list), dtype=torch.float32)\n",
    "    \n",
    "    return states, actions, returns_to_go, timesteps, mask\n",
    "\n",
    "# ============================================================\n",
    "# üß™ Î∞∞Ïπò ÏÉùÏÑ± ÌÖåÏä§Ìä∏\n",
    "# ============================================================\n",
    "print(\"üß™ Î∞∞Ïπò ÏÉùÏÑ± ÌÖåÏä§Ìä∏\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "batch = get_batch(trajectories, batch_size=4, max_len=CONTEXT_LEN)\n",
    "states, actions, rtg, timesteps, mask = batch\n",
    "\n",
    "print(\"\\nüì¶ ÏÉùÏÑ±Îêú Î∞∞Ïπò Shapes:\")\n",
    "print(f\"   states:        {states.shape}\")\n",
    "print(f\"   actions:       {actions.shape}\")\n",
    "print(f\"   returns_to_go: {rtg.shape}\")\n",
    "print(f\"   timesteps:     {timesteps.shape}\")\n",
    "print(f\"   mask:          {mask.shape}\")\n",
    "\n",
    "print(f\"\\nüìä Ï≤´ Î≤àÏß∏ ÏÉòÌîåÏùò mask:\")\n",
    "print(f\"   {mask[0].numpy()}\")\n",
    "print(f\"   ‚îî‚îÄ 0: Ìå®Îî©, 1: Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞\")\n",
    "print(f\"   Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Í∏∏Ïù¥: {int(mask[0].sum())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üöÄ ÌïôÏäµ Î£®ÌîÑ Íµ¨ÌòÑ Î∞è Ïã§Ìñâ\n",
    "# ============================================================\n",
    "# Decision Transformer ÌïôÏäµÏùÑ Ïã§ÌñâÌï©ÎãàÎã§.\n",
    "#\n",
    "# ÌïôÏäµ Î™©Ìëú:\n",
    "#   Ï£ºÏñ¥ÏßÑ (RTG, State) Ï°∞Í±¥ÏóêÏÑú Ïò¨Î∞îÎ•∏ ActionÏùÑ ÏòàÏ∏°ÌïòÎèÑÎ°ù ÌïôÏäµ\n",
    "#\n",
    "# ÏÜêÏã§ Ìï®Ïàò:\n",
    "#   L = MSE(action_preds, action_target)\n",
    "#     = (1/N) Œ£ (ÏòàÏ∏° action - Ïã§Ï†ú action)¬≤\n",
    "#\n",
    "# Ïù¥Í≤ÉÏùÄ Supervised LearningÏûÖÎãàÎã§!\n",
    "#   - ÏûÖÎ†•: (RTG, State, Í≥ºÍ±∞ Actions)\n",
    "#   - ÌÉÄÍ≤ü: Ïã§Ï†ú ÏàòÌñâÎêú Action\n",
    "#   - Q-LearningÏ≤òÎüº Î≥¥ÏÉÅÏùÑ ÏµúÎåÄÌôîÌïòÎäî Í≤ÉÏù¥ ÏïÑÎãò\n",
    "# ============================================================\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"üöÄ ÌïôÏäµ ÏÑ§Ï†ï\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Î™®Îç∏ÏùÑ ÎîîÎ∞îÏù¥Ïä§Î°ú Ïù¥Îèô\n",
    "model = model.to(device)\n",
    "\n",
    "# ÏòµÌã∞ÎßàÏù¥Ï†Ä ÏÑ§Ï†ï\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,           # ÌïôÏäµÎ•†\n",
    "    weight_decay=1e-4  # L2 Ï†ïÍ∑úÌôî\n",
    ")\n",
    "\n",
    "print(f\"   Optimizer: AdamW\")\n",
    "print(f\"   Learning rate: 1e-4\")\n",
    "print(f\"   Weight decay: 1e-4\")\n",
    "\n",
    "# ============================================================\n",
    "# ÌïôÏäµ Ïä§ÌÖù Ìï®Ïàò\n",
    "# ============================================================\n",
    "def train_step(model, optimizer, batch, device):\n",
    "    \"\"\"\n",
    "    Îã®Ïùº ÌïôÏäµ Ïä§ÌÖù ÏàòÌñâ\n",
    "    \n",
    "    Returns:\n",
    "        float: ÏÜêÏã§ Í∞í\n",
    "    \"\"\"\n",
    "    # Îç∞Ïù¥ÌÑ∞Î•º ÎîîÎ∞îÏù¥Ïä§Î°ú Ïù¥Îèô\n",
    "    states, actions, rtg, timesteps, mask = [b.to(device) for b in batch]\n",
    "    \n",
    "    # Forward pass\n",
    "    if DT_AVAILABLE:\n",
    "        state_preds, action_preds, return_preds = model(\n",
    "            states, actions, None, rtg, timesteps, mask\n",
    "        )\n",
    "    else:\n",
    "        state_preds, action_preds, return_preds = model(\n",
    "            states, actions, rtg, timesteps\n",
    "        )\n",
    "    \n",
    "    # ÏÜêÏã§ Í≥ÑÏÇ∞ (MSE)\n",
    "    # action_preds: Î™®Îç∏Ïù¥ ÏòàÏ∏°Ìïú action\n",
    "    # actions: Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞Ïùò action (ÌÉÄÍ≤ü)\n",
    "    action_target = actions\n",
    "    \n",
    "    # MSE ÏÜêÏã§ = ÌèâÍ∑†((ÏòàÏ∏° - ÌÉÄÍ≤ü)¬≤)\n",
    "    loss = torch.mean((action_preds - action_target) ** 2)\n",
    "    \n",
    "    # Ïó≠Ï†ÑÌåå\n",
    "    optimizer.zero_grad()  # Í∑∏ÎûòÎîîÏñ∏Ìä∏ Ï¥àÍ∏∞Ìôî\n",
    "    loss.backward()        # Í∑∏ÎûòÎîîÏñ∏Ìä∏ Í≥ÑÏÇ∞\n",
    "    \n",
    "    # Gradient clipping (ÌïôÏäµ ÏïàÏ†ïÌôî)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.25)\n",
    "    \n",
    "    # ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# ============================================================\n",
    "# ÌïôÏäµ Ïã§Ìñâ\n",
    "# ============================================================\n",
    "print(f\"\\nüèÉ ÌïôÏäµ ÏãúÏûë!\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "num_steps = 100  # ÌïôÏäµ Ïä§ÌÖù Ïàò (Îç∞Î™®Ïö©ÏúºÎ°ú Ï†ÅÍ≤å ÏÑ§Ï†ï)\n",
    "batch_size = 32\n",
    "losses = []\n",
    "\n",
    "model.train()  # ÌïôÏäµ Î™®Îìú\n",
    "\n",
    "for step in tqdm(range(num_steps), desc=\"Training\"):\n",
    "    # Î∞∞Ïπò ÏÉùÏÑ±\n",
    "    batch = get_batch(trajectories, batch_size=batch_size, max_len=CONTEXT_LEN)\n",
    "    \n",
    "    # ÌïôÏäµ Ïä§ÌÖù\n",
    "    loss = train_step(model, optimizer, batch, device)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # ÏßÑÌñâ ÏÉÅÌô© Ï∂úÎ†• (20 Ïä§ÌÖùÎßàÎã§)\n",
    "    if (step + 1) % 20 == 0:\n",
    "        avg_loss = np.mean(losses[-20:])\n",
    "        print(f\"   Step {step+1:>4}: Loss = {loss:.4f} (avg: {avg_loss:.4f})\")\n",
    "\n",
    "# Í≤∞Í≥º ÏöîÏïΩ\n",
    "print(\"-\"*60)\n",
    "print(f\"\\nüìä ÌïôÏäµ Í≤∞Í≥º:\")\n",
    "print(f\"   Ï¥àÍ∏∞ Loss: {losses[0]:.4f}\")\n",
    "print(f\"   ÏµúÏ¢Ö Loss: {losses[-1]:.4f}\")\n",
    "print(f\"   Í∞úÏÑ†Ïú®: {(losses[0] - losses[-1]) / losses[0] * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìà ÌïôÏäµ Í≥°ÏÑ† ÏãúÍ∞ÅÌôî\n",
    "# ============================================================\n",
    "# ÏÜêÏã§(Loss)Ïù¥ ÌïôÏäµ Í≥ºÏ†ïÏóêÏÑú Ïñ¥ÎñªÍ≤å Î≥ÄÌôîÌïòÎäîÏßÄ ÏãúÍ∞ÅÌôîÌï©ÎãàÎã§.\n",
    "# Ï¢ãÏùÄ ÌïôÏäµÏùò ÌäπÏßï:\n",
    "#   - ÏÜêÏã§Ïù¥ Ï†êÏ∞® Í∞êÏÜå\n",
    "#   - ÎÑàÎ¨¥ Í∏âÍ≤©Ìïú Î≥ÄÎèô ÏóÜÏùå\n",
    "#   - ÏàòÎ†¥ Ï∂îÏÑ∏ Î≥¥ÏûÑ\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìà ÌïôÏäµ Í≥°ÏÑ† ÏãúÍ∞ÅÌôî\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Ï†ÑÏ≤¥ ÌïôÏäµ Í≥°ÏÑ†\n",
    "ax1 = axes[0]\n",
    "ax1.plot(losses, alpha=0.7, linewidth=1, label='Loss per step')\n",
    "\n",
    "# Ïù¥Îèô ÌèâÍ∑† (smoothing)\n",
    "window = 10\n",
    "if len(losses) >= window:\n",
    "    moving_avg = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(losses)), moving_avg, \n",
    "             color='red', linewidth=2, label=f'Moving avg (window={window})')\n",
    "\n",
    "ax1.set_xlabel('Training Step', fontsize=12)\n",
    "ax1.set_ylabel('Loss (MSE)', fontsize=12)\n",
    "ax1.set_title('Training Loss Curve', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. ÏÜêÏã§ Î∂ÑÌè¨ (ÌûàÏä§ÌÜ†Í∑∏Îû®)\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Ï≤òÏùå 20%ÏôÄ ÎßàÏßÄÎßâ 20% ÎπÑÍµê\n",
    "first_portion = losses[:len(losses)//5]\n",
    "last_portion = losses[-len(losses)//5:]\n",
    "\n",
    "ax2.hist(first_portion, bins=20, alpha=0.6, label='First 20%', color='red')\n",
    "ax2.hist(last_portion, bins=20, alpha=0.6, label='Last 20%', color='green')\n",
    "ax2.set_xlabel('Loss Value', fontsize=12)\n",
    "ax2.set_ylabel('Frequency', fontsize=12)\n",
    "ax2.set_title('Loss Distribution: Beginning vs End', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ÌÜµÍ≥Ñ Ï∂úÎ†•\n",
    "print(f\"\\nüìä ÏÜêÏã§ ÌÜµÍ≥Ñ:\")\n",
    "print(f\"   Ï≤òÏùå 20% ÌèâÍ∑†: {np.mean(first_portion):.4f}\")\n",
    "print(f\"   ÎßàÏßÄÎßâ 20% ÌèâÍ∑†: {np.mean(last_portion):.4f}\")\n",
    "print(f\"   Í∞êÏÜåÎüâ: {np.mean(first_portion) - np.mean(last_portion):.4f}\")\n",
    "\n",
    "if np.mean(last_portion) < np.mean(first_portion):\n",
    "    print(\"\\n‚úÖ ÏÜêÏã§Ïù¥ Í∞êÏÜåÌñàÏäµÎãàÎã§! Î™®Îç∏Ïù¥ ÌïôÏäµÌïòÍ≥† ÏûàÏäµÎãàÎã§.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è ÏÜêÏã§Ïù¥ Í∞êÏÜåÌïòÏßÄ ÏïäÏïòÏäµÎãàÎã§. ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Ï°∞Ï†ïÏù¥ ÌïÑÏöîÌï† Ïàò ÏûàÏäµÎãàÎã§.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. ÌèâÍ∞Ä Í≥ºÏ†ï Ïù¥Ìï¥\n",
    "\n",
    "## üéØ Î™©Ìëú\n",
    "> ÌïôÏäµÎêú Decision TransformerÎ°ú **Ï∂îÎ°†(Inference)**ÌïòÎäî Î∞©Î≤ïÏùÑ Ïù¥Ìï¥Ìï©ÎãàÎã§.\n",
    "\n",
    "## üîÑ Ï∂îÎ°† Í≥ºÏ†ï Í∞úÏöî\n",
    "\n",
    "Decision TransformerÏùò Ï∂îÎ°†ÏùÄ **ÏûêÍ∏∞ ÌöåÍ∑Ä(Autoregressive)** Î∞©ÏãùÏûÖÎãàÎã§:\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Inference[\"üîÑ Ï∂îÎ°† Î£®ÌîÑ\"]\n",
    "        direction TB\n",
    "        subgraph Init[\"Step 1: Ï¥àÍ∏∞ ÏÑ§Ï†ï\"]\n",
    "            I1[\"Î™©Ìëú RTG ÏÑ§Ï†ï (Ïòà: 1800)\"]\n",
    "            I2[\"Ï¥àÍ∏∞ state Í¥ÄÏ∏° (env.reset)\"]\n",
    "            I3[\"ÌûàÏä§ÌÜ†Î¶¨ Ï¥àÍ∏∞Ìôî\"]\n",
    "            I1 --> I2 --> I3\n",
    "        end\n",
    "\n",
    "        subgraph Predict[\"Step 2: Action ÏòàÏ∏°\"]\n",
    "            P1[\"model(states, actions, rtg, timesteps)\"]\n",
    "            P2[\"ÎßàÏßÄÎßâ timestepÏùò\\naction_pred ÏÇ¨Ïö©\"]\n",
    "            P1 --> P2\n",
    "        end\n",
    "\n",
    "        subgraph Execute[\"Step 3: ÌôòÍ≤ΩÏóêÏÑú Ïã§Ìñâ\"]\n",
    "            E1[\"next_state, reward = env.step(action)\"]\n",
    "        end\n",
    "\n",
    "        subgraph Update[\"Step 4: RTG ÏóÖÎç∞Ïù¥Ìä∏\"]\n",
    "            U1[\"new_rtg = current_rtg - reward\"]\n",
    "            U2[\"Î∞õÏùÄ rewardÎßåÌÅº\\nÎÇ®ÏùÄ Î™©ÌëúÏóêÏÑú Ï∞®Í∞ê\"]\n",
    "            U1 --> U2\n",
    "        end\n",
    "\n",
    "        subgraph History[\"Step 5: ÌûàÏä§ÌÜ†Î¶¨ ÏóÖÎç∞Ïù¥Ìä∏\"]\n",
    "            H1[\"states.append(next_state)\"]\n",
    "            H2[\"actions.append(action)\"]\n",
    "            H3[\"rtg.append(new_rtg)\"]\n",
    "        end\n",
    "\n",
    "        Init --> Predict --> Execute --> Update --> History\n",
    "        History -.->|\"ÏóêÌîºÏÜåÎìú Ï¢ÖÎ£åÍπåÏßÄ Î∞òÎ≥µ\"| Predict\n",
    "    end\n",
    "\n",
    "    style Init fill:#e3f2fd\n",
    "    style Predict fill:#fff3e0\n",
    "    style Execute fill:#e8f5e9\n",
    "    style Update fill:#fce4ec\n",
    "    style History fill:#f3e5f5\n",
    "```\n",
    "\n",
    "## üí° ÌïµÏã¨: RTG ÏóÖÎç∞Ïù¥Ìä∏ Î©îÏª§ÎãàÏ¶ò\n",
    "\n",
    "RTGÎäî \"ÏïÑÏßÅ ÏñªÏñ¥Ïïº Ìï† ÎÇ®ÏùÄ Î≥¥ÏÉÅ\"ÏùÑ ÎÇòÌÉÄÎÉÖÎãàÎã§.  \n",
    "Îß§ Ïä§ÌÖù rewardÎ•º Î∞õÏúºÎ©¥ Í∑∏ÎßåÌÅº RTGÏóêÏÑú Ï∞®Í∞êÎê©ÎãàÎã§:\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph RTG_Update[\"üìâ RTG ÏóÖÎç∞Ïù¥Ìä∏ Í≥ºÏ†ï\"]\n",
    "        direction TB\n",
    "        R0[\"üéØ ÏãúÏûë: RTG = 1800\\nÎ™©Ìëú 1800Ï†ê!\"] \n",
    "        R0 -->|\"reward = 3.5\"| R1[\"RTG = 1796.5\\nÏïÑÏßÅ 1796.5Ï†ê Îçî ÌïÑÏöî\"]\n",
    "        R1 -->|\"reward = 4.2\"| R2[\"RTG = 1792.3\\nÏïÑÏßÅ 1792.3Ï†ê Îçî ÌïÑÏöî\"]\n",
    "        R2 -->|\"...Í≥ÑÏÜç...\"| R3[\"RTG ‚âà 0\\nüéâ Î™©Ìëú Îã¨ÏÑ±!\"]\n",
    "    end\n",
    "\n",
    "    style R0 fill:#ffcdd2\n",
    "    style R1 fill:#fff9c4\n",
    "    style R2 fill:#dcedc8\n",
    "    style R3 fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px\n",
    "```\n",
    "\n",
    "> üí° **RTGÍ∞Ä \"ÎÇ®ÏùÄ Î™©Ìëú\"Î•º ÎÇòÌÉÄÎÇ¥ÎØÄÎ°ú, rewardÎ•º Î∞õÏùÑ ÎïåÎßàÎã§ Ï∞®Í∞êÎê©ÎãàÎã§!**  \n",
    "> ÏóêÌîºÏÜåÎìú ÎÅùÏóêÏÑú RTG ‚âà 0Ïù¥Î©¥ Î™©ÌëúÎ•º Îã¨ÏÑ±Ìïú Í≤ÉÏûÖÎãàÎã§.\n",
    "\n",
    "## üéÆ Î™©Ìëú ReturnÏóê Îî∞Î•∏ ÌñâÎèô Î≥ÄÌôî\n",
    "\n",
    "Decision TransformerÏùò Í∞ïÎ†•Ìïú ÌäπÏÑ±ÏùÄ **Î™©Ìëú ReturnÏùÑ Ï°∞Ï†àÌïòÏó¨ Ï†ïÏ±ÖÏùò ÌíàÏßàÏùÑ Ï†úÏñ¥**Ìï† Ïàò ÏûàÎã§Îäî Í≤ÉÏûÖÎãàÎã§:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph TargetReturn[\"üéØ Î™©Ìëú ReturnÏóê Îî∞Î•∏ ÌñâÎèô\"]\n",
    "        direction TB\n",
    "        High[\"ÎÜíÏùÄ Î™©Ìëú (RTG=3600)\\n‚Üí Í≥µÍ≤©Ï†ÅÏù¥Í≥† ÏµúÏ†ÅÏùò ÌñâÎèô\\n‚Üí Ï†ÑÎ¨∏Í∞Ä ÏàòÏ§Ä Ï†ïÏ±Ö\"]\n",
    "        Med[\"Ï§ëÍ∞Ñ Î™©Ìëú (RTG=1800)\\n‚Üí ÏïàÏ†ïÏ†ÅÏù∏ Ï§ëÍ∞Ñ ÏàòÏ§Ä ÌñâÎèô\\n‚Üí medium Ï†ïÏ±Ö\"]\n",
    "        Low[\"ÎÇÆÏùÄ Î™©Ìëú (RTG=600)\\n‚Üí Î≥¥ÏàòÏ†ÅÏù∏ ÌñâÎèô\\n‚Üí Ï¥àÎ≥¥Ïûê ÏàòÏ§Ä Ï†ïÏ±Ö\"]\n",
    "    end\n",
    "\n",
    "    style High fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px\n",
    "    style Med fill:#fff9c4\n",
    "    style Low fill:#ffcdd2\n",
    "```\n",
    "\n",
    "> üìå **Ïù¥Í≤ÉÏù¥ DTÏùò ÌïµÏã¨ Ïû•Ï†êÏûÖÎãàÎã§!**  \n",
    "> ÌïòÎÇòÏùò Î™®Îç∏Î°ú Îã§ÏñëÌïú ÏàòÏ§ÄÏùò Ï†ïÏ±ÖÏùÑ Íµ¨ÌòÑÌï† Ïàò ÏûàÏäµÎãàÎã§.  \n",
    "> Í∏∞Ï°¥ RLÏóêÏÑúÎäî Î≥¥ÏÉÅ Ìï®ÏàòÎ•º Î∞îÍøîÏïº ÌñàÏßÄÎßå, DTÎäî RTGÎßå Ï°∞Ï†àÌïòÎ©¥ Îê©ÎãàÎã§.\n",
    "\n",
    "## üîç Context WindowÏôÄ Ìå®Îî©\n",
    "\n",
    "Ï∂îÎ°† Ïãú ÌûàÏä§ÌÜ†Î¶¨ Í¥ÄÎ¶¨ Î∞©Î≤ï:\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Context[\"üìè Context Window Í¥ÄÎ¶¨\"]\n",
    "        direction TB\n",
    "        subgraph Short[\"ÌûàÏä§ÌÜ†Î¶¨ < K\"]\n",
    "            direction LR\n",
    "            SP[\"Ìå®Îî© 0 ... 0\"] --> SD[\"s‚ÇÄ s‚ÇÅ s‚ÇÇ\"]\n",
    "        end\n",
    "        subgraph Exact[\"ÌûàÏä§ÌÜ†Î¶¨ = K\"]\n",
    "            direction LR\n",
    "            ED[\"s‚ÇÄ s‚ÇÅ ... s_{K-1}\"]\n",
    "        end\n",
    "        subgraph Long[\"ÌûàÏä§ÌÜ†Î¶¨ > K\"]\n",
    "            direction LR\n",
    "            LD1[\"s‚ÇÄ s‚ÇÅ ... (Î≤ÑÎ¶º)\"]\n",
    "            LD2[\"s_{t-K+1} ... s_t (ÏµúÍ∑º KÍ∞úÎßå ÏÇ¨Ïö©)\"]\n",
    "            LD1 -.->|\"ÏûòÎùºÎÉÑ\"| LD2\n",
    "        end\n",
    "    end\n",
    "\n",
    "    style Short fill:#e3f2fd\n",
    "    style Exact fill:#e8f5e9\n",
    "    style Long fill:#fff3e0\n",
    "    style SP fill:#eeeeee\n",
    "    style LD1 fill:#ffcdd2,stroke-dasharray: 5 5\n",
    "    style LD2 fill:#c8e6c9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üéØ get_action Ìï®Ïàò Íµ¨ÌòÑ\n",
    "# ============================================================\n",
    "# evaluate_episodes.pyÏùò ÌïµÏã¨ Ìï®ÏàòÏûÖÎãàÎã§.\n",
    "# ÌòÑÏû¨ÍπåÏßÄÏùò ÌûàÏä§ÌÜ†Î¶¨Î•º Î∞îÌÉïÏúºÎ°ú Îã§Ïùå actionÏùÑ ÏòàÏ∏°Ìï©ÎãàÎã§.\n",
    "#\n",
    "# Ï§ëÏöî Ìè¨Ïù∏Ìä∏:\n",
    "#   1. ÌûàÏä§ÌÜ†Î¶¨Í∞Ä max_lengthÎ≥¥Îã§ Í∏∏Î©¥ ÏµúÍ∑º KÍ∞úÎßå ÏÇ¨Ïö©\n",
    "#   2. ÌûàÏä§ÌÜ†Î¶¨Í∞Ä max_lengthÎ≥¥Îã§ ÏßßÏúºÎ©¥ ÏïûÏóê Ìå®Îî©\n",
    "#   3. ÎßàÏßÄÎßâ timestepÏùò ÏòàÏ∏°Îßå Î∞òÌôò\n",
    "# ============================================================\n",
    "\n",
    "def get_action(model, states, actions, returns_to_go, timesteps, device, max_length=20):\n",
    "    \"\"\"\n",
    "    ÌòÑÏû¨ÍπåÏßÄÏùò ÌûàÏä§ÌÜ†Î¶¨Î•º Î∞îÌÉïÏúºÎ°ú Îã§Ïùå action ÏòàÏ∏°\n",
    "    \n",
    "    Ïã§Ï†ú ÏΩîÎìú(decision_transformer.pyÏùò get_action)ÏôÄ ÎèôÏùºÌïú Î∞©ÏãùÏûÖÎãàÎã§.\n",
    "    ÌïµÏã¨: states, actions, rtg, timesteps Í∞ÅÍ∞ÅÏùò Í∏∏Ïù¥Í∞Ä Îã§Î•º Ïàò ÏûàÏúºÎØÄÎ°ú\n",
    "    Í∞ÅÏûê ÎèÖÎ¶ΩÏ†ÅÏúºÎ°ú Ìå®Îî©Ìï©ÎãàÎã§.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    states : (1, t, state_dim) - ÌòÑÏû¨ÍπåÏßÄÏùò ÏÉÅÌÉú ÌûàÏä§ÌÜ†Î¶¨\n",
    "    actions : (1, t-1, act_dim) - Ïù¥Ï†ÑÍπåÏßÄÏùò ÌñâÎèô (ÌòÑÏû¨ actionÏùÄ ÏïÑÏßÅ Î™®Î¶Ñ)\n",
    "    returns_to_go : (1, t, 1) - RTG ÌûàÏä§ÌÜ†Î¶¨\n",
    "    timesteps : (1, t) - ÌÉÄÏûÑÏä§ÌÖù ÌûàÏä§ÌÜ†Î¶¨\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    state_dim = states.shape[-1]\n",
    "    act_dim = ACT_DIM\n",
    "    \n",
    "    # Í∞Å ÌÖêÏÑúÎ•º max_lengthÎ°ú ÏûêÎ•¥Í∏∞\n",
    "    states = states[:, -max_length:]\n",
    "    actions = actions[:, -max_length:]\n",
    "    returns_to_go = returns_to_go[:, -max_length:]\n",
    "    timesteps = timesteps[:, -max_length:]\n",
    "    \n",
    "    # Í∞Å ÌÖêÏÑúÎ•º Í∞úÎ≥ÑÏ†ÅÏúºÎ°ú max_lengthÍπåÏßÄ Ìå®Îî©\n",
    "    # (statesÍ∞Ä 5, actionsÍ∞Ä 4Ïùº Îïå, Í∞ÅÍ∞Å 15, 16ÎßåÌÅº Ìå®Îî©)\n",
    "    s_pad = max_length - states.shape[1]\n",
    "    a_pad = max_length - actions.shape[1]\n",
    "    r_pad = max_length - returns_to_go.shape[1]\n",
    "    t_pad = max_length - timesteps.shape[1]\n",
    "    \n",
    "    # attention maskÎäî states Í∏∞Ï§Ä\n",
    "    attention_mask = torch.cat([\n",
    "        torch.zeros(1, s_pad, device=device),\n",
    "        torch.ones(1, states.shape[1], device=device)\n",
    "    ], dim=1).to(dtype=torch.long)\n",
    "    \n",
    "    states = torch.cat([\n",
    "        torch.zeros(1, s_pad, state_dim, device=device), states\n",
    "    ], dim=1).to(dtype=torch.float32)\n",
    "    actions = torch.cat([\n",
    "        torch.zeros(1, a_pad, act_dim, device=device), actions\n",
    "    ], dim=1).to(dtype=torch.float32)\n",
    "    returns_to_go = torch.cat([\n",
    "        torch.zeros(1, r_pad, 1, device=device), returns_to_go\n",
    "    ], dim=1).to(dtype=torch.float32)\n",
    "    timesteps = torch.cat([\n",
    "        torch.zeros(1, t_pad, device=device, dtype=torch.long), timesteps\n",
    "    ], dim=1).to(dtype=torch.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if DT_AVAILABLE:\n",
    "            _, action_preds, _ = model(\n",
    "                states, actions, None, returns_to_go, timesteps, attention_mask\n",
    "            )\n",
    "        else:\n",
    "            _, action_preds, _ = model(\n",
    "                states, actions, returns_to_go, timesteps\n",
    "            )\n",
    "    \n",
    "    return action_preds[0, -1]\n",
    "\n",
    "# ============================================================\n",
    "# üß™ get_action ÌÖåÏä§Ìä∏\n",
    "# ============================================================\n",
    "print(\"üß™ get_action Ìï®Ïàò ÌÖåÏä§Ìä∏\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ÎçîÎØ∏ ÌûàÏä§ÌÜ†Î¶¨ ÏÉùÏÑ±\n",
    "test_states = torch.randn(1, 5, STATE_DIM).to(device)  # 5 Ïä§ÌÖùÏùò ÌûàÏä§ÌÜ†Î¶¨\n",
    "test_actions = torch.randn(1, 5, ACT_DIM).to(device)   # 5 Ïä§ÌÖùÏùò action\n",
    "test_rtg = torch.ones(1, 5, 1).to(device) * 1.8        # Î™©Ìëú RTG = 1800/1000\n",
    "test_timesteps = torch.arange(5).unsqueeze(0).to(device)\n",
    "\n",
    "print(f\"ÏûÖÎ†• ÌûàÏä§ÌÜ†Î¶¨ Í∏∏Ïù¥: 5 Ïä§ÌÖù\")\n",
    "print(f\"Î™©Ìëú RTG: 1800 (Ï†ïÍ∑úÌôî: 1.8)\")\n",
    "\n",
    "# Action ÏòàÏ∏°\n",
    "predicted_action = get_action(\n",
    "    model, test_states, test_actions, test_rtg, test_timesteps, device\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ ÏòàÏ∏°Îêú Action: {predicted_action.cpu().numpy()}\")\n",
    "print(f\"   ‚îî‚îÄ shape: {predicted_action.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üéÆ ÏóêÌîºÏÜåÎìú Î°§ÏïÑÏõÉ ÏãúÎÆ¨Î†àÏù¥ÏÖò\n",
    "# ============================================================\n",
    "# Ïã§Ï†ú ÌôòÍ≤Ω ÏóÜÏù¥ Ï∂îÎ°† Í≥ºÏ†ïÏùÑ ÏãúÎÆ¨Î†àÏù¥ÏÖòÌï©ÎãàÎã§.\n",
    "# (Ïã§Ï†ú ÌôòÍ≤ΩÏù¥ ÏûàÎã§Î©¥ env.step()ÏúºÎ°ú next_stateÏôÄ rewardÎ•º Î∞õÏäµÎãàÎã§)\n",
    "#\n",
    "# Ïù¥ ÏãúÎÆ¨Î†àÏù¥ÏÖòÏùÄ Ï∂îÎ°† Î£®ÌîÑÏùò Íµ¨Ï°∞Î•º Ïù¥Ìï¥ÌïòÍ∏∞ ÏúÑÌïú Í≤ÉÏûÖÎãàÎã§.\n",
    "# ============================================================\n",
    "\n",
    "def simulate_episode(model, target_return, max_ep_len=50, scale=1000, device='cpu'):\n",
    "    \"\"\"\n",
    "    ÌôòÍ≤Ω ÏóÜÏù¥ ÏóêÌîºÏÜåÎìúÎ•º ÏãúÎÆ¨Î†àÏù¥ÏÖò\n",
    "    (Ïã§Ï†ú ÌôòÍ≤ΩÏù¥ ÏûàÎã§Î©¥ envÏóêÏÑú stateÏôÄ rewardÎ•º Î∞õÏùå)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        ÌïôÏäµÎêú Decision Transformer\n",
    "    target_return : float\n",
    "        Î™©Ìëú return (Ïòà: 1800)\n",
    "    max_ep_len : int\n",
    "        ÏµúÎåÄ ÏóêÌîºÏÜåÎìú Í∏∏Ïù¥\n",
    "    scale : float\n",
    "        RTG Ï†ïÍ∑úÌôî Ïä§ÏºÄÏùº\n",
    "    device : torch.device\n",
    "        Ïó∞ÏÇ∞ Ïû•Ïπò\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (rtg_history, action_history, reward_history)\n",
    "    \"\"\"\n",
    "    state_dim, act_dim = STATE_DIM, ACT_DIM\n",
    "    \n",
    "    # ==========================================\n",
    "    # Step 1: Ï¥àÍ∏∞Ìôî\n",
    "    # ==========================================\n",
    "    # Ï¥àÍ∏∞ state (Ïã§Ï†úÎ°úÎäî env.reset()ÏóêÏÑú Î∞õÏùå)\n",
    "    states = torch.randn(1, 1, state_dim).to(device)\n",
    "    \n",
    "    # Îπà action ÌûàÏä§ÌÜ†Î¶¨ (ÏïÑÏßÅ ÌñâÎèô ÏóÜÏùå)\n",
    "    actions = torch.zeros(1, 0, act_dim).to(device)\n",
    "    \n",
    "    # Ï¥àÍ∏∞ RTG = Î™©Ìëú return\n",
    "    rtg = torch.tensor([[[target_return / scale]]]).to(device)\n",
    "    \n",
    "    # Ï¥àÍ∏∞ timestep\n",
    "    timesteps = torch.tensor([[0]]).to(device)\n",
    "    \n",
    "    # Í∏∞Î°ùÏö©\n",
    "    rtg_history = [target_return]\n",
    "    action_history = []\n",
    "    reward_history = []\n",
    "    \n",
    "    print(f\"üéÆ ÏóêÌîºÏÜåÎìú ÏãúÎÆ¨Î†àÏù¥ÏÖò ÏãúÏûë\")\n",
    "    print(f\"   Î™©Ìëú Return: {target_return}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ==========================================\n",
    "    # Step 2-5: Ï∂îÎ°† Î£®ÌîÑ\n",
    "    # ==========================================\n",
    "    num_steps_to_show = min(10, max_ep_len)  # Ï∂úÎ†•Ìï† Ïä§ÌÖù Ïàò\n",
    "    \n",
    "    for t in range(num_steps_to_show):\n",
    "        # ----- Step 2: Action ÏòàÏ∏° -----\n",
    "        action = get_action(model, states, actions, rtg, timesteps, device)\n",
    "        action_history.append(action.cpu().numpy())\n",
    "        \n",
    "        # ----- Step 3: ÌôòÍ≤ΩÏóêÏÑú Ïã§Ìñâ (ÏãúÎÆ¨Î†àÏù¥ÏÖò) -----\n",
    "        # Ïã§Ï†ú: next_state, reward, done, _ = env.step(action)\n",
    "        fake_reward = np.random.uniform(0, 5)  # Í∞ÄÏÉÅÏùò reward\n",
    "        reward_history.append(fake_reward)\n",
    "        \n",
    "        # ----- Step 4: RTG ÏóÖÎç∞Ïù¥Ìä∏ -----\n",
    "        new_rtg = rtg_history[-1] - fake_reward\n",
    "        rtg_history.append(new_rtg)\n",
    "        \n",
    "        # ÏßÑÌñâ ÏÉÅÌô© Ï∂úÎ†•\n",
    "        action_str = np.array2string(action.cpu().numpy(), precision=2, suppress_small=True)\n",
    "        print(f\"Step {t:>2}: RTG={rtg_history[-2]:>7.1f} ‚Üí Action={action_str}\")\n",
    "        print(f\"         reward={fake_reward:>5.2f} ‚Üí new RTG={new_rtg:>7.1f}\")\n",
    "        \n",
    "        # ----- Step 5: ÌûàÏä§ÌÜ†Î¶¨ ÏóÖÎç∞Ïù¥Ìä∏ -----\n",
    "        # ÏÉàÎ°úÏö¥ state (Ïã§Ï†úÎ°úÎäî envÏóêÏÑú Î∞õÏùå)\n",
    "        new_state = torch.randn(1, 1, state_dim).to(device)\n",
    "        \n",
    "        # ÌûàÏä§ÌÜ†Î¶¨Ïóê Ï∂îÍ∞Ä\n",
    "        states = torch.cat([states, new_state], dim=1)\n",
    "        actions = torch.cat([actions, action.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "        rtg = torch.cat([rtg, torch.tensor([[[new_rtg / scale]]]).to(device)], dim=1)\n",
    "        timesteps = torch.cat([timesteps, torch.tensor([[t + 1]]).to(device)], dim=1)\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    return rtg_history, action_history, reward_history\n",
    "\n",
    "# ============================================================\n",
    "# üéÆ ÏãúÎÆ¨Î†àÏù¥ÏÖò Ïã§Ìñâ\n",
    "# ============================================================\n",
    "rtg_hist, act_hist, reward_hist = simulate_episode(\n",
    "    model, \n",
    "    target_return=1800, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"üìä ÏãúÎÆ¨Î†àÏù¥ÏÖò Í≤∞Í≥º:\")\n",
    "print(f\"   Ï¥ù reward ÌöçÎìù: {sum(reward_hist):.2f}\")\n",
    "print(f\"   ÌèâÍ∑† reward: {np.mean(reward_hist):.2f}\")\n",
    "print(f\"   ÏµúÏ¢Ö RTG: {rtg_hist[-1]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìà RTG Î≥ÄÌôî ÏãúÍ∞ÅÌôî\n",
    "# ============================================================\n",
    "# Ï∂îÎ°† Í≥ºÏ†ïÏóêÏÑú RTGÍ∞Ä Ïñ¥ÎñªÍ≤å Î≥ÄÌôîÌïòÎäîÏßÄ ÏãúÍ∞ÅÌôîÌï©ÎãàÎã§.\n",
    "#\n",
    "# Í¥ÄÏ∞∞ Ìè¨Ïù∏Ìä∏:\n",
    "#   - RTGÎäî rewardÎ•º Î∞õÏùÑ ÎïåÎßàÎã§ Í∞êÏÜå\n",
    "#   - Ïù¥ÏÉÅÏ†ÅÏúºÎ°úÎäî ÏóêÌîºÏÜåÎìú ÎÅùÏóêÏÑú RTG ‚âà 0\n",
    "#   - RTGÍ∞Ä ÏùåÏàòÍ∞Ä ÎêòÎ©¥ Î™©Ìëú Ï¥àÍ≥º Îã¨ÏÑ±!\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìà RTG Î≥ÄÌôî ÏãúÍ∞ÅÌôî\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. RTG Î≥ÄÌôî\n",
    "ax1 = axes[0]\n",
    "ax1.plot(rtg_hist, 'o-', linewidth=2, markersize=8, color='orange')\n",
    "ax1.axhline(y=0, color='green', linestyle='--', linewidth=2, label='Goal (RTG=0)')\n",
    "ax1.fill_between(range(len(rtg_hist)), rtg_hist, 0, alpha=0.3, color='orange')\n",
    "\n",
    "ax1.set_xlabel('Step', fontsize=12)\n",
    "ax1.set_ylabel('Return-to-Go', fontsize=12)\n",
    "ax1.set_title('RTG Evolution During Inference', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# RTG Í∞í ÌëúÏãú\n",
    "for i, rtg in enumerate(rtg_hist):\n",
    "    if i % 2 == 0:  # 2Í∞ú Í±¥ÎÑà ÌëúÏãú\n",
    "        ax1.annotate(f'{rtg:.0f}', (i, rtg), textcoords=\"offset points\", \n",
    "                     xytext=(0, 10), ha='center', fontsize=8)\n",
    "\n",
    "# 2. Reward Î≥ÄÌôî\n",
    "ax2 = axes[1]\n",
    "ax2.bar(range(len(reward_hist)), reward_hist, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax2.axhline(y=np.mean(reward_hist), color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Mean: {np.mean(reward_hist):.2f}')\n",
    "ax2.set_xlabel('Step', fontsize=12)\n",
    "ax2.set_ylabel('Reward', fontsize=12)\n",
    "ax2.set_title('Instantaneous Rewards', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ÌïµÏã¨ Ïù∏ÏÇ¨Ïù¥Ìä∏\n",
    "print(f\"\\nüìå ÌïµÏã¨ Ïù∏ÏÇ¨Ïù¥Ìä∏:\")\n",
    "print(f\"   ‚Ä¢ ÏãúÏûë RTG: {rtg_hist[0]:.0f} (Î™©Ìëú)\")\n",
    "print(f\"   ‚Ä¢ ÏµúÏ¢Ö RTG: {rtg_hist[-1]:.0f}\")\n",
    "print(f\"   ‚Ä¢ RTG Í∞êÏÜåÎüâ: {rtg_hist[0] - rtg_hist[-1]:.0f} (= ÌöçÎìùÌïú Ï¥ù reward)\")\n",
    "print()\n",
    "if rtg_hist[-1] <= 0:\n",
    "    print(\"   üéâ Î™©Ìëú Îã¨ÏÑ±! (RTG ‚â§ 0)\")\n",
    "else:\n",
    "    print(f\"   üìç Î™©ÌëúÍπåÏßÄ ÎÇ®ÏùÄ reward: {rtg_hist[-1]:.0f}\")\n",
    "print()\n",
    "print(\"üí° Ïã§Ï†ú ÌôòÍ≤ΩÏóêÏÑúÎäî ÏóêÌîºÏÜåÎìú ÎÅùÏóêÏÑú RTGÍ∞Ä 0Ïóê Í∞ÄÍπåÏõåÏïº\")\n",
    "print(\"   Î™©Ìëú returnÏùÑ Îã¨ÏÑ±Ìïú Í≤ÉÏûÖÎãàÎã§.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéì Phase 3 ÏôÑÎ£å!\n",
    "\n",
    "## üìö Ïù¥ PhaseÏóêÏÑú Î∞∞Ïö¥ ÎÇ¥Ïö©\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Summary[\"üéì Phase 3 ÌïôÏäµ ÏöîÏïΩ\"]\n",
    "        direction TB\n",
    "        subgraph S1[\"1. Îç∞Ïù¥ÌÑ∞ÏÖã Íµ¨Ï°∞\"]\n",
    "            D1[\"trajectories = list of episodes\"]\n",
    "            D2[\"Í∞Å traj = observations + actions + rewards\"]\n",
    "        end\n",
    "\n",
    "        subgraph S2[\"2. RTG Í≥ÑÏÇ∞\"]\n",
    "            R1[\"RTG_t = r_t + RTG_{t+1}\"]\n",
    "            R2[\"Ïó≠Ïàú Í≥ÑÏÇ∞ discount_cumsum\"]\n",
    "        end\n",
    "\n",
    "        subgraph S3[\"3. Î™®Îç∏ ÏïÑÌÇ§ÌÖçÏ≤ò\"]\n",
    "            M1[\"ÏûÖÎ†•: (RTG, State, Action) ÏãúÌÄÄÏä§\"]\n",
    "            M2[\"Embedding ‚Üí GPT-2 ‚Üí predict_action\"]\n",
    "        end\n",
    "\n",
    "        subgraph S4[\"4. ÌïôÏäµ Í≥ºÏ†ï\"]\n",
    "            T1[\"Loss = MSE(predicted, target action)\"]\n",
    "            T2[\"= Supervised Learning!\"]\n",
    "        end\n",
    "\n",
    "        subgraph S5[\"5. Ï∂îÎ°† Í≥ºÏ†ï\"]\n",
    "            I1[\"Î™©Ìëú RTG ÏÑ§Ï†ï ‚Üí Action ÏòàÏ∏°\"]\n",
    "            I2[\"‚Üí ÌôòÍ≤Ω Ïã§Ìñâ ‚Üí RTG ÏóÖÎç∞Ïù¥Ìä∏ ‚Üí Î∞òÎ≥µ\"]\n",
    "        end\n",
    "\n",
    "        S1 --> S2 --> S3 --> S4 --> S5\n",
    "    end\n",
    "\n",
    "    style S1 fill:#e3f2fd\n",
    "    style S2 fill:#fff3e0\n",
    "    style S3 fill:#e8f5e9\n",
    "    style S4 fill:#fce4ec\n",
    "    style S5 fill:#f3e5f5\n",
    "```\n",
    "\n",
    "## üîë ÌïµÏã¨ Í∞úÎÖê ÌïúÎààÏóê Î≥¥Í∏∞\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Training[\"üìö ÌïôÏäµ (Offline)\"]\n",
    "        direction TB\n",
    "        TR1[\"Í≥ºÍ±∞ Îç∞Ïù¥ÌÑ∞ÏÖã\"] --> TR2[\"RTG Í≥ÑÏÇ∞\"]\n",
    "        TR2 --> TR3[\"(R, s, a) ÏãúÌÄÄÏä§ Íµ¨ÏÑ±\"]\n",
    "        TR3 --> TR4[\"Transformer Forward\"]\n",
    "        TR4 --> TR5[\"MSE Loss ‚Üí Backprop\"]\n",
    "    end\n",
    "\n",
    "    subgraph Inference[\"üéØ Ï∂îÎ°† (Online)\"]\n",
    "        direction TB\n",
    "        IN1[\"Î™©Ìëú Return ÏÑ§Ï†ï\"] --> IN2[\"State Í¥ÄÏ∏°\"]\n",
    "        IN2 --> IN3[\"Action ÏòàÏ∏°\"]\n",
    "        IN3 --> IN4[\"ÌôòÍ≤ΩÏóêÏÑú Ïã§Ìñâ\"]\n",
    "        IN4 --> IN5[\"RTG -= reward\"]\n",
    "        IN5 -.-> IN2\n",
    "    end\n",
    "\n",
    "    Training -->|\"ÌïôÏäµÎêú Î™®Îç∏\"| Inference\n",
    "\n",
    "    style Training fill:#e3f2fd\n",
    "    style Inference fill:#fff8e1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Phase 3 Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏\n",
    "\n",
    "| ÏôÑÎ£å | Ìï≠Î™© |\n",
    "|:---:|:---|\n",
    "| ‚òê | ÌôòÍ≤Ω ÏÑ§Ï†ï Î∞è Í≤ΩÎ°ú Íµ¨ÏÑ± ÏôÑÎ£å |\n",
    "| ‚òê | D4RL Îç∞Ïù¥ÌÑ∞ÏÖã Íµ¨Ï°∞ Ïù¥Ìï¥ |\n",
    "| ‚òê | RTG Í≥ÑÏÇ∞ (`discount_cumsum`) Íµ¨ÌòÑ |\n",
    "| ‚òê | DecisionTransformer Î™®Îç∏ Íµ¨Ï°∞ Ïù¥Ìï¥ |\n",
    "| ‚òê | Forward pass ÎèôÏûë ÌôïÏù∏ |\n",
    "| ‚òê | Î∞∞Ïπò ÏÉùÏÑ± (`get_batch`) Ïù¥Ìï¥ |\n",
    "| ‚òê | ÌïôÏäµ Î£®ÌîÑ Íµ¨ÌòÑ Î∞è Ïã§Ìñâ |\n",
    "| ‚òê | Ï∂îÎ°† Ïãú RTG ÏóÖÎç∞Ïù¥Ìä∏ Î©îÏª§ÎãàÏ¶ò Ïù¥Ìï¥ |\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Îã§Ïùå Îã®Í≥Ñ\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    P3[\"‚úÖ Phase 3\\nGym ÌôòÍ≤Ω Ïã§Ïäµ\\nÏôÑÎ£å!\"] --> P4[\"üéØ Phase 4\\nAtari ÌôòÍ≤Ω Ïã§Ïäµ\"]\n",
    "\n",
    "    subgraph Next[\"Phase 4ÏóêÏÑú Î∞∞Ïö∏ ÎÇ¥Ïö©\"]\n",
    "        direction TB\n",
    "        N1[\"üñºÔ∏è Ïù¥ÎØ∏ÏßÄ Í∏∞Î∞ò ÌôòÍ≤Ω (84√ó84 ÌîΩÏÖÄ)\"]\n",
    "        N2[\"üéÆ DQN-replay Îç∞Ïù¥ÌÑ∞ÏÖã\"]\n",
    "        N3[\"üëæ Atari Í≤åÏûÑ (Breakout, Pong Îì±)\"]\n",
    "        N4[\"üèóÔ∏è Îçî ÌÅ∞ Î™®Îç∏ + CNN Ïù∏ÏΩîÎçî\"]\n",
    "    end\n",
    "\n",
    "    P4 --> Next\n",
    "\n",
    "    style P3 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px\n",
    "    style P4 fill:#ffeb3b,stroke:#f57f17,stroke-width:2px\n",
    "    style Next fill:#fff3e0\n",
    "```\n",
    "\n",
    "**Phase 4: Atari ÌôòÍ≤Ω Ïã§Ïäµ** ‚Üí [phase4_atari_practice.ipynb](./phase4_atari_practice.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Ï∂îÍ∞Ä ÌïôÏäµ ÏûêÎ£å\n",
    "\n",
    "1. **ÏΩîÎìú Î∂ÑÏÑù**: `gym/decision_transformer/models/decision_transformer.py` ÏùΩÏñ¥Î≥¥Í∏∞\n",
    "2. **Ïã§Ï†ú Ïã§Ìñâ**: D4RL Îç∞Ïù¥ÌÑ∞ÏÖã Îã§Ïö¥Î°úÎìú ÌõÑ `experiment.py` Ïã§Ìñâ\n",
    "3. **ÎÖºÎ¨∏**: [Decision Transformer](https://arxiv.org/abs/2106.01345) ÏõêÎ¨∏ ÏùΩÍ∏∞\n",
    "\n",
    "---\n",
    "\n",
    "> üéâ **ÏàòÍ≥†ÌïòÏÖ®ÏäµÎãàÎã§!** Phase 3ÏóêÏÑú Decision TransformerÏùò Ïã§Ï†ú Íµ¨ÌòÑÏùÑ ÏÇ¥Ìé¥Î≥¥ÏïòÏäµÎãàÎã§.  \n",
    "> Ïù¥Ï†ú Atari ÌôòÍ≤ΩÏóêÏÑúÏùò ÏùëÏö©ÏùÑ Î∞∞Ïö∏ Ï§ÄÎπÑÍ∞Ä ÎêòÏóàÏäµÎãàÎã§!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}