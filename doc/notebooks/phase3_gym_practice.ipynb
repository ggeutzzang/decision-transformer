{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ® Phase 3: Gym í™˜ê²½ ì‹¤ìŠµ\n",
    "\n",
    "> **ëª©í‘œ**: ì‹¤ì œ Decision Transformer ì½”ë“œë¥¼ ë¶„ì„í•˜ê³ , Gym í™˜ê²½ì—ì„œ í•™ìŠµ ë° ì¶”ë¡  ê³¼ì •ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ºï¸ ì´ Phaseì—ì„œ ë°°ìš¸ ë‚´ìš©\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Phase3[\"ğŸ® Phase 3: Gym í™˜ê²½ ì‹¤ìŠµ\"]\n",
    "        direction LR\n",
    "        subgraph S1[\"ğŸ“‚ ë°ì´í„°ì…‹ íƒìƒ‰\"]\n",
    "            D1[\"ê¶¤ì  ë°ì´í„° êµ¬ì¡°\"]\n",
    "            D2[\"State / Action / Reward\"]\n",
    "            D3[\"RTG ê³„ì‚°\"]\n",
    "        end\n",
    "\n",
    "        subgraph S2[\"ğŸ” ëª¨ë¸ ì½”ë“œ ë¶„ì„\"]\n",
    "            M1[\"ëª¨ë¸ ì•„í‚¤í…ì²˜\"]\n",
    "            M2[\"ì„ë² ë”© ë ˆì´ì–´\"]\n",
    "            M3[\"Forward Pass\"]\n",
    "        end\n",
    "\n",
    "        subgraph S3[\"ğŸš€ í•™ìŠµ & ì¶”ë¡ \"]\n",
    "            T1[\"ë°°ì¹˜ ìƒì„±\"]\n",
    "            T2[\"ì†ì‹¤ í•¨ìˆ˜ MSE\"]\n",
    "            T3[\"RTG ì¡°ê±´ë¶€ ì¶”ë¡ \"]\n",
    "        end\n",
    "\n",
    "        S1 --> S2 --> S3\n",
    "    end\n",
    "\n",
    "    style S1 fill:#e3f2fd\n",
    "    style S2 fill:#fff3e0\n",
    "    style S3 fill:#e8f5e9\n",
    "```\n",
    "\n",
    "## ğŸ“‹ ëª©ì°¨\n",
    "\n",
    "| ì„¹ì…˜ | ì£¼ì œ | í•™ìŠµ ë‚´ìš© |\n",
    "|:---:|:---|:---|\n",
    "| **1** | [í™˜ê²½ ì„¤ì •](#1-í™˜ê²½-ì„¤ì •) | Python íŒ¨í‚¤ì§€ í™•ì¸ ë° ê²½ë¡œ ì„¤ì • |\n",
    "| **2** | [ë°ì´í„°ì…‹ íƒìƒ‰](#2-ë°ì´í„°ì…‹-íƒìƒ‰) | D4RL ë°ì´í„°ì…‹ êµ¬ì¡°ì™€ í†µê³„ ë¶„ì„ |\n",
    "| **3** | [ëª¨ë¸ ì½”ë“œ ë¶„ì„](#3-ëª¨ë¸-ì½”ë“œ-ë¶„ì„) | DecisionTransformer ë‚´ë¶€ êµ¬ì¡° ì´í•´ |\n",
    "| **4** | [í•™ìŠµ ì‹¤í–‰](#4-í•™ìŠµ-ì‹¤í–‰) | ë°°ì¹˜ ìƒì„±ê³¼ í•™ìŠµ ë£¨í”„ êµ¬í˜„ |\n",
    "| **5** | [í‰ê°€ ê³¼ì • ì´í•´](#5-í‰ê°€-ê³¼ì •-ì´í•´) | RTG ì¡°ê±´ë¶€ ì¶”ë¡  ë©”ì»¤ë‹ˆì¦˜ |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Phase ì—°ê²°\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    P1[\"Phase 1\\nğŸ“– ë°°ê²½ì§€ì‹\\nRL/Transformer\\nê¸°ì´ˆ ê°œë…\"] --> P2[\"Phase 2\\nğŸ§  í•µì‹¬ê°œë…\\nRTG/ì‹œí€€ìŠ¤ êµ¬ì„±\\nDTì˜ ì‘ë™ ì›ë¦¬\"]\n",
    "    P2 --> P3[\"Phase 3\\nğŸ® Gym ì‹¤ìŠµ\\nì‹¤ì œ ì½”ë“œ ì‹¤í–‰\\ní•™ìŠµ & ì¶”ë¡  ì‹¤ìŠµ\"]\n",
    "    P3 --> P4[\"Phase 4\\nğŸ‘¾ Atari\\nì´ë¯¸ì§€ í™˜ê²½\\nê³ ê¸‰ ì‘ìš©\"]\n",
    "\n",
    "    style P3 fill:#ffeb3b,stroke:#f57f17,stroke-width:3px\n",
    "    style P1 fill:#e3f2fd\n",
    "    style P2 fill:#e8f5e9\n",
    "    style P4 fill:#fce4ec\n",
    "```\n",
    "\n",
    "> ğŸ’¡ **Gym í™˜ê²½ì´ë€?**  \n",
    "> OpenAI Gymì€ ê°•í™”í•™ìŠµ ì—°êµ¬ë¥¼ ìœ„í•œ í‘œì¤€ í™˜ê²½ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.  \n",
    "> **Hopper**, **HalfCheetah**, **Walker2d** ë“± ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ë¡œë´‡ ì œì–´ íƒœìŠ¤í¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¦ ì‚¬ì „ ìš”êµ¬ì‚¬í•­\n",
    "\n",
    "```python\n",
    "# í•„ìš”í•œ íŒ¨í‚¤ì§€\n",
    "- Python 3.7+\n",
    "- PyTorch 1.8+\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "- transformers (Hugging Face)\n",
    "\n",
    "# ì„ íƒì  (ì‹¤ì œ í™˜ê²½ ì‹¤í–‰ ì‹œ)\n",
    "- MuJoCo\n",
    "- d4rl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "## ğŸ¯ ëª©í‘œ\n",
    "> Python í™˜ê²½ì„ í™•ì¸í•˜ê³ , Decision Transformer ì½”ë“œì— ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€ êµ¬ì¡°\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Project[\"ğŸ“ decision-transformer/\"]\n",
    "        direction TB\n",
    "        subgraph Gym[\"ğŸ“‚ gym/ â† ì´ë²ˆ Phaseì—ì„œ ì‚¬ìš©\"]\n",
    "            direction TB\n",
    "            subgraph DT_Mod[\"decision_transformer/\"]\n",
    "                direction TB\n",
    "                Models[\"models/\\nâ”œ decision_transformer.py â† í•µì‹¬ ëª¨ë¸\\nâ”” trajectory_gpt2.py â† GPT-2 ê¸°ë°˜\"]\n",
    "                Training[\"training/\\nâ”” seq_trainer.py â† í•™ìŠµ ì½”ë“œ\"]\n",
    "                Eval[\"evaluation/\\nâ”” evaluate_episodes.py â† í‰ê°€ ì½”ë“œ\"]\n",
    "            end\n",
    "            Data[\"data/\\nâ”” hopper-medium-v2.pkl â† D4RL ë°ì´í„°ì…‹\"]\n",
    "            Exp[\"experiment.py â† ë©”ì¸ ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸\"]\n",
    "        end\n",
    "        Notebooks[\"doc/notebooks/ â† í˜„ì¬ ìœ„ì¹˜\"]\n",
    "    end\n",
    "\n",
    "    style Gym fill:#e8f5e9,stroke:#4caf50,stroke-width:2px\n",
    "    style DT_Mod fill:#fff3e0\n",
    "    style Notebooks fill:#ffeb3b\n",
    "```\n",
    "\n",
    "## ğŸ”§ íŒ¨í‚¤ì§€ ì—­í• \n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Packages[\"í•µì‹¬ íŒ¨í‚¤ì§€\"]\n",
    "        direction TB\n",
    "        PT[\"ğŸ”¥ PyTorch\\nëª¨ë¸ ì •ì˜ & í•™ìŠµ\"] --> TF[\"ğŸ¤— Transformers\\nGPT-2 ëª¨ë¸ ì‚¬ìš©\"]\n",
    "        NP[\"ğŸ“Š NumPy\\në°ì´í„° ì „ì²˜ë¦¬\"] --> MPL[\"ğŸ“ˆ Matplotlib\\nì‹œê°í™”\"]\n",
    "    end\n",
    "\n",
    "    subgraph Optional[\"ì„ íƒ íŒ¨í‚¤ì§€\"]\n",
    "        MJ[\"ğŸ¤– MuJoCo\\në¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜\"]\n",
    "        D4[\"ğŸ“¦ D4RL\\në°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\"]\n",
    "    end\n",
    "\n",
    "    style Packages fill:#e3f2fd\n",
    "    style Optional fill:#fce4ec\n",
    "```\n",
    "\n",
    "ì•„ë˜ ì…€ì—ì„œ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“¦ íŒ¨í‚¤ì§€ ë²„ì „ í™•ì¸\n# ============================================================\n# Decision Transformer ì‹¤í–‰ì— í•„ìš”í•œ í•µì‹¬ íŒ¨í‚¤ì§€ë“¤ì…ë‹ˆë‹¤.\n# ê° íŒ¨í‚¤ì§€ì˜ ì—­í• :\n#   - PyTorch: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ (ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ)\n#   - NumPy: ìˆ˜ì¹˜ ê³„ì‚° (ë°ì´í„° ì „ì²˜ë¦¬)\n#   - Transformers: Hugging Faceì˜ GPT-2 ëª¨ë¸ ì‚¬ìš©\n# ============================================================\n\nimport sys\nprint(f\"ğŸ Python: {sys.version}\")\nprint()\n\n# PyTorch í™•ì¸\ntry:\n    import torch\n    print(f\"âœ… PyTorch: {torch.__version__}\")\n    print(f\"   â””â”€ CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"   â””â”€ CUDA device: {torch.cuda.get_device_name(0)}\")\nexcept ImportError:\n    print(\"âŒ PyTorch not installed!\")\n    print(\"   ì„¤ì¹˜: pip install torch\")\n\n# NumPy í™•ì¸\ntry:\n    import numpy as np\n    print(f\"âœ… NumPy: {np.__version__}\")\nexcept ImportError:\n    print(\"âŒ NumPy not installed!\")\n    print(\"   ì„¤ì¹˜: pip install numpy\")\n\n# Matplotlib í™•ì¸\ntry:\n    import matplotlib\n    print(f\"âœ… Matplotlib: {matplotlib.__version__}\")\nexcept ImportError:\n    print(\"âŒ Matplotlib not installed!\")\n    print(\"   ì„¤ì¹˜: pip install matplotlib\")\n\n# Transformers í™•ì¸\ntry:\n    import transformers\n    print(f\"âœ… Transformers: {transformers.__version__}\")\nexcept ImportError:\n    print(\"âš ï¸ Transformers not installed!\")\n    print(\"   ì„¤ì¹˜: pip install transformers\")\n    print(\"   (ì—†ì–´ë„ SimpleDTë¡œ ì‹¤ìŠµ ê°€ëŠ¥)\")\n\nprint()\nprint(\"=\"*50)\nprint(\"ëª¨ë“  í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì—ˆë‹¤ë©´ ë‹¤ìŒ ì…€ë¡œ ì§„í–‰í•˜ì„¸ìš”!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“‚ í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •\n# ============================================================\n# Pythonì´ gym/ í´ë”ì˜ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ê²½ë¡œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n#\n# ê²½ë¡œ êµ¬ì¡°:\n#   doc/notebooks/phase3_gym_practice.ipynb  (í˜„ì¬ íŒŒì¼)\n#        â†‘\n#        â”œâ”€â”€ .. (doc/)\n#        â””â”€â”€ ../.. (project root) â† PROJECT_ROOT\n#              â””â”€â”€ gym/           â† GYM_PATH (ì—¬ê¸°ë¥¼ sys.pathì— ì¶”ê°€)\n# ============================================================\n\nimport os\nimport sys\n\n# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ê³„ì‚°\n# os.getcwd(): í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ (ë³´í†µ ë…¸íŠ¸ë¶ ìœ„ì¹˜)\n# os.path.join(..., '..', '..'): ë‘ ë‹¨ê³„ ìƒìœ„ í´ë”ë¡œ ì´ë™\nPROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\nGYM_PATH = os.path.join(PROJECT_ROOT, 'gym')\n\n# sys.pathì— ê²½ë¡œ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)\n# ì´ë ‡ê²Œ í•˜ë©´ `from decision_transformer.models import ...` ê°€ ê°€ëŠ¥í•´ì§\nif GYM_PATH not in sys.path:\n    sys.path.insert(0, GYM_PATH)\n\nprint(\"ğŸ“‚ ê²½ë¡œ ì„¤ì • ê²°ê³¼:\")\nprint(f\"   Project root: {PROJECT_ROOT}\")\nprint(f\"   Gym path:     {GYM_PATH}\")\nprint()\n\n# ê²½ë¡œ ê²€ì¦\nif os.path.exists(GYM_PATH):\n    print(\"âœ… Gym ê²½ë¡œ ì¡´ì¬ í™•ì¸!\")\n    # í•˜ìœ„ í´ë” í™•ì¸\n    dt_path = os.path.join(GYM_PATH, 'decision_transformer')\n    if os.path.exists(dt_path):\n        print(\"âœ… decision_transformer ëª¨ë“ˆ ì¡´ì¬ í™•ì¸!\")\n    else:\n        print(\"âš ï¸ decision_transformer ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\nelse:\n    print(\"âŒ Gym ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!\")\n    print(\"   ë…¸íŠ¸ë¶ ìœ„ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. ë°ì´í„°ì…‹ íƒìƒ‰\n",
    "\n",
    "## ğŸ¯ ëª©í‘œ\n",
    "> D4RL ë°ì´í„°ì…‹ì˜ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê³ , ê°•í™”í•™ìŠµ ê¶¤ì (trajectory) ë°ì´í„°ê°€ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ íŒŒì•…í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“š D4RLì´ë€?\n",
    "\n",
    "**D4RL** (Datasets for Deep Data-Driven Reinforcement Learning)ì€ ì˜¤í”„ë¼ì¸ ê°•í™”í•™ìŠµ ì—°êµ¬ë¥¼ ìœ„í•œ í‘œì¤€ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph D4RL[\"ğŸ“¦ D4RL ë°ì´í„°ì…‹\"]\n",
    "        direction LR\n",
    "        subgraph Envs[\"ğŸ‹ï¸ í™˜ê²½\"]\n",
    "            E1[\"hopper\"]\n",
    "            E2[\"halfcheetah\"]\n",
    "            E3[\"walker2d\"]\n",
    "        end\n",
    "\n",
    "        subgraph Quality[\"ğŸ“Š ë°ì´í„° í’ˆì§ˆ\"]\n",
    "            Q1[\"medium\\nì¤‘ê°„ ìˆ˜ì¤€ ì •ì±…\"]\n",
    "            Q2[\"medium-replay\\ní•™ìŠµ ì¤‘ replay buffer\"]\n",
    "            Q3[\"medium-expert\\nmedium + expert í˜¼í•©\"]\n",
    "            Q4[\"expert\\nì „ë¬¸ê°€ ì •ì±…\"]\n",
    "        end\n",
    "\n",
    "        Envs --> Quality\n",
    "    end\n",
    "\n",
    "    subgraph Naming[\"ğŸ“ íŒŒì¼ëª… ê·œì¹™\"]\n",
    "        direction LR\n",
    "        N1[\"hopper\"] --- N2[\"-\"] --- N3[\"medium\"] --- N4[\"-v2\"] --- N5[\".pkl\"]\n",
    "        N1b[\"í™˜ê²½\"] ~~~ N3b[\"ë°ì´í„°ì…‹ ì¢…ë¥˜\"] ~~~ N4b[\"ë²„ì „\"]\n",
    "    end\n",
    "\n",
    "    style D4RL fill:#e3f2fd\n",
    "    style Envs fill:#fff3e0\n",
    "    style Quality fill:#e8f5e9\n",
    "    style Naming fill:#f3e5f5\n",
    "```\n",
    "\n",
    "## ğŸ¦¿ Hopper í™˜ê²½\n",
    "\n",
    "ì´ë²ˆ ì‹¤ìŠµì—ì„œ ì‚¬ìš©í•  **Hopper**ëŠ” í•œ ë‹¤ë¦¬ë¡œ ë›°ëŠ” ë¡œë´‡ì…ë‹ˆë‹¤.\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Hopper[\"ğŸ¦¿ Hopper ë¡œë´‡\"]\n",
    "        direction TB\n",
    "        Torso[\"â— ëª¸í†µ torso\"] --> Thigh[\"â”‚ í—ˆë²…ì§€ thigh\"]\n",
    "        Thigh --> Leg[\"â”‚ ì •ê°•ì´ leg\"]\n",
    "        Leg --> Foot[\"â”€â”€ ë°œ foot\"]\n",
    "    end\n",
    "\n",
    "    subgraph Specs[\"ğŸ“‹ í™˜ê²½ ìŠ¤í™\"]\n",
    "        direction TB\n",
    "        SP1[\"State ì°¨ì›: 11\\nìœ„ì¹˜, ì†ë„, ê°ë„ ë“±\"]\n",
    "        SP2[\"Action ì°¨ì›: 3\\nê° ê´€ì ˆì˜ í† í¬\"]\n",
    "        SP3[\"ëª©í‘œ: ìµœëŒ€í•œ ë©€ë¦¬ ë›°ê¸°\\në„˜ì–´ì§€ì§€ ì•Šê³  ì „ì§„\"]\n",
    "    end\n",
    "\n",
    "    Hopper --- Specs\n",
    "\n",
    "    style Hopper fill:#fff3e0\n",
    "    style Specs fill:#e8f5e9\n",
    "```\n",
    "\n",
    "## ğŸ“Š ê¶¤ì (Trajectory) ë°ì´í„° êµ¬ì¡°\n",
    "\n",
    "D4RL ë°ì´í„°ì…‹ì€ ì—¬ëŸ¬ ê°œì˜ **ê¶¤ì (trajectory)**ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.  \n",
    "ê° ê¶¤ì ì€ ì—ì´ì „íŠ¸ê°€ í™˜ê²½ì—ì„œ í•œ ì—í”¼ì†Œë“œ ë™ì•ˆ ê²½í—˜í•œ ë°ì´í„°ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Dataset[\"ğŸ“‚ trajectories ë¦¬ìŠ¤íŠ¸\"]\n",
    "        direction TB\n",
    "        T1[\"traj_0\"] \n",
    "        T2[\"traj_1\"]\n",
    "        TN[\"traj_N\"]\n",
    "        T1 ~~~ T2 ~~~ TN\n",
    "    end\n",
    "\n",
    "    subgraph OneTraj[\"ğŸ“‹ ê° trajectory êµ¬ì¡°\"]\n",
    "        direction TB\n",
    "        OBS[\"observations: (T, 11)\\nsâ‚€, sâ‚, ..., s_T\"]\n",
    "        ACT[\"actions: (T, 3)\\naâ‚€, aâ‚, ..., a_T\"]\n",
    "        REW[\"rewards: (T,)\\nrâ‚€, râ‚, ..., r_T\"]\n",
    "    end\n",
    "\n",
    "    Dataset --> OneTraj\n",
    "\n",
    "    style Dataset fill:#e3f2fd\n",
    "    style OBS fill:#c8e6c9\n",
    "    style ACT fill:#bbdefb\n",
    "    style REW fill:#ffcdd2\n",
    "```\n",
    "\n",
    "> ğŸ’¡ **TëŠ” ì—í”¼ì†Œë“œ ê¸¸ì´**ë¡œ, ê¶¤ì ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "> Hopperì˜ ê²½ìš° ë³´í†µ 100~1000 ìŠ¤í… ì •ë„ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“‚ ë°ì´í„°ì…‹ íŒŒì¼ í™•ì¸\n# ============================================================\n# D4RL ë°ì´í„°ì…‹ì€ pickle í˜•ì‹ìœ¼ë¡œ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n# ê° íŒŒì¼ì—ëŠ” ì—¬ëŸ¬ ê°œì˜ ì—í”¼ì†Œë“œ(ê¶¤ì )ê°€ ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤.\n# ============================================================\n\nimport pickle\nimport numpy as np\n\n# ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ\ndata_dir = os.path.join(GYM_PATH, 'data')\n\nprint(\"ğŸ“‚ ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ íƒìƒ‰\")\nprint(\"=\"*60)\n\nif os.path.exists(data_dir):\n    files = os.listdir(data_dir)\n    pkl_files = [f for f in files if f.endswith('.pkl')]\n    \n    if pkl_files:\n        print(f\"\\nâœ… ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ({len(pkl_files)}ê°œ):\\n\")\n        print(f\"{'íŒŒì¼ëª…':<35} {'í¬ê¸°':>10}\")\n        print(\"-\"*50)\n        for f in sorted(pkl_files):\n            size = os.path.getsize(os.path.join(data_dir, f)) / (1024*1024)\n            print(f\"  {f:<33} {size:>8.1f} MB\")\n    else:\n        print(\"\\nâš ï¸ .pkl íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n        print(\"\\në°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë°©ë²•:\")\n        print(\"  cd gym\")\n        print(\"  python data/download_d4rl_datasets.py\")\nelse:\n    print(f\"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {data_dir}\")\n    print(\"\\në°ì´í„°ì…‹ì„ ë¨¼ì € ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:\")\n    print(\"  cd gym && python data/download_d4rl_datasets.py\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“¥ ë°ì´í„°ì…‹ ë¡œë“œ\n# ============================================================\n# hopper-medium-v2 ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n# íŒŒì¼ì´ ì—†ìœ¼ë©´ ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n#\n# ë°ì´í„° êµ¬ì¡°:\n#   trajectories = [traj_1, traj_2, ..., traj_N]\n#   ê° trajëŠ” í•˜ë‚˜ì˜ ì—í”¼ì†Œë“œ(ê²Œì„ í•œ íŒ)ë¥¼ ì˜ë¯¸\n# ============================================================\n\ndataset_path = os.path.join(data_dir, 'hopper-medium-v2.pkl')\n\nprint(\"ğŸ“¥ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...\")\nprint(\"=\"*60)\n\nif os.path.exists(dataset_path):\n    # ì‹¤ì œ ë°ì´í„°ì…‹ ë¡œë“œ\n    with open(dataset_path, 'rb') as f:\n        trajectories = pickle.load(f)\n    \n    print(f\"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ!\")\n    print(f\"\\nğŸ“Š ê¸°ë³¸ ì •ë³´:\")\n    print(f\"   ì´ ê¶¤ì (ì—í”¼ì†Œë“œ) ìˆ˜: {len(trajectories)}\")\n    print(f\"   ì²« ë²ˆì§¸ ê¶¤ì ì˜ í‚¤: {list(trajectories[0].keys())}\")\n    \n    DATA_SOURCE = \"D4RL\"\nelse:\n    # ë”ë¯¸ ë°ì´í„° ìƒì„±\n    print(f\"âš ï¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {dataset_path}\")\n    print(\"\\nğŸ”§ ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...\")\n    print(\"   (ì‹¤ì œ ë°ì´í„°ì™€ ë™ì¼í•œ êµ¬ì¡°ë¡œ ìƒì„±)\")\n    \n    np.random.seed(42)  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ\n    trajectories = []\n    \n    for i in range(100):  # 100ê°œì˜ ì—í”¼ì†Œë“œ\n        ep_len = np.random.randint(100, 500)  # ì—í”¼ì†Œë“œ ê¸¸ì´\n        traj = {\n            'observations': np.random.randn(ep_len, 11).astype(np.float32),  # 11ì°¨ì› ìƒíƒœ\n            'actions': np.clip(np.random.randn(ep_len, 3), -1, 1).astype(np.float32),  # 3ì°¨ì› í–‰ë™\n            'rewards': np.random.uniform(0, 3, ep_len).astype(np.float32)  # ë³´ìƒ\n        }\n        trajectories.append(traj)\n    \n    print(f\"âœ… ë”ë¯¸ ê¶¤ì  {len(trajectories)}ê°œ ìƒì„± ì™„ë£Œ!\")\n    DATA_SOURCE = \"Dummy\"\n\nprint(f\"\\nğŸ“Œ ë°ì´í„° ì¶œì²˜: {DATA_SOURCE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ” ê¶¤ì  ë°ì´í„° ìƒì„¸ ë¶„ì„\n# ============================================================\n# í•˜ë‚˜ì˜ ê¶¤ì (trajectory)ì´ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ ì‚´í´ë´…ë‹ˆë‹¤.\n#\n# ê¶¤ì  êµ¬ì¡°:\n# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n# â”‚  traj = {                                               â”‚\n# â”‚    'observations': [s_0, s_1, s_2, ..., s_T]           â”‚\n# â”‚    'actions':      [a_0, a_1, a_2, ..., a_T]           â”‚\n# â”‚    'rewards':      [r_0, r_1, r_2, ..., r_T]           â”‚\n# â”‚  }                                                     â”‚\n# â”‚                                                        â”‚\n# â”‚  T = ì—í”¼ì†Œë“œ ê¸¸ì´ (í™˜ê²½ë§ˆë‹¤ ë‹¤ë¦„)                        â”‚\n# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n# ============================================================\n\n# ì²« ë²ˆì§¸ ê¶¤ì  ì„ íƒ\ntraj = trajectories[0]\n\nprint(\"ğŸ” ì²« ë²ˆì§¸ ê¶¤ì  ìƒì„¸ ë¶„ì„\")\nprint(\"=\"*60)\n\n# Shape ì •ë³´\nprint(\"\\nğŸ“ Shape ì •ë³´:\")\nprint(f\"   observations: {traj['observations'].shape}\")\nprint(f\"                 â””â”€ (ì—í”¼ì†Œë“œ ê¸¸ì´, state ì°¨ì›)\")\nprint(f\"   actions:      {traj['actions'].shape}\")\nprint(f\"                 â””â”€ (ì—í”¼ì†Œë“œ ê¸¸ì´, action ì°¨ì›)\")\nprint(f\"   rewards:      {traj['rewards'].shape}\")\nprint(f\"                 â””â”€ (ì—í”¼ì†Œë“œ ê¸¸ì´,)\")\n\n# ì—í”¼ì†Œë“œ í†µê³„\nep_len = len(traj['rewards'])\ntotal_return = sum(traj['rewards'])\n\nprint(f\"\\nğŸ“Š ì—í”¼ì†Œë“œ í†µê³„:\")\nprint(f\"   ì—í”¼ì†Œë“œ ê¸¸ì´: {ep_len} ìŠ¤í…\")\nprint(f\"   ì´ Return: {total_return:.2f}\")\nprint(f\"   í‰ê·  Reward: {total_return / ep_len:.4f}\")\n\n# ì‹¤ì œ ë°ì´í„° ìƒ˜í”Œ\nprint(f\"\\nğŸ“ ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 3 ìŠ¤í…):\")\nprint()\nprint(\"   Timestep 0:\")\nprint(f\"     State:  {traj['observations'][0][:5]}... (ì• 5ê°œë§Œ í‘œì‹œ)\")\nprint(f\"     Action: {traj['actions'][0]}\")\nprint(f\"     Reward: {traj['rewards'][0]:.4f}\")\nprint()\nprint(\"   Timestep 1:\")\nprint(f\"     State:  {traj['observations'][1][:5]}...\")\nprint(f\"     Action: {traj['actions'][1]}\")\nprint(f\"     Reward: {traj['rewards'][1]:.4f}\")\nprint()\nprint(\"   Timestep 2:\")\nprint(f\"     State:  {traj['observations'][2][:5]}...\")\nprint(f\"     Action: {traj['actions'][2]}\")\nprint(f\"     Reward: {traj['rewards'][2]:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“Š ë°ì´í„°ì…‹ ì „ì²´ í†µê³„ ì‹œê°í™”\n# ============================================================\n# ì „ì²´ ë°ì´í„°ì…‹ì˜ ë¶„í¬ë¥¼ ì‹œê°í™”í•˜ì—¬ ë°ì´í„° íŠ¹ì„±ì„ íŒŒì•…í•©ë‹ˆë‹¤.\n# - ì—í”¼ì†Œë“œ ê¸¸ì´ ë¶„í¬: ì—í”¼ì†Œë“œê°€ ì–¼ë§ˆë‚˜ ê¸¸ê²Œ ì§€ì†ë˜ëŠ”ê°€?\n# - Return ë¶„í¬: ì •ì±…ì˜ í’ˆì§ˆì´ ì–´ë– í•œê°€?\n# ============================================================\n\nimport matplotlib.pyplot as plt\n\n# í•œê¸€ í°íŠ¸ ì„¤ì • (utils.pyì—ì„œ ê°€ì ¸ì˜´)\ntry:\n    from utils import setup_matplotlib\nexcept:\n    plt.rcParams['axes.unicode_minus'] = False\n\n# í†µê³„ ê³„ì‚°\nlengths = [len(t['rewards']) for t in trajectories]\nreturns = [sum(t['rewards']) for t in trajectories]\n\nprint(\"ğŸ“Š ë°ì´í„°ì…‹ í†µê³„ ì‹œê°í™”\")\nprint(\"=\"*60)\n\n# ê·¸ë˜í”„ ìƒì„±\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 1. ì—í”¼ì†Œë“œ ê¸¸ì´ ë¶„í¬\nax1 = axes[0]\nax1.hist(lengths, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\nax1.axvline(np.mean(lengths), color='red', linestyle='--', linewidth=2, \n            label=f'Mean: {np.mean(lengths):.0f}')\nax1.axvline(np.median(lengths), color='orange', linestyle=':', linewidth=2,\n            label=f'Median: {np.median(lengths):.0f}')\nax1.set_xlabel('Episode Length (steps)', fontsize=12)\nax1.set_ylabel('Count', fontsize=12)\nax1.set_title('Episode Length Distribution', fontsize=14)\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 2. Return ë¶„í¬\nax2 = axes[1]\nax2.hist(returns, bins=30, edgecolor='black', alpha=0.7, color='forestgreen')\nax2.axvline(np.mean(returns), color='red', linestyle='--', linewidth=2,\n            label=f'Mean: {np.mean(returns):.0f}')\nax2.axvline(np.median(returns), color='orange', linestyle=':', linewidth=2,\n            label=f'Median: {np.median(returns):.0f}')\nax2.set_xlabel('Episode Return', fontsize=12)\nax2.set_ylabel('Count', fontsize=12)\nax2.set_title('Episode Return Distribution', fontsize=14)\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# ìˆ˜ì¹˜ í†µê³„ ì¶œë ¥\nprint(f\"\\nğŸ“ˆ ìˆ˜ì¹˜ í†µê³„:\")\nprint(f\"â”Œ{'â”€'*50}â”\")\nprint(f\"â”‚ {'í•­ëª©':<20} {'ê°’':>25} â”‚\")\nprint(f\"â”œ{'â”€'*50}â”¤\")\nprint(f\"â”‚ {'ì´ ì—í”¼ì†Œë“œ ìˆ˜':<20} {len(trajectories):>25,} â”‚\")\nprint(f\"â”‚ {'í‰ê·  ì—í”¼ì†Œë“œ ê¸¸ì´':<20} {np.mean(lengths):>22.1f} Â± {np.std(lengths):.1f} â”‚\")\nprint(f\"â”‚ {'ìµœì†Œ/ìµœëŒ€ ê¸¸ì´':<20} {min(lengths):>11} / {max(lengths):<11} â”‚\")\nprint(f\"â”‚ {'í‰ê·  Return':<20} {np.mean(returns):>22.1f} Â± {np.std(returns):.1f} â”‚\")\nprint(f\"â”‚ {'ìµœì†Œ/ìµœëŒ€ Return':<20} {min(returns):>11.1f} / {max(returns):<11.1f} â”‚\")\nprint(f\"â””{'â”€'*50}â”˜\")\n\n# Decision Transformerì—ì„œ ì‚¬ìš©í•  ëª©í‘œ Return ì°¸ê³ \nprint(f\"\\nğŸ’¡ íŒ: í•™ìŠµ ì‹œ ëª©í‘œ Returnì„ í‰ê· ({np.mean(returns):.0f}) ì´ìƒìœ¼ë¡œ ì„¤ì •í•˜ë©´\")\nprint(f\"       ë” ë‚˜ì€ ì •ì±…ì„ ìœ ë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ RTG (Return-to-Go) ê³„ì‚° ì‹¤ìŠµ\n",
    "\n",
    "> **RTG**ëŠ” Decision Transformerì˜ í•µì‹¬ ê°œë…ì…ë‹ˆë‹¤.  \n",
    "> \"í˜„ì¬ ì‹œì ë¶€í„° ì—í”¼ì†Œë“œ ëê¹Œì§€ ì–»ì„ ìˆ˜ ìˆëŠ” ì´ ë³´ìƒ\"ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "### RTG ê³„ì‚° ê³µì‹\n",
    "\n",
    "$$\\text{RTG}_t = r_t + r_{t+1} + r_{t+2} + \\cdots + r_T$$\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Example[\"ğŸ“ RTG ê³„ì‚° ì˜ˆì‹œ: rewards = 1, 2, 3, 4, 5\"]\n",
    "        direction TB\n",
    "        subgraph T0[\"t=0\"]\n",
    "            R0[\"ë‚¨ì€ ë³´ìƒ: 1+2+3+4+5\\nRTG = 15\"]\n",
    "        end\n",
    "        subgraph T1[\"t=1\"]\n",
    "            R1[\"ë‚¨ì€ ë³´ìƒ: 2+3+4+5\\nRTG = 14\"]\n",
    "        end\n",
    "        subgraph T2[\"t=2\"]\n",
    "            R2[\"ë‚¨ì€ ë³´ìƒ: 3+4+5\\nRTG = 12\"]\n",
    "        end\n",
    "        subgraph T3[\"t=3\"]\n",
    "            R3[\"ë‚¨ì€ ë³´ìƒ: 4+5\\nRTG = 9\"]\n",
    "        end\n",
    "        subgraph T4[\"t=4\"]\n",
    "            R4[\"ë‚¨ì€ ë³´ìƒ: 5\\nRTG = 5\"]\n",
    "        end\n",
    "        T0 --> T1 --> T2 --> T3 --> T4\n",
    "    end\n",
    "\n",
    "    style T0 fill:#ffcdd2\n",
    "    style T1 fill:#ffccbc\n",
    "    style T2 fill:#fff9c4\n",
    "    style T3 fill:#dcedc8\n",
    "    style T4 fill:#c8e6c9\n",
    "```\n",
    "\n",
    "### í• ì¸ìœ¨(Î³)ì„ ì ìš©í•œ RTG\n",
    "\n",
    "ì‹¤ì œë¡œëŠ” **í• ì¸ìœ¨(discount factor, Î³)**ì„ ì ìš©í•˜ê¸°ë„ í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$\\text{RTG}_t = r_t + \\gamma \\cdot r_{t+1} + \\gamma^2 \\cdot r_{t+2} + \\cdots$$\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Gamma[\"âš™ï¸ í• ì¸ìœ¨ì— ë”°ë¥¸ ì°¨ì´\"]\n",
    "        direction TB\n",
    "        G1[\"Î³ = 1.0\\nëª¨ë“  ë¯¸ë˜ ë³´ìƒì„ ë™ë“±í•˜ê²Œ\\nâ†’ Decision Transformer ë…¼ë¬¸ì—ì„œ ì‚¬ìš©\"]\n",
    "        G2[\"Î³ = 0.99\\në¨¼ ë¯¸ë˜ ë³´ìƒì„ ì•½ê°„ í• ì¸\"]\n",
    "        G3[\"Î³ = 0.9\\në¨¼ ë¯¸ë˜ ë³´ìƒì„ ë§ì´ í• ì¸\"]\n",
    "    end\n",
    "\n",
    "    style G1 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px\n",
    "    style G2 fill:#fff9c4\n",
    "    style G3 fill:#ffcdd2\n",
    "```\n",
    "\n",
    "### ğŸ§® RTG ì—­ìˆœ ê³„ì‚° ì•Œê³ ë¦¬ì¦˜\n",
    "\n",
    "RTGëŠ” **ë’¤ì—ì„œ ì•ìœ¼ë¡œ(ì—­ìˆœ)** ê³„ì‚°í•˜ë©´ íš¨ìœ¨ì ì…ë‹ˆë‹¤:\n",
    "\n",
    "```mermaid\n",
    "flowchart RL\n",
    "    subgraph Algorithm[\"ğŸ”„ ì—­ìˆœ ê³„ì‚°\"]\n",
    "        direction RL\n",
    "        S4[\"t=4: RTG=râ‚„=5\"] --> S3[\"t=3: RTG=râ‚ƒ+Î³Â·5=9\"]\n",
    "        S3 --> S2[\"t=2: RTG=râ‚‚+Î³Â·9=12\"]\n",
    "        S2 --> S1[\"t=1: RTG=râ‚+Î³Â·12=14\"]\n",
    "        S1 --> S0[\"t=0: RTG=râ‚€+Î³Â·14=15\"]\n",
    "    end\n",
    "\n",
    "    style Algorithm fill:#e3f2fd\n",
    "    style S0 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px\n",
    "```\n",
    "\n",
    "> ğŸ’¡ **Decision Transformer ë…¼ë¬¸ì—ì„œëŠ” Î³=1.0 (í• ì¸ ì—†ìŒ)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.**  \n",
    "> ì´ëŠ” ëª¨ë“  ë¯¸ë˜ ë³´ìƒì„ ë™ë“±í•˜ê²Œ ì·¨ê¸‰í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ”¢ RTG ê³„ì‚° í•¨ìˆ˜ êµ¬í˜„\n# ============================================================\n# experiment.pyì—ì„œ ì‚¬ìš©í•˜ëŠ” discount_cumsum í•¨ìˆ˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n# \n# ì•Œê³ ë¦¬ì¦˜:\n#   1. ë§ˆì§€ë§‰ ì‹œì ì˜ RTG = ë§ˆì§€ë§‰ reward\n#   2. ì—­ìˆœìœ¼ë¡œ ì§„í–‰í•˜ë©° RTG[t] = reward[t] + Î³ * RTG[t+1]\n# ============================================================\n\ndef discount_cumsum(x, gamma=1.0):\n    \"\"\"\n    í• ì¸ëœ ëˆ„ì í•©(RTG) ê³„ì‚°\n    \n    Parameters:\n    -----------\n    x : numpy.ndarray\n        reward ì‹œí€€ìŠ¤ [r_0, r_1, ..., r_T]\n    gamma : float\n        í• ì¸ìœ¨ (default: 1.0, ì¦‰ í• ì¸ ì—†ìŒ)\n    \n    Returns:\n    --------\n    numpy.ndarray\n        RTG ì‹œí€€ìŠ¤ [RTG_0, RTG_1, ..., RTG_T]\n    \n    ì˜ˆì‹œ:\n    -----\n    >>> rewards = [1, 2, 3]\n    >>> discount_cumsum(rewards, gamma=1.0)\n    array([6, 5, 3])  # [1+2+3, 2+3, 3]\n    \"\"\"\n    # ê²°ê³¼ ë°°ì—´ ì´ˆê¸°í™”\n    discount_cumsum_arr = np.zeros_like(x)\n    \n    # ë§ˆì§€ë§‰ ì‹œì : RTG[-1] = reward[-1]\n    discount_cumsum_arr[-1] = x[-1]\n    \n    # ì—­ìˆœìœ¼ë¡œ ê³„ì‚°: RTG[t] = reward[t] + Î³ * RTG[t+1]\n    for t in reversed(range(x.shape[0] - 1)):\n        discount_cumsum_arr[t] = x[t] + gamma * discount_cumsum_arr[t + 1]\n    \n    return discount_cumsum_arr\n\n# ============================================================\n# ğŸ“ RTG ê³„ì‚° ì˜ˆì‹œ\n# ============================================================\nprint(\"ğŸ”¢ RTG ê³„ì‚° ì˜ˆì‹œ\")\nprint(\"=\"*60)\n\n# ê°„ë‹¨í•œ ì˜ˆì‹œ\nrewards = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n\n# í• ì¸ìœ¨ì— ë”°ë¥¸ RTG ë¹„êµ\nrtg_gamma1 = discount_cumsum(rewards, gamma=1.0)\nrtg_gamma099 = discount_cumsum(rewards, gamma=0.99)\nrtg_gamma09 = discount_cumsum(rewards, gamma=0.9)\n\nprint(f\"\\nRewards: {rewards}\")\nprint()\nprint(f\"{'ì‹œì  t':<8} {'Î³=1.0':>10} {'Î³=0.99':>12} {'Î³=0.9':>12}\")\nprint(\"-\"*45)\nfor t in range(len(rewards)):\n    print(f\"  {t:<6} {rtg_gamma1[t]:>10.2f} {rtg_gamma099[t]:>12.2f} {rtg_gamma09[t]:>12.2f}\")\n\nprint()\nprint(\"ğŸ’¡ Î³=1.0: ëª¨ë“  ë¯¸ë˜ ë³´ìƒì„ ë™ë“±í•˜ê²Œ í•©ì‚°\")\nprint(\"   Î³<1.0: ë¨¼ ë¯¸ë˜ ë³´ìƒì¼ìˆ˜ë¡ ë” ì ê²Œ ë°˜ì˜\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“ˆ ì‹¤ì œ ê¶¤ì ì—ì„œ RTG ì‹œê°í™”\n# ============================================================\n# ì‹¤ì œ ë°ì´í„°ì—ì„œ Rewardì™€ RTGê°€ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ì‹œê°í™”í•©ë‹ˆë‹¤.\n#\n# ê´€ì°° í¬ì¸íŠ¸:\n#   - Reward: ìˆœê°„ì ì¸ ë³´ìƒ (ë…¸ì´ì¦ˆê°€ ë§ìŒ)\n#   - RTG: ë¯¸ë˜ ë³´ìƒì˜ í•© (ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ê°ì†Œ)\n# ============================================================\n\n# ì²« ë²ˆì§¸ ê¶¤ì ì˜ ë°ì´í„°\ntraj = trajectories[0]\nrewards_traj = traj['rewards']\nrtg = discount_cumsum(rewards_traj, gamma=1.0)\n\nprint(\"ğŸ“ˆ ì‹¤ì œ ê¶¤ì ì—ì„œ Reward vs RTG\")\nprint(\"=\"*60)\n\n# ê·¸ë˜í”„ (ì²˜ìŒ 100 ìŠ¤í…ë§Œ)\nnum_steps = min(100, len(rewards_traj))\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 1. Reward over time\nax1 = axes[0]\nax1.plot(rewards_traj[:num_steps], alpha=0.8, linewidth=1.5, color='blue')\nax1.fill_between(range(num_steps), rewards_traj[:num_steps], alpha=0.3, color='blue')\nax1.axhline(y=np.mean(rewards_traj[:num_steps]), color='red', linestyle='--', \n            label=f'Mean: {np.mean(rewards_traj[:num_steps]):.2f}')\nax1.set_xlabel('Timestep', fontsize=12)\nax1.set_ylabel('Reward', fontsize=12)\nax1.set_title('Instantaneous Reward Over Time', fontsize=14)\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 2. RTG over time\nax2 = axes[1]\nax2.plot(rtg[:num_steps], alpha=0.8, linewidth=2, color='orange')\nax2.fill_between(range(num_steps), rtg[:num_steps], alpha=0.3, color='orange')\nax2.set_xlabel('Timestep', fontsize=12)\nax2.set_ylabel('Return-to-Go', fontsize=12)\nax2.set_title('Return-to-Go Over Time', fontsize=14)\nax2.grid(True, alpha=0.3)\n\n# RTG ê°ì†Œ ì¶”ì„¸ í‘œì‹œ\nax2.annotate(f'ì‹œì‘: {rtg[0]:.0f}', xy=(0, rtg[0]), fontsize=10,\n             xytext=(10, rtg[0] - (rtg[0] - rtg[num_steps-1])*0.2),\n             arrowprops=dict(arrowstyle='->', color='gray'))\nax2.annotate(f't={num_steps-1}: {rtg[num_steps-1]:.0f}', \n             xy=(num_steps-1, rtg[num_steps-1]), fontsize=10,\n             xytext=(num_steps-20, rtg[num_steps-1] + (rtg[0] - rtg[num_steps-1])*0.2),\n             arrowprops=dict(arrowstyle='->', color='gray'))\n\nplt.tight_layout()\nplt.show()\n\n# í•µì‹¬ í¬ì¸íŠ¸ ì¶œë ¥\nprint(f\"\\nğŸ“Œ í•µì‹¬ ê´€ì°°:\")\nprint(f\"   â€¢ ì´ˆê¸° RTG (t=0): {rtg[0]:.2f}\")\nprint(f\"     â””â”€ ì—í”¼ì†Œë“œ ì „ì²´ì—ì„œ ì–»ì„ ì´ ë³´ìƒ\")\nprint(f\"   â€¢ ì¤‘ê°„ RTG (t={num_steps//2}): {rtg[num_steps//2]:.2f}\")\nprint(f\"     â””â”€ ë‚¨ì€ ì ˆë°˜ ë™ì•ˆ ì–»ì„ ë³´ìƒ\")\nprint(f\"   â€¢ ìµœì¢… RTG (t={num_steps-1}): {rtg[num_steps-1]:.2f}\")\nprint(f\"     â””â”€ ê±°ì˜ ì—í”¼ì†Œë“œ ë, ë‚¨ì€ ë³´ìƒ ì ìŒ\")\nprint()\nprint(\"ğŸ’¡ RTGëŠ” ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ë‹¨ì¡° ê°ì†Œí•©ë‹ˆë‹¤!\")\nprint(\"   (ë§¤ ìŠ¤í… rewardë¥¼ ë°›ìœ¼ë©´ì„œ 'ë‚¨ì€ ë³´ìƒ'ì´ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. ëª¨ë¸ ì½”ë“œ ë¶„ì„\n",
    "\n",
    "## ğŸ¯ ëª©í‘œ\n",
    "> Decision Transformer ëª¨ë¸ì˜ ë‚´ë¶€ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê³ , ê° ì»´í¬ë„ŒíŠ¸ì˜ ì—­í• ì„ íŒŒì•…í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ—ï¸ DecisionTransformer ì „ì²´ ì•„í‚¤í…ì²˜\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Inputs[\"ğŸ“¥ ì…ë ¥\"]\n",
    "        RTG[\"RTG\\n(batch, K, 1)\"]\n",
    "        State[\"State\\n(batch, K, 11)\"]\n",
    "        Action[\"Action\\n(batch, K, 3)\"]\n",
    "        Time[\"Timestep\\n(batch, K)\"]\n",
    "    end\n",
    "\n",
    "    subgraph Embedding[\"1ï¸âƒ£ ì„ë² ë”© ë ˆì´ì–´\"]\n",
    "        RTG -->|\"Linear(1â†’128)\"| RE[\"RTG Emb\"]\n",
    "        State -->|\"Linear(11â†’128)\"| SE[\"State Emb\"]\n",
    "        Action -->|\"Linear(3â†’128)\"| AE[\"Action Emb\"]\n",
    "        Time -->|\"Embedding(1000, 128)\"| TE[\"Time Emb\"]\n",
    "\n",
    "        RE --- |\"+ Time\"| REF[\"R + T\"]\n",
    "        SE --- |\"+ Time\"| SEF[\"S + T\"]\n",
    "        AE --- |\"+ Time\"| AEF[\"A + T\"]\n",
    "    end\n",
    "\n",
    "    subgraph Interleave[\"2ï¸âƒ£ ì‹œí€€ìŠ¤ êµ¬ì„± (Interleave)\"]\n",
    "        REF --> Stack\n",
    "        SEF --> Stack\n",
    "        AEF --> Stack\n",
    "        Stack[\"[Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚, aâ‚, ...]\\nShape: (batch, KÃ—3, 128)\"] --> LN[\"LayerNorm\"]\n",
    "    end\n",
    "\n",
    "    subgraph Transformer[\"3ï¸âƒ£ GPT-2 Transformer\"]\n",
    "        LN --> L1[\"Layer 1: Attention â†’ FFN â†’ LN\"]\n",
    "        L1 --> L2[\"Layer 2: Attention â†’ FFN â†’ LN\"]\n",
    "        L2 --> L3[\"Layer 3: Attention â†’ FFN â†’ LN\"]\n",
    "        L3 --> Out[\"(batch, KÃ—3, 128)\"]\n",
    "    end\n",
    "\n",
    "    subgraph Heads[\"4ï¸âƒ£ ì˜ˆì¸¡ í—¤ë“œ\"]\n",
    "        Out -->|\"[:, 1::3, :]\\nstate ìœ„ì¹˜\"| PA[\"â­ predict_action\\nLinear(128â†’3)\"]\n",
    "        Out -->|\"[:, 2::3, :]\"| PS[\"predict_state\\n(ë¯¸ì‚¬ìš©)\"]\n",
    "        Out -->|\"[:, 2::3, :]\"| PR[\"predict_return\\n(ë¯¸ì‚¬ìš©)\"]\n",
    "    end\n",
    "\n",
    "    style Inputs fill:#e1f5fe\n",
    "    style Embedding fill:#fff3e0\n",
    "    style Interleave fill:#f3e5f5\n",
    "    style Transformer fill:#e8f5e9\n",
    "    style Heads fill:#ffebee\n",
    "    style PA fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px\n",
    "    style PS fill:#eeeeee,stroke:#bdbdbd,stroke-dasharray: 5 5\n",
    "    style PR fill:#eeeeee,stroke:#bdbdbd,stroke-dasharray: 5 5\n",
    "```\n",
    "\n",
    "## ğŸ”‘ í•µì‹¬ ì•„ì´ë””ì–´: ì‹œí€€ìŠ¤ êµ¬ì„± (Interleaving)\n",
    "\n",
    "Decision Transformerì˜ ê°€ì¥ ë…íŠ¹í•œ ì„¤ê³„ëŠ” **ì„¸ ì¢…ë¥˜ì˜ í† í°ì„ ì¸í„°ë¦¬ë¹™**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Sequence[\"ğŸ“ í† í° ì‹œí€€ìŠ¤ êµ¬ì„±\"]\n",
    "        direction LR\n",
    "        R0[\"RÌ‚â‚€\"] --> S0[\"sâ‚€\"] --> A0[\"aâ‚€\"]\n",
    "        A0 --> R1[\"RÌ‚â‚\"] --> S1[\"sâ‚\"] --> A1[\"aâ‚\"]\n",
    "        A1 --> R2[\"RÌ‚â‚‚\"] --> S2[\"sâ‚‚\"] --> A2[\"aâ‚‚\"]\n",
    "    end\n",
    "\n",
    "    subgraph Predict[\"ğŸ¯ ì˜ˆì¸¡ ë°©í–¥\"]\n",
    "        direction TB\n",
    "        P1[\"State í† í° ìœ„ì¹˜ì—ì„œ\\nActionì„ ì˜ˆì¸¡\"]\n",
    "        P2[\"ì¦‰, sâ‚‚ ìœ„ì¹˜ì˜ ì¶œë ¥ìœ¼ë¡œ\\naâ‚‚ë¥¼ ì˜ˆì¸¡\"]\n",
    "    end\n",
    "\n",
    "    Sequence --> Predict\n",
    "\n",
    "    style R0 fill:#ffcdd2\n",
    "    style R1 fill:#ffcdd2\n",
    "    style R2 fill:#ffcdd2\n",
    "    style S0 fill:#c8e6c9\n",
    "    style S1 fill:#c8e6c9\n",
    "    style S2 fill:#c8e6c9\n",
    "    style A0 fill:#bbdefb\n",
    "    style A1 fill:#bbdefb\n",
    "    style A2 fill:#bbdefb\n",
    "```\n",
    "\n",
    "## ğŸ” Causal Attention Mask\n",
    "\n",
    "GPT-2ì˜ **Causal Attention**ì€ ë¯¸ë˜ í† í°ì„ ë³¼ ìˆ˜ ì—†ê²Œ í•©ë‹ˆë‹¤:  \n",
    "ê° í† í°ì€ ìì‹ ê³¼ ê³¼ê±° í† í°ë§Œ ì°¸ì¡°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Mask[\"ğŸ­ Causal Mask ì‹œê°í™”\"]\n",
    "        direction TB\n",
    "        M1[\"RÌ‚â‚€ â†’ RÌ‚â‚€ë§Œ ë³¼ ìˆ˜ ìˆìŒ\"]\n",
    "        M2[\"sâ‚€ â†’ RÌ‚â‚€, sâ‚€ ë³¼ ìˆ˜ ìˆìŒ\"]\n",
    "        M3[\"aâ‚€ â†’ RÌ‚â‚€, sâ‚€, aâ‚€ ë³¼ ìˆ˜ ìˆìŒ\"]\n",
    "        M4[\"RÌ‚â‚ â†’ RÌ‚â‚€, sâ‚€, aâ‚€, RÌ‚â‚ ë³¼ ìˆ˜ ìˆìŒ\"]\n",
    "        M5[\"sâ‚ â†’ ... , RÌ‚â‚, sâ‚ê¹Œì§€ ë³¼ ìˆ˜ ìˆìŒ\"]\n",
    "    end\n",
    "\n",
    "    style Mask fill:#fff3e0\n",
    "```\n",
    "\n",
    "> ğŸ’¡ **ì™œ Causal Maskë¥¼ ì‚¬ìš©í•˜ë‚˜ìš”?**  \n",
    "> ì¶”ë¡  ì‹œì—ëŠ” ë¯¸ë˜ ì •ë³´ë¥¼ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, í•™ìŠµ ì‹œì—ë„ ë¯¸ë˜ë¥¼ ë³´ì§€ ëª»í•˜ê²Œ í•´ì•¼ í•©ë‹ˆë‹¤.  \n",
    "> ì´ëŠ” GPTì™€ ë™ì¼í•œ ì›ë¦¬ì…ë‹ˆë‹¤: **ê³¼ê±° ë¬¸ë§¥ë§Œìœ¼ë¡œ ë‹¤ìŒ í† í°(action)ì„ ì˜ˆì¸¡**í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ’¡ ì»´í¬ë„ŒíŠ¸ë³„ ìš”ì•½\n",
    "\n",
    "| êµ¬ì„± ìš”ì†Œ | ì—­í•  | í¬ê¸° ì˜ˆì‹œ |\n",
    "|:---|:---|:---|\n",
    "| `embed_state` | ìƒíƒœ ë²¡í„°ë¥¼ hidden ì°¨ì›ìœ¼ë¡œ ë³€í™˜ | 11 â†’ 128 |\n",
    "| `embed_action` | í–‰ë™ ë²¡í„°ë¥¼ hidden ì°¨ì›ìœ¼ë¡œ ë³€í™˜ | 3 â†’ 128 |\n",
    "| `embed_return` | RTG ìŠ¤ì¹¼ë¼ë¥¼ hidden ì°¨ì›ìœ¼ë¡œ ë³€í™˜ | 1 â†’ 128 |\n",
    "| `embed_timestep` | ì‹œê°„ ì •ë³´ ì¶”ê°€ (ì ˆëŒ€ ìœ„ì¹˜) | 1000 Ã— 128 |\n",
    "| `transformer` | ì‹œí€€ìŠ¤ íŒ¨í„´ í•™ìŠµ (GPT-2) | 3 layers |\n",
    "| `predict_action` | state í† í° â†’ action ì˜ˆì¸¡ | 128 â†’ 3 |\n",
    "\n",
    "> ğŸ“Œ **ì½”ë“œ ì°¸ì¡°**: `gym/decision_transformer/models/decision_transformer.py`ì˜  \n",
    "> `forward()` ë©”ì„œë“œ (line 55-99)ì—ì„œ ì´ íë¦„ì„ ì§ì ‘ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“¦ DecisionTransformer í´ë˜ìŠ¤ ì„í¬íŠ¸\n# ============================================================\n# gym/decision_transformer/models/decision_transformer.pyì— ì •ì˜ëœ\n# DecisionTransformer í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.\n#\n# ì„í¬íŠ¸ ì‹¤íŒ¨ ì‹œ SimpleDT(ìš°ë¦¬ê°€ ì§ì ‘ êµ¬í˜„í•œ ë²„ì „)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n# ============================================================\n\ntry:\n    from decision_transformer.models.decision_transformer import DecisionTransformer\n    print(\"âœ… DecisionTransformer í´ë˜ìŠ¤ ë¡œë“œ ì„±ê³µ!\")\n    print(\"   â””â”€ ê²½ë¡œ: gym/decision_transformer/models/decision_transformer.py\")\n    DT_AVAILABLE = True\nexcept ImportError as e:\n    print(f\"âš ï¸ DecisionTransformer ì„í¬íŠ¸ ì‹¤íŒ¨\")\n    print(f\"   â””â”€ ì˜¤ë¥˜: {e}\")\n    print()\n    print(\"ğŸ”§ ëŒ€ì•ˆ: SimpleDT (ì§ì ‘ êµ¬í˜„ ë²„ì „)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n    print(\"   â””â”€ í•µì‹¬ êµ¬ì¡°ëŠ” ë™ì¼í•˜ë©°, í•™ìŠµìš©ìœ¼ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤.\")\n    DT_AVAILABLE = False"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ—ï¸ ëª¨ë¸ ìƒì„±\n# ============================================================\n# DecisionTransformer ë˜ëŠ” SimpleDT ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n#\n# ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°:\n#   state_dim (11): Hopperì˜ ìƒíƒœ ì°¨ì›\n#   act_dim (3): Hopperì˜ í–‰ë™ ì°¨ì›\n#   max_length (K=20): ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (í•œ ë²ˆì— ë³´ëŠ” ê³¼ê±° ìŠ¤í… ìˆ˜)\n#   hidden_size (128): íŠ¸ëœìŠ¤í¬ë¨¸ì˜ hidden dimension\n#   n_layer (3): íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ìˆ˜\n#   n_head (1): Attention head ìˆ˜\n# ============================================================\n\nimport torch\nimport torch.nn as nn\n\n# Hopper í™˜ê²½ ì„¤ì •ê°’\nSTATE_DIM = 11   # Hopper state dimension\nACT_DIM = 3      # Hopper action dimension\nCONTEXT_LEN = 20 # K: í•œ ë²ˆì— ë³´ëŠ” ê³¼ê±° ìŠ¤í… ìˆ˜\n\nprint(\"ğŸ—ï¸ ëª¨ë¸ ìƒì„±\")\nprint(\"=\"*60)\n\nif DT_AVAILABLE:\n    # ============================\n    # ì‹¤ì œ DecisionTransformer ì‚¬ìš©\n    # ============================\n    model = DecisionTransformer(\n        state_dim=STATE_DIM,      # ìƒíƒœ ì°¨ì›\n        act_dim=ACT_DIM,          # í–‰ë™ ì°¨ì›\n        max_length=CONTEXT_LEN,   # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (K)\n        max_ep_len=1000,          # ìµœëŒ€ ì—í”¼ì†Œë“œ ê¸¸ì´\n        hidden_size=128,          # hidden dimension\n        n_layer=3,                # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ìˆ˜\n        n_head=1,                 # Attention head ìˆ˜\n        n_inner=128*4,            # FFN ë‚´ë¶€ ì°¨ì› (ë³´í†µ hidden*4)\n        activation_function='relu',\n        n_positions=1024,         # ìœ„ì¹˜ ì„ë² ë”© ìµœëŒ€ ê¸¸ì´\n        resid_pdrop=0.1,          # Residual dropout\n        attn_pdrop=0.1,           # Attention dropout\n    )\n    print(\"âœ… DecisionTransformer ëª¨ë¸ ìƒì„± ì™„ë£Œ\")\n    \nelse:\n    # ============================\n    # SimpleDT: ì§ì ‘ êµ¬í˜„í•œ ê²½ëŸ‰ ë²„ì „\n    # ============================\n    # í•µì‹¬ êµ¬ì¡°ëŠ” DecisionTransformerì™€ ë™ì¼í•©ë‹ˆë‹¤.\n    # PyTorchì˜ ê¸°ë³¸ TransformerEncoderë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n    \n    class SimpleEmbeddings(nn.Module):\n        \"\"\"\n        ì„ë² ë”© ë ˆì´ì–´: RTG, State, Actionì„ hidden spaceë¡œ ë³€í™˜\n        \n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚  RTG (1)    â”‚     â”‚ State (11)  â”‚     â”‚ Action (3)  â”‚\n        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n               â”‚ Linear            â”‚ Linear           â”‚ Linear\n               â–¼                   â–¼                   â–¼\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚   (128)     â”‚     â”‚   (128)     â”‚     â”‚   (128)     â”‚\n        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n               â”‚    + timestep embedding    â”‚\n               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                               â–¼\n                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                        â”‚  LayerNorm  â”‚\n                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        \"\"\"\n        def __init__(self, state_dim, action_dim, hidden_size, max_timestep=1000):\n            super().__init__()\n            # ê° ëª¨ë‹¬ë¦¬í‹°ë³„ ì„ë² ë”© ë ˆì´ì–´\n            self.embed_state = nn.Linear(state_dim, hidden_size)\n            self.embed_action = nn.Linear(action_dim, hidden_size)\n            self.embed_return = nn.Linear(1, hidden_size)\n            \n            # ì‹œê°„ ì •ë³´ë¥¼ ìœ„í•œ ì„ë² ë”© (ë£©ì—… í…Œì´ë¸”)\n            self.embed_timestep = nn.Embedding(max_timestep, hidden_size)\n            \n            # ì •ê·œí™” ë ˆì´ì–´\n            self.embed_ln = nn.LayerNorm(hidden_size)\n            \n        def forward(self, states, actions, returns_to_go, timesteps):\n            \"\"\"\n            ì…ë ¥ì„ ì„ë² ë”©í•˜ê³  ì‹œí€€ìŠ¤ë¡œ êµ¬ì„±\n            \n            Args:\n                states: (batch, seq_len, state_dim)\n                actions: (batch, seq_len, action_dim)  \n                returns_to_go: (batch, seq_len, 1)\n                timesteps: (batch, seq_len)\n                \n            Returns:\n                (batch, seq_len*3, hidden_size)\n            \"\"\"\n            batch_size, seq_len = states.shape[0], states.shape[1]\n            \n            # Step 1: ê° ëª¨ë‹¬ë¦¬í‹° ì„ë² ë”©\n            state_emb = self.embed_state(states)      # (B, K, H)\n            action_emb = self.embed_action(actions)   # (B, K, H)\n            return_emb = self.embed_return(returns_to_go)  # (B, K, H)\n            \n            # Step 2: ì‹œê°„ ì„ë² ë”© ì¶”ê°€\n            time_emb = self.embed_timestep(timesteps)  # (B, K, H)\n            state_emb = state_emb + time_emb\n            action_emb = action_emb + time_emb\n            return_emb = return_emb + time_emb\n            \n            # Step 3: (RTG, s, a) ìˆœì„œë¡œ interleave\n            # [R_0, s_0, a_0, R_1, s_1, a_1, ...]\n            stacked = torch.stack([return_emb, state_emb, action_emb], dim=2)\n            token_embeddings = stacked.reshape(batch_size, seq_len * 3, -1)\n            \n            # Step 4: LayerNorm ì ìš©\n            return self.embed_ln(token_embeddings)\n\n    class SimpleDT(nn.Module):\n        \"\"\"\n        ê°„ì†Œí™”ëœ Decision Transformer\n        \n        PyTorchì˜ TransformerEncoderë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„\n        \"\"\"\n        def __init__(self, state_dim, action_dim, hidden_size=128, n_layer=3, n_head=1):\n            super().__init__()\n            self.hidden_size = hidden_size\n            \n            # ì„ë² ë”© ë ˆì´ì–´\n            self.embeddings = SimpleEmbeddings(state_dim, action_dim, hidden_size)\n            \n            # íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”\n            encoder_layer = nn.TransformerEncoderLayer(\n                d_model=hidden_size, \n                nhead=n_head,\n                dim_feedforward=hidden_size * 4,  # FFN ì°¨ì›\n                dropout=0.1, \n                batch_first=True  # (batch, seq, feature) ìˆœì„œ ì‚¬ìš©\n            )\n            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layer)\n            \n            # Action ì˜ˆì¸¡ í—¤ë“œ\n            # state í† í°ì˜ ì¶œë ¥ì—ì„œ actionì„ ì˜ˆì¸¡\n            self.predict_action = nn.Sequential(\n                nn.Linear(hidden_size, action_dim),\n                nn.Tanh()  # Actionì„ [-1, 1] ë²”ìœ„ë¡œ ì œí•œ\n            )\n            \n        def forward(self, states, actions, returns_to_go, timesteps, attention_mask=None):\n            \"\"\"\n            Forward pass\n            \n            Returns:\n                (state_preds, action_preds, return_preds)\n                - action_predsë§Œ ì‚¬ìš© (ë‚˜ë¨¸ì§€ëŠ” None)\n            \"\"\"\n            batch_size, seq_len = states.shape[0], states.shape[1]\n            \n            # Step 1: ì„ë² ë”©\n            token_embeddings = self.embeddings(states, actions, returns_to_go, timesteps)\n            # Shape: (batch, seq_len*3, hidden_size)\n            \n            # Step 2: Causal mask ìƒì„± (ë¯¸ë˜ ì •ë³´ ì°¨ë‹¨)\n            seq_len_3 = seq_len * 3\n            causal_mask = torch.triu(\n                torch.ones(seq_len_3, seq_len_3) * float('-inf'), \n                diagonal=1\n            ).to(states.device)\n            \n            # Step 3: íŠ¸ëœìŠ¤í¬ë¨¸ í†µê³¼\n            hidden_states = self.transformer(token_embeddings, mask=causal_mask)\n            \n            # Step 4: Action ì˜ˆì¸¡ (state í† í° ìœ„ì¹˜ì—ì„œ)\n            # í† í° ìˆœì„œ: [R_0, s_0, a_0, R_1, s_1, a_1, ...]\n            # state ìœ„ì¹˜: 1, 4, 7, ... (1::3)\n            action_hidden = hidden_states[:, 1::3, :]  # state í† í°ë§Œ ì„ íƒ\n            action_preds = self.predict_action(action_hidden)\n            \n            return None, action_preds, None\n    \n    model = SimpleDT(state_dim=STATE_DIM, action_dim=ACT_DIM)\n    print(\"âœ… SimpleDT ëª¨ë¸ ìƒì„± ì™„ë£Œ\")\n\n# ëª¨ë¸ ì •ë³´ ì¶œë ¥\nprint(f\"\\nğŸ“ ëª¨ë¸ ì„¤ì •:\")\nprint(f\"   State dimension:  {STATE_DIM}\")\nprint(f\"   Action dimension: {ACT_DIM}\")\nprint(f\"   Context length:   {CONTEXT_LEN}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“Š ëª¨ë¸ êµ¬ì¡° ë° íŒŒë¼ë¯¸í„° ë¶„ì„\n# ============================================================\n# ëª¨ë¸ì˜ êµ¬ì¡°ì™€ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n# ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n# ============================================================\n\nprint(\"ğŸ“Š ëª¨ë¸ êµ¬ì¡° ë¶„ì„\")\nprint(\"=\"*60)\n\n# ëª¨ë¸ êµ¬ì¡° ì¶œë ¥\nprint(\"\\nğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜:\")\nprint(\"-\"*60)\nprint(model)\nprint(\"-\"*60)\n\n# íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"\\nğŸ“ˆ íŒŒë¼ë¯¸í„° í†µê³„:\")\nprint(f\"   ì´ íŒŒë¼ë¯¸í„° ìˆ˜:     {total_params:>12,}\")\nprint(f\"   í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:>12,}\")\n\n# ë ˆì´ì–´ë³„ íŒŒë¼ë¯¸í„° ë¶„ì„\nprint(f\"\\nğŸ“‹ ë ˆì´ì–´ë³„ íŒŒë¼ë¯¸í„° ìˆ˜:\")\nprint(f\"{'ë ˆì´ì–´':<40} {'íŒŒë¼ë¯¸í„°':>15}\")\nprint(\"-\"*60)\n\nfor name, param in model.named_parameters():\n    # ì´ë¦„ ì¶•ì•½\n    short_name = name if len(name) <= 38 else \"...\" + name[-35:]\n    print(f\"{short_name:<40} {param.numel():>15,}\")\n\n# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •\nparam_memory = total_params * 4 / (1024 * 1024)  # float32 = 4 bytes\nprint(f\"\\nğŸ’¾ ì¶”ì • ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:\")\nprint(f\"   íŒŒë¼ë¯¸í„°: ~{param_memory:.2f} MB\")\nprint(f\"   (ì‹¤ì œ í•™ìŠµ ì‹œ gradient ë“±ìœ¼ë¡œ ì•½ 3-4ë°° í•„ìš”)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ”„ Forward Pass í…ŒìŠ¤íŠ¸\n# ============================================================\n# ëª¨ë¸ì— ë”ë¯¸ ì…ë ¥ì„ ë„£ì–´ ì¶œë ¥ shapeì„ í™•ì¸í•©ë‹ˆë‹¤.\n# ì´ë¥¼ í†µí•´ ë°ì´í„° íë¦„ì„ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n#\n# ì…ë ¥ â†’ ì¶œë ¥ íë¦„:\n#   (states, actions, rtg, timesteps)\n#       â†“\n#   DecisionTransformer\n#       â†“\n#   (state_preds, action_preds, return_preds)\n#       â””â”€ action_predsë§Œ ì‚¬ìš©\n# ============================================================\n\nbatch_size = 4\nseq_len = CONTEXT_LEN  # K = 20\n\nprint(\"ğŸ”„ Forward Pass í…ŒìŠ¤íŠ¸\")\nprint(\"=\"*60)\n\n# ë”ë¯¸ ì…ë ¥ ìƒì„±\nstates = torch.randn(batch_size, seq_len, STATE_DIM)     # (B, K, 11)\nactions = torch.randn(batch_size, seq_len, ACT_DIM)      # (B, K, 3)\nreturns_to_go = torch.randn(batch_size, seq_len, 1)      # (B, K, 1)\ntimesteps = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)  # (B, K)\nattention_mask = torch.ones(batch_size, seq_len)         # (B, K)\n\nprint(\"\\nğŸ“¥ ì…ë ¥ Shapes:\")\nprint(f\"   states:        {states.shape}\")\nprint(f\"                  â””â”€ (batch, context_len, state_dim)\")\nprint(f\"   actions:       {actions.shape}\")\nprint(f\"                  â””â”€ (batch, context_len, action_dim)\")\nprint(f\"   returns_to_go: {returns_to_go.shape}\")\nprint(f\"                  â””â”€ (batch, context_len, 1)\")\nprint(f\"   timesteps:     {timesteps.shape}\")\nprint(f\"                  â””â”€ (batch, context_len)\")\n\n# Forward pass\nmodel.eval()\nwith torch.no_grad():\n    if DT_AVAILABLE:\n        # ì‹¤ì œ DecisionTransformer API\n        state_preds, action_preds, return_preds = model(\n            states, actions, None, returns_to_go, timesteps, attention_mask\n        )\n    else:\n        # SimpleDT API\n        state_preds, action_preds, return_preds = model(\n            states, actions, returns_to_go, timesteps\n        )\n\nprint(f\"\\nğŸ“¤ ì¶œë ¥ Shapes:\")\nprint(f\"   action_preds:  {action_preds.shape}\")\nprint(f\"                  â””â”€ (batch, context_len, action_dim)\")\n\n# ë§ˆì§€ë§‰ timestepì˜ action ì˜ˆì¸¡ (ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ê°’)\nprint(f\"\\nğŸ¯ ë§ˆì§€ë§‰ timestepì˜ action ì˜ˆì¸¡:\")\nprint(f\"   action_preds[0, -1, :] = {action_preds[0, -1, :]}\")\nprint(f\"   â””â”€ ì²« ë²ˆì§¸ ë°°ì¹˜, ë§ˆì§€ë§‰ timestepì˜ ì˜ˆì¸¡ action\")\nprint()\nprint(\"ğŸ’¡ ì¶”ë¡  ì‹œì—ëŠ” ë§ˆì§€ë§‰ timestepì˜ actionë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤!\")\nprint(\"   (ê°€ì¥ ìµœì‹  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì˜ˆì¸¡ì´ê¸° ë•Œë¬¸)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. í•™ìŠµ ì‹¤í–‰\n",
    "\n",
    "## ğŸ¯ ëª©í‘œ\n",
    "> Decision Transformer í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“Š í•™ìŠµ íŒŒì´í”„ë¼ì¸ ê°œìš”\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Pipeline[\"ğŸ“Š í•™ìŠµ íŒŒì´í”„ë¼ì¸\"]\n",
    "        direction TB\n",
    "        subgraph Step1[\"1ï¸âƒ£ ë°°ì¹˜ ìƒì„± get_batch\"]\n",
    "            B1[\"trajectoriesì—ì„œ\\në¬´ì‘ìœ„ë¡œ K ê¸¸ì´ì˜\\nì„œë¸Œì‹œí€€ìŠ¤ ìƒ˜í”Œë§\"]\n",
    "            B2[\"â†’ states, actions,\\n  rtg, timesteps, mask\"]\n",
    "            B1 --> B2\n",
    "        end\n",
    "\n",
    "        subgraph Step2[\"2ï¸âƒ£ Forward Pass\"]\n",
    "            F1[\"ëª¨ë¸ì— ì…ë ¥\"]\n",
    "            F2[\"action_preds ì¶œë ¥\"]\n",
    "            F1 --> F2\n",
    "        end\n",
    "\n",
    "        subgraph Step3[\"3ï¸âƒ£ ì†ì‹¤ ê³„ì‚°\"]\n",
    "            L1[\"MSE Loss\"]\n",
    "            L2[\"= mean(action_preds - action_target)Â²\"]\n",
    "            L1 --> L2\n",
    "        end\n",
    "\n",
    "        subgraph Step4[\"4ï¸âƒ£ ì—­ì „íŒŒ & ìµœì í™”\"]\n",
    "            O1[\"loss.backward()\"]\n",
    "            O2[\"gradient clipping (0.25)\"]\n",
    "            O3[\"optimizer.step()\"]\n",
    "            O1 --> O2 --> O3\n",
    "        end\n",
    "\n",
    "        Step1 --> Step2 --> Step3 --> Step4\n",
    "        Step4 -.->|\"ë°˜ë³µ\"| Step1\n",
    "    end\n",
    "\n",
    "    style Step1 fill:#e3f2fd\n",
    "    style Step2 fill:#fff3e0\n",
    "    style Step3 fill:#ffebee\n",
    "    style Step4 fill:#e8f5e9\n",
    "```\n",
    "\n",
    "## ğŸ”‘ ë°°ì¹˜ ìƒì„± ê³¼ì • ìƒì„¸\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Sampling[\"ğŸ“¦ ë°°ì¹˜ ìƒì„± ìƒì„¸\"]\n",
    "        direction TB\n",
    "        S1[\"1. ê¶¤ì  ì„ íƒ\\nê¸´ ê¶¤ì ì¼ìˆ˜ë¡ ìì£¼ ì„ íƒ\"] --> S2[\"2. ì‹œì‘ì  ë¬´ì‘ìœ„ ì„ íƒ\\nsi = random(0, T)\"]\n",
    "        S2 --> S3[\"3. K ê¸¸ì´ ì„œë¸Œì‹œí€€ìŠ¤ ì¶”ì¶œ\\ntraj[si : si+K]\"]\n",
    "        S3 --> S4[\"4. RTG ê³„ì‚°\\nsië¶€í„° ì—í”¼ì†Œë“œ ëê¹Œì§€\"]\n",
    "        S4 --> S5[\"5. íŒ¨ë”© (ì•ìª½ì— 0 ì¶”ê°€)\\nKë³´ë‹¤ ì§§ìœ¼ë©´ ì•ì— íŒ¨ë”©\"]\n",
    "        S5 --> S6[\"6. Tensorë¡œ ë³€í™˜\"]\n",
    "    end\n",
    "\n",
    "    style Sampling fill:#e3f2fd\n",
    "```\n",
    "\n",
    "## ğŸ¯ íŒ¨ë”© ì‹œê°í™”\n",
    "\n",
    "ì„œë¸Œì‹œí€€ìŠ¤ ê¸¸ì´ê°€ Kë³´ë‹¤ ì§§ì€ ê²½ìš°, **ì•ìª½(left)**ì— 0ì„ ì±„ì›ë‹ˆë‹¤:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Before[\"ì›ë³¸ (ê¸¸ì´ 3)\"]\n",
    "        direction LR\n",
    "        B1[\"sâ‚ƒ\"] --> B2[\"sâ‚„\"] --> B3[\"sâ‚…\"]\n",
    "    end\n",
    "\n",
    "    subgraph After[\"íŒ¨ë”© í›„ (ê¸¸ì´ K=20)\"]\n",
    "        direction LR\n",
    "        P1[\"0\"] --> P2[\"0\"] --> P3[\"...\"] --> P4[\"0\"]\n",
    "        P4 --> A1[\"sâ‚ƒ\"] --> A2[\"sâ‚„\"] --> A3[\"sâ‚…\"]\n",
    "    end\n",
    "\n",
    "    Before -->|\"ì•ìª½ íŒ¨ë”©\"| After\n",
    "\n",
    "    style P1 fill:#eeeeee\n",
    "    style P2 fill:#eeeeee\n",
    "    style P3 fill:#eeeeee\n",
    "    style P4 fill:#eeeeee\n",
    "    style A1 fill:#c8e6c9\n",
    "    style A2 fill:#c8e6c9\n",
    "    style A3 fill:#c8e6c9\n",
    "```\n",
    "\n",
    "> ğŸ’¡ **ì™œ ì•ìª½ì— íŒ¨ë”©í• ê¹Œìš”?**  \n",
    "> Causal attentionì—ì„œ ê°€ì¥ ìµœê·¼(ì˜¤ë¥¸ìª½) í† í°ì´ ì˜ˆì¸¡ì— ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤.  \n",
    "> ì•ìª½ íŒ¨ë”©ì€ attention maskë¡œ ë¬´ì‹œë˜ë¯€ë¡œ ëª¨ë¸ ì„±ëŠ¥ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "\n",
    "## âš¡ DT í•™ìŠµ = Supervised Learning!\n",
    "\n",
    "Decision Transformerì˜ í•™ìŠµì€ ê¸°ì¡´ RLê³¼ ì™„ì „íˆ ë‹¤ë¦…ë‹ˆë‹¤:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph TraditionalRL[\"ğŸ”„ ê¸°ì¡´ RL í•™ìŠµ\"]\n",
    "        direction TB\n",
    "        TR1[\"í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©\"]\n",
    "        TR2[\"ë³´ìƒ ìµœëŒ€í™”\\n(Bellman ë°©ì •ì‹)\"]\n",
    "        TR3[\"Online í•™ìŠµ\"]\n",
    "    end\n",
    "\n",
    "    subgraph DTSL[\"ğŸ¤– DT í•™ìŠµ = Supervised Learning\"]\n",
    "        direction TB\n",
    "        DT1[\"ì˜¤í”„ë¼ì¸ ë°ì´í„°ì…‹\\n(ë¯¸ë¦¬ ìˆ˜ì§‘ëœ ê¶¤ì )\"]\n",
    "        DT2[\"Action ì˜ˆì¸¡\\n(MSE ì†ì‹¤)\"]\n",
    "        DT3[\"Offline í•™ìŠµ\\n(í™˜ê²½ ë¶ˆí•„ìš”)\"]\n",
    "    end\n",
    "\n",
    "    style TraditionalRL fill:#ffebee\n",
    "    style DTSL fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px\n",
    "```\n",
    "\n",
    "## ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸\n",
    "\n",
    "| ë‹¨ê³„ | ë‚´ìš© | ë¹„ê³  |\n",
    "|:---|:---|:---|\n",
    "| **ë°°ì¹˜ ìƒì„±** | ê¸´ ê¶¤ì ì—ì„œ K ê¸¸ì´ ì„œë¸Œì‹œí€€ìŠ¤ ì¶”ì¶œ | íŒ¨ë”© í•„ìš” ì‹œ ì•ì— ì¶”ê°€ |\n",
    "| **ì†ì‹¤ í•¨ìˆ˜** | MSE (Mean Squared Error) | ì—°ì† action ì˜ˆì¸¡ì— ì í•© |\n",
    "| **ì •ê·œí™”** | RTGë¥¼ scaleë¡œ ë‚˜ëˆ” (ì˜ˆ: 1000) | í•™ìŠµ ì•ˆì •í™” |\n",
    "| **Gradient Clipping** | max_norm = 0.25 | í•™ìŠµ ì•ˆì •í™” |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“¦ ë°°ì¹˜ ìƒì„± í•¨ìˆ˜ (get_batch)\n# ============================================================\n# experiment.pyì˜ get_batch í•¨ìˆ˜ë¥¼ ê°„ì†Œí™”í•œ ë²„ì „ì…ë‹ˆë‹¤.\n#\n# ë°°ì¹˜ ìƒì„± ê³¼ì •:\n#   1. ê¶¤ì  ì„ íƒ (ê¸´ ê¶¤ì ì´ ë” ìì£¼ ì„ íƒë¨)\n#   2. ì‹œì‘ì  ë¬´ì‘ìœ„ ì„ íƒ\n#   3. K ê¸¸ì´ì˜ ì„œë¸Œì‹œí€€ìŠ¤ ì¶”ì¶œ\n#   4. RTG ê³„ì‚° (í•´ë‹¹ ì‹œì ë¶€í„° ëê¹Œì§€)\n#   5. íŒ¨ë”© (ì•ìª½ì— 0 ì¶”ê°€)\n#   6. Tensorë¡œ ë³€í™˜\n#\n# íŒ¨ë”© ì‹œê°í™”:\n#   ì›ë³¸: [s3, s4, s5]  (ê¸¸ì´ 3)\n#   íŒ¨ë”©: [0, 0, 0, ..., 0, s3, s4, s5]  (ê¸¸ì´ K)\n#         â”œâ”€ padding â”€â”¤  â”œâ”€ ì‹¤ì œ ë°ì´í„° â”€â”¤\n# ============================================================\n\ndef get_batch(trajectories, batch_size=64, max_len=20, state_dim=11, act_dim=3, scale=1000):\n    \"\"\"\n    í•™ìŠµìš© ë°°ì¹˜ ë°ì´í„° ìƒì„±\n    \n    Parameters:\n    -----------\n    trajectories : list\n        ê¶¤ì  ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n    batch_size : int\n        ë°°ì¹˜ í¬ê¸°\n    max_len : int\n        ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (K)\n    state_dim, act_dim : int\n        ìƒíƒœ/í–‰ë™ ì°¨ì›\n    scale : float\n        RTG ì •ê·œí™” ìŠ¤ì¼€ì¼ (ë³´í†µ í™˜ê²½ì˜ ìµœëŒ€ return)\n        \n    Returns:\n    --------\n    tuple: (states, actions, returns_to_go, timesteps, mask)\n    \"\"\"\n    \n    # Step 1: ê¶¤ì  ì„ íƒ (ê¸´ ê¶¤ì ì¼ìˆ˜ë¡ ë” ìì£¼ ì„ íƒ)\n    # ì´ìœ : ê¸´ ê¶¤ì ì€ ë” ë§ì€ transitionì„ í¬í•¨í•˜ë¯€ë¡œ ê³µì •í•œ ìƒ˜í”Œë§ì„ ìœ„í•¨\n    traj_lengths = [len(t['rewards']) for t in trajectories]\n    total_transitions = sum(traj_lengths)\n    probabilities = [l / total_transitions for l in traj_lengths]\n    \n    batch_inds = np.random.choice(\n        len(trajectories),\n        size=batch_size,\n        replace=True,  # ë³µì› ì¶”ì¶œ (ê°™ì€ ê¶¤ì ì„ ì—¬ëŸ¬ ë²ˆ ì„ íƒ ê°€ëŠ¥)\n        p=probabilities\n    )\n    \n    # ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸\n    states_list, actions_list, rtg_list, timesteps_list, mask_list = [], [], [], [], []\n    \n    for i in batch_inds:\n        traj = trajectories[i]\n        traj_len = len(traj['rewards'])\n        \n        # Step 2: ì‹œì‘ì  ë¬´ì‘ìœ„ ì„ íƒ\n        si = np.random.randint(0, traj_len)\n        \n        # Step 3: ì„œë¸Œì‹œí€€ìŠ¤ ì¶”ì¶œ (sië¶€í„° ìµœëŒ€ max_len ê¸¸ì´)\n        s = traj['observations'][si:si + max_len]\n        a = traj['actions'][si:si + max_len]\n        r = traj['rewards'][si:si + max_len]\n        \n        # Step 4: RTG ê³„ì‚° (sië¶€í„° ì—í”¼ì†Œë“œ ëê¹Œì§€)\n        # ì£¼ì˜: si ì´ì „ì˜ rewardëŠ” ì´ë¯¸ ë°›ì€ ê²ƒì´ë¯€ë¡œ ì œì™¸\n        full_rtg = discount_cumsum(traj['rewards'][si:], gamma=1.0)\n        rtg = full_rtg[:max_len]\n        \n        # Timesteps (ì ˆëŒ€ ì‹œê°„)\n        t = np.arange(si, min(si + max_len, traj_len))\n        \n        # Step 5: ì‹¤ì œ ì¶”ì¶œëœ ê¸¸ì´\n        actual_len = len(s)\n        \n        # Step 6: ì•ìª½ íŒ¨ë”© (max_lenë³´ë‹¤ ì§§ì€ ê²½ìš°)\n        padding_len = max_len - actual_len\n        \n        s = np.concatenate([np.zeros((padding_len, state_dim)), s], axis=0)\n        a = np.concatenate([np.zeros((padding_len, act_dim)), a], axis=0)\n        rtg = np.concatenate([np.zeros((padding_len,)), rtg], axis=0)\n        t = np.concatenate([np.zeros((padding_len,)), t], axis=0)\n        \n        # Attention mask: íŒ¨ë”©=0, ì‹¤ì œ ë°ì´í„°=1\n        m = np.concatenate([np.zeros((padding_len,)), np.ones((actual_len,))], axis=0)\n        \n        states_list.append(s)\n        actions_list.append(a)\n        rtg_list.append(rtg)\n        timesteps_list.append(t)\n        mask_list.append(m)\n    \n    # Step 7: Numpy â†’ Tensor ë³€í™˜\n    states = torch.tensor(np.array(states_list), dtype=torch.float32)\n    actions = torch.tensor(np.array(actions_list), dtype=torch.float32)\n    returns_to_go = torch.tensor(np.array(rtg_list), dtype=torch.float32).unsqueeze(-1) / scale\n    timesteps = torch.tensor(np.array(timesteps_list), dtype=torch.long)\n    mask = torch.tensor(np.array(mask_list), dtype=torch.float32)\n    \n    return states, actions, returns_to_go, timesteps, mask\n\n# ============================================================\n# ğŸ§ª ë°°ì¹˜ ìƒì„± í…ŒìŠ¤íŠ¸\n# ============================================================\nprint(\"ğŸ§ª ë°°ì¹˜ ìƒì„± í…ŒìŠ¤íŠ¸\")\nprint(\"=\"*60)\n\nbatch = get_batch(trajectories, batch_size=4, max_len=CONTEXT_LEN)\nstates, actions, rtg, timesteps, mask = batch\n\nprint(\"\\nğŸ“¦ ìƒì„±ëœ ë°°ì¹˜ Shapes:\")\nprint(f\"   states:        {states.shape}\")\nprint(f\"   actions:       {actions.shape}\")\nprint(f\"   returns_to_go: {rtg.shape}\")\nprint(f\"   timesteps:     {timesteps.shape}\")\nprint(f\"   mask:          {mask.shape}\")\n\nprint(f\"\\nğŸ“Š ì²« ë²ˆì§¸ ìƒ˜í”Œì˜ mask:\")\nprint(f\"   {mask[0].numpy()}\")\nprint(f\"   â””â”€ 0: íŒ¨ë”©, 1: ì‹¤ì œ ë°ì´í„°\")\nprint(f\"   ì‹¤ì œ ë°ì´í„° ê¸¸ì´: {int(mask[0].sum())}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸš€ í•™ìŠµ ë£¨í”„ êµ¬í˜„ ë° ì‹¤í–‰\n# ============================================================\n# Decision Transformer í•™ìŠµì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n#\n# í•™ìŠµ ëª©í‘œ:\n#   ì£¼ì–´ì§„ (RTG, State) ì¡°ê±´ì—ì„œ ì˜¬ë°”ë¥¸ Actionì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµ\n#\n# ì†ì‹¤ í•¨ìˆ˜:\n#   L = MSE(action_preds, action_target)\n#     = (1/N) Î£ (ì˜ˆì¸¡ action - ì‹¤ì œ action)Â²\n#\n# ì´ê²ƒì€ Supervised Learningì…ë‹ˆë‹¤!\n#   - ì…ë ¥: (RTG, State, ê³¼ê±° Actions)\n#   - íƒ€ê²Ÿ: ì‹¤ì œ ìˆ˜í–‰ëœ Action\n#   - Q-Learningì²˜ëŸ¼ ë³´ìƒì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ ì•„ë‹˜\n# ============================================================\n\nimport torch.optim as optim\nfrom tqdm import tqdm\n\nprint(\"ğŸš€ í•™ìŠµ ì„¤ì •\")\nprint(\"=\"*60)\n\n# ë””ë°”ì´ìŠ¤ ì„¤ì •\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"   Device: {device}\")\n\n# ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\nmodel = model.to(device)\n\n# ì˜µí‹°ë§ˆì´ì € ì„¤ì •\noptimizer = optim.AdamW(\n    model.parameters(),\n    lr=1e-4,           # í•™ìŠµë¥ \n    weight_decay=1e-4  # L2 ì •ê·œí™”\n)\n\nprint(f\"   Optimizer: AdamW\")\nprint(f\"   Learning rate: 1e-4\")\nprint(f\"   Weight decay: 1e-4\")\n\n# ============================================================\n# í•™ìŠµ ìŠ¤í… í•¨ìˆ˜\n# ============================================================\ndef train_step(model, optimizer, batch, device):\n    \"\"\"\n    ë‹¨ì¼ í•™ìŠµ ìŠ¤í… ìˆ˜í–‰\n    \n    Returns:\n        float: ì†ì‹¤ ê°’\n    \"\"\"\n    # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n    states, actions, rtg, timesteps, mask = [b.to(device) for b in batch]\n    \n    # Forward pass\n    if DT_AVAILABLE:\n        state_preds, action_preds, return_preds = model(\n            states, actions, None, rtg, timesteps, mask\n        )\n    else:\n        state_preds, action_preds, return_preds = model(\n            states, actions, rtg, timesteps\n        )\n    \n    # ì†ì‹¤ ê³„ì‚° (MSE)\n    # action_preds: ëª¨ë¸ì´ ì˜ˆì¸¡í•œ action\n    # actions: ì‹¤ì œ ë°ì´í„°ì˜ action (íƒ€ê²Ÿ)\n    action_target = actions\n    \n    # MSE ì†ì‹¤ = í‰ê· ((ì˜ˆì¸¡ - íƒ€ê²Ÿ)Â²)\n    loss = torch.mean((action_preds - action_target) ** 2)\n    \n    # ì—­ì „íŒŒ\n    optimizer.zero_grad()  # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”\n    loss.backward()        # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°\n    \n    # Gradient clipping (í•™ìŠµ ì•ˆì •í™”)\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.25)\n    \n    # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n    optimizer.step()\n    \n    return loss.item()\n\n# ============================================================\n# í•™ìŠµ ì‹¤í–‰\n# ============================================================\nprint(f\"\\nğŸƒ í•™ìŠµ ì‹œì‘!\")\nprint(\"-\"*60)\n\nnum_steps = 100  # í•™ìŠµ ìŠ¤í… ìˆ˜ (ë°ëª¨ìš©ìœ¼ë¡œ ì ê²Œ ì„¤ì •)\nbatch_size = 32\nlosses = []\n\nmodel.train()  # í•™ìŠµ ëª¨ë“œ\n\nfor step in tqdm(range(num_steps), desc=\"Training\"):\n    # ë°°ì¹˜ ìƒì„±\n    batch = get_batch(trajectories, batch_size=batch_size, max_len=CONTEXT_LEN)\n    \n    # í•™ìŠµ ìŠ¤í…\n    loss = train_step(model, optimizer, batch, device)\n    losses.append(loss)\n    \n    # ì§„í–‰ ìƒí™© ì¶œë ¥ (20 ìŠ¤í…ë§ˆë‹¤)\n    if (step + 1) % 20 == 0:\n        avg_loss = np.mean(losses[-20:])\n        print(f\"   Step {step+1:>4}: Loss = {loss:.4f} (avg: {avg_loss:.4f})\")\n\n# ê²°ê³¼ ìš”ì•½\nprint(\"-\"*60)\nprint(f\"\\nğŸ“Š í•™ìŠµ ê²°ê³¼:\")\nprint(f\"   ì´ˆê¸° Loss: {losses[0]:.4f}\")\nprint(f\"   ìµœì¢… Loss: {losses[-1]:.4f}\")\nprint(f\"   ê°œì„ ìœ¨: {(losses[0] - losses[-1]) / losses[0] * 100:.1f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“ˆ í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n# ============================================================\n# ì†ì‹¤(Loss)ì´ í•™ìŠµ ê³¼ì •ì—ì„œ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ì‹œê°í™”í•©ë‹ˆë‹¤.\n# ì¢‹ì€ í•™ìŠµì˜ íŠ¹ì§•:\n#   - ì†ì‹¤ì´ ì ì°¨ ê°ì†Œ\n#   - ë„ˆë¬´ ê¸‰ê²©í•œ ë³€ë™ ì—†ìŒ\n#   - ìˆ˜ë ´ ì¶”ì„¸ ë³´ì„\n# ============================================================\n\nprint(\"ğŸ“ˆ í•™ìŠµ ê³¡ì„  ì‹œê°í™”\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 1. ì „ì²´ í•™ìŠµ ê³¡ì„ \nax1 = axes[0]\nax1.plot(losses, alpha=0.7, linewidth=1, label='Loss per step')\n\n# ì´ë™ í‰ê·  (smoothing)\nwindow = 10\nif len(losses) >= window:\n    moving_avg = np.convolve(losses, np.ones(window)/window, mode='valid')\n    ax1.plot(range(window-1, len(losses)), moving_avg, \n             color='red', linewidth=2, label=f'Moving avg (window={window})')\n\nax1.set_xlabel('Training Step', fontsize=12)\nax1.set_ylabel('Loss (MSE)', fontsize=12)\nax1.set_title('Training Loss Curve', fontsize=14)\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 2. ì†ì‹¤ ë¶„í¬ (íˆìŠ¤í† ê·¸ë¨)\nax2 = axes[1]\n\n# ì²˜ìŒ 20%ì™€ ë§ˆì§€ë§‰ 20% ë¹„êµ\nfirst_portion = losses[:len(losses)//5]\nlast_portion = losses[-len(losses)//5:]\n\nax2.hist(first_portion, bins=20, alpha=0.6, label='First 20%', color='red')\nax2.hist(last_portion, bins=20, alpha=0.6, label='Last 20%', color='green')\nax2.set_xlabel('Loss Value', fontsize=12)\nax2.set_ylabel('Frequency', fontsize=12)\nax2.set_title('Loss Distribution: Beginning vs End', fontsize=14)\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# í†µê³„ ì¶œë ¥\nprint(f\"\\nğŸ“Š ì†ì‹¤ í†µê³„:\")\nprint(f\"   ì²˜ìŒ 20% í‰ê· : {np.mean(first_portion):.4f}\")\nprint(f\"   ë§ˆì§€ë§‰ 20% í‰ê· : {np.mean(last_portion):.4f}\")\nprint(f\"   ê°ì†ŒëŸ‰: {np.mean(first_portion) - np.mean(last_portion):.4f}\")\n\nif np.mean(last_portion) < np.mean(first_portion):\n    print(\"\\nâœ… ì†ì‹¤ì´ ê°ì†Œí–ˆìŠµë‹ˆë‹¤! ëª¨ë¸ì´ í•™ìŠµí•˜ê³  ìˆìŠµë‹ˆë‹¤.\")\nelse:\n    print(\"\\nâš ï¸ ì†ì‹¤ì´ ê°ì†Œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. í‰ê°€ ê³¼ì • ì´í•´\n",
    "\n",
    "## ğŸ¯ ëª©í‘œ\n",
    "> í•™ìŠµëœ Decision Transformerë¡œ **ì¶”ë¡ (Inference)**í•˜ëŠ” ë°©ë²•ì„ ì´í•´í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ”„ ì¶”ë¡  ê³¼ì • ê°œìš”\n",
    "\n",
    "Decision Transformerì˜ ì¶”ë¡ ì€ **ìê¸° íšŒê·€(Autoregressive)** ë°©ì‹ì…ë‹ˆë‹¤:\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Inference[\"ğŸ”„ ì¶”ë¡  ë£¨í”„\"]\n",
    "        direction TB\n",
    "        subgraph Init[\"Step 1: ì´ˆê¸° ì„¤ì •\"]\n",
    "            I1[\"ëª©í‘œ RTG ì„¤ì • (ì˜ˆ: 1800)\"]\n",
    "            I2[\"ì´ˆê¸° state ê´€ì¸¡ (env.reset)\"]\n",
    "            I3[\"íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”\"]\n",
    "            I1 --> I2 --> I3\n",
    "        end\n",
    "\n",
    "        subgraph Predict[\"Step 2: Action ì˜ˆì¸¡\"]\n",
    "            P1[\"model(states, actions, rtg, timesteps)\"]\n",
    "            P2[\"ë§ˆì§€ë§‰ timestepì˜\\naction_pred ì‚¬ìš©\"]\n",
    "            P1 --> P2\n",
    "        end\n",
    "\n",
    "        subgraph Execute[\"Step 3: í™˜ê²½ì—ì„œ ì‹¤í–‰\"]\n",
    "            E1[\"next_state, reward = env.step(action)\"]\n",
    "        end\n",
    "\n",
    "        subgraph Update[\"Step 4: RTG ì—…ë°ì´íŠ¸\"]\n",
    "            U1[\"new_rtg = current_rtg - reward\"]\n",
    "            U2[\"ë°›ì€ rewardë§Œí¼\\në‚¨ì€ ëª©í‘œì—ì„œ ì°¨ê°\"]\n",
    "            U1 --> U2\n",
    "        end\n",
    "\n",
    "        subgraph History[\"Step 5: íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸\"]\n",
    "            H1[\"states.append(next_state)\"]\n",
    "            H2[\"actions.append(action)\"]\n",
    "            H3[\"rtg.append(new_rtg)\"]\n",
    "        end\n",
    "\n",
    "        Init --> Predict --> Execute --> Update --> History\n",
    "        History -.->|\"ì—í”¼ì†Œë“œ ì¢…ë£Œê¹Œì§€ ë°˜ë³µ\"| Predict\n",
    "    end\n",
    "\n",
    "    style Init fill:#e3f2fd\n",
    "    style Predict fill:#fff3e0\n",
    "    style Execute fill:#e8f5e9\n",
    "    style Update fill:#fce4ec\n",
    "    style History fill:#f3e5f5\n",
    "```\n",
    "\n",
    "## ğŸ’¡ í•µì‹¬: RTG ì—…ë°ì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜\n",
    "\n",
    "RTGëŠ” \"ì•„ì§ ì–»ì–´ì•¼ í•  ë‚¨ì€ ë³´ìƒ\"ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.  \n",
    "ë§¤ ìŠ¤í… rewardë¥¼ ë°›ìœ¼ë©´ ê·¸ë§Œí¼ RTGì—ì„œ ì°¨ê°ë©ë‹ˆë‹¤:\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph RTG_Update[\"ğŸ“‰ RTG ì—…ë°ì´íŠ¸ ê³¼ì •\"]\n",
    "        direction TB\n",
    "        R0[\"ğŸ¯ ì‹œì‘: RTG = 1800\\nëª©í‘œ 1800ì !\"] \n",
    "        R0 -->|\"reward = 3.5\"| R1[\"RTG = 1796.5\\nì•„ì§ 1796.5ì  ë” í•„ìš”\"]\n",
    "        R1 -->|\"reward = 4.2\"| R2[\"RTG = 1792.3\\nì•„ì§ 1792.3ì  ë” í•„ìš”\"]\n",
    "        R2 -->|\"...ê³„ì†...\"| R3[\"RTG â‰ˆ 0\\nğŸ‰ ëª©í‘œ ë‹¬ì„±!\"]\n",
    "    end\n",
    "\n",
    "    style R0 fill:#ffcdd2\n",
    "    style R1 fill:#fff9c4\n",
    "    style R2 fill:#dcedc8\n",
    "    style R3 fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px\n",
    "```\n",
    "\n",
    "> ğŸ’¡ **RTGê°€ \"ë‚¨ì€ ëª©í‘œ\"ë¥¼ ë‚˜íƒ€ë‚´ë¯€ë¡œ, rewardë¥¼ ë°›ì„ ë•Œë§ˆë‹¤ ì°¨ê°ë©ë‹ˆë‹¤!**  \n",
    "> ì—í”¼ì†Œë“œ ëì—ì„œ RTG â‰ˆ 0ì´ë©´ ëª©í‘œë¥¼ ë‹¬ì„±í•œ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ® ëª©í‘œ Returnì— ë”°ë¥¸ í–‰ë™ ë³€í™”\n",
    "\n",
    "Decision Transformerì˜ ê°•ë ¥í•œ íŠ¹ì„±ì€ **ëª©í‘œ Returnì„ ì¡°ì ˆí•˜ì—¬ ì •ì±…ì˜ í’ˆì§ˆì„ ì œì–´**í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph TargetReturn[\"ğŸ¯ ëª©í‘œ Returnì— ë”°ë¥¸ í–‰ë™\"]\n",
    "        direction TB\n",
    "        High[\"ë†’ì€ ëª©í‘œ (RTG=3600)\\nâ†’ ê³µê²©ì ì´ê³  ìµœì ì˜ í–‰ë™\\nâ†’ ì „ë¬¸ê°€ ìˆ˜ì¤€ ì •ì±…\"]\n",
    "        Med[\"ì¤‘ê°„ ëª©í‘œ (RTG=1800)\\nâ†’ ì•ˆì •ì ì¸ ì¤‘ê°„ ìˆ˜ì¤€ í–‰ë™\\nâ†’ medium ì •ì±…\"]\n",
    "        Low[\"ë‚®ì€ ëª©í‘œ (RTG=600)\\nâ†’ ë³´ìˆ˜ì ì¸ í–‰ë™\\nâ†’ ì´ˆë³´ì ìˆ˜ì¤€ ì •ì±…\"]\n",
    "    end\n",
    "\n",
    "    style High fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px\n",
    "    style Med fill:#fff9c4\n",
    "    style Low fill:#ffcdd2\n",
    "```\n",
    "\n",
    "> ğŸ“Œ **ì´ê²ƒì´ DTì˜ í•µì‹¬ ì¥ì ì…ë‹ˆë‹¤!**  \n",
    "> í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ë‹¤ì–‘í•œ ìˆ˜ì¤€ì˜ ì •ì±…ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "> ê¸°ì¡´ RLì—ì„œëŠ” ë³´ìƒ í•¨ìˆ˜ë¥¼ ë°”ê¿”ì•¼ í–ˆì§€ë§Œ, DTëŠ” RTGë§Œ ì¡°ì ˆí•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ” Context Windowì™€ íŒ¨ë”©\n",
    "\n",
    "ì¶”ë¡  ì‹œ íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ë°©ë²•:\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Context[\"ğŸ“ Context Window ê´€ë¦¬\"]\n",
    "        direction TB\n",
    "        subgraph Short[\"íˆìŠ¤í† ë¦¬ < K\"]\n",
    "            direction LR\n",
    "            SP[\"íŒ¨ë”© 0 ... 0\"] --> SD[\"sâ‚€ sâ‚ sâ‚‚\"]\n",
    "        end\n",
    "        subgraph Exact[\"íˆìŠ¤í† ë¦¬ = K\"]\n",
    "            direction LR\n",
    "            ED[\"sâ‚€ sâ‚ ... s_{K-1}\"]\n",
    "        end\n",
    "        subgraph Long[\"íˆìŠ¤í† ë¦¬ > K\"]\n",
    "            direction LR\n",
    "            LD1[\"sâ‚€ sâ‚ ... (ë²„ë¦¼)\"]\n",
    "            LD2[\"s_{t-K+1} ... s_t (ìµœê·¼ Kê°œë§Œ ì‚¬ìš©)\"]\n",
    "            LD1 -.->|\"ì˜ë¼ëƒ„\"| LD2\n",
    "        end\n",
    "    end\n",
    "\n",
    "    style Short fill:#e3f2fd\n",
    "    style Exact fill:#e8f5e9\n",
    "    style Long fill:#fff3e0\n",
    "    style SP fill:#eeeeee\n",
    "    style LD1 fill:#ffcdd2,stroke-dasharray: 5 5\n",
    "    style LD2 fill:#c8e6c9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ¯ get_action í•¨ìˆ˜ êµ¬í˜„\n# ============================================================\n# evaluate_episodes.pyì˜ í•µì‹¬ í•¨ìˆ˜ì…ë‹ˆë‹¤.\n# í˜„ì¬ê¹Œì§€ì˜ íˆìŠ¤í† ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ actionì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n#\n# ì¤‘ìš” í¬ì¸íŠ¸:\n#   1. íˆìŠ¤í† ë¦¬ê°€ max_lengthë³´ë‹¤ ê¸¸ë©´ ìµœê·¼ Kê°œë§Œ ì‚¬ìš©\n#   2. íˆìŠ¤í† ë¦¬ê°€ max_lengthë³´ë‹¤ ì§§ìœ¼ë©´ ì•ì— íŒ¨ë”©\n#   3. ë§ˆì§€ë§‰ timestepì˜ ì˜ˆì¸¡ë§Œ ë°˜í™˜\n# ============================================================\n\ndef get_action(model, states, actions, returns_to_go, timesteps, device, max_length=20):\n    \"\"\"\n    í˜„ì¬ê¹Œì§€ì˜ íˆìŠ¤í† ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ action ì˜ˆì¸¡\n    \n    Parameters:\n    -----------\n    model : nn.Module\n        í•™ìŠµëœ Decision Transformer\n    states : torch.Tensor\n        ìƒíƒœ íˆìŠ¤í† ë¦¬ (1, current_len, state_dim)\n    actions : torch.Tensor\n        í–‰ë™ íˆìŠ¤í† ë¦¬ (1, current_len-1, action_dim) - í˜„ì¬ actionì€ ì˜ˆì¸¡í•  ê²ƒ\n    returns_to_go : torch.Tensor\n        RTG íˆìŠ¤í† ë¦¬ (1, current_len, 1)\n    timesteps : torch.Tensor\n        timestep íˆìŠ¤í† ë¦¬ (1, current_len)\n    device : torch.device\n        ì—°ì‚° ì¥ì¹˜\n    max_length : int\n        ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (K)\n        \n    Returns:\n    --------\n    torch.Tensor\n        ì˜ˆì¸¡ëœ action (action_dim,)\n    \"\"\"\n    model.eval()  # í‰ê°€ ëª¨ë“œ\n    \n    # í˜„ì¬ ì‹œí€€ìŠ¤ ê¸¸ì´\n    seq_len = states.shape[1]\n    \n    # max_lengthë¡œ ìë¥´ê¸° (ë„ˆë¬´ ê¸¸ë©´ ìµœê·¼ ê²ƒë§Œ ì‚¬ìš©)\n    if seq_len > max_length:\n        states = states[:, -max_length:]\n        actions = actions[:, -max_length:]\n        returns_to_go = returns_to_go[:, -max_length:]\n        timesteps = timesteps[:, -max_length:]\n        seq_len = max_length\n    \n    # ì°¨ì› ì •ë³´\n    state_dim = states.shape[-1]\n    act_dim = actions.shape[-1] if actions.shape[1] > 0 else ACT_DIM\n    \n    # ì•ìª½ íŒ¨ë”© (max_lengthë³´ë‹¤ ì§§ì€ ê²½ìš°)\n    padding_len = max_length - seq_len\n    \n    if padding_len > 0:\n        # íŒ¨ë”© í…ì„œ ìƒì„±\n        state_padding = torch.zeros(1, padding_len, state_dim, device=device)\n        action_padding = torch.zeros(1, padding_len, act_dim, device=device)\n        rtg_padding = torch.zeros(1, padding_len, 1, device=device)\n        timestep_padding = torch.zeros(1, padding_len, dtype=torch.long, device=device)\n        \n        # íŒ¨ë”© ì ìš© (ì•ì— ë¶™ì´ê¸°)\n        states = torch.cat([state_padding, states], dim=1)\n        actions = torch.cat([action_padding, actions], dim=1)\n        returns_to_go = torch.cat([rtg_padding, returns_to_go], dim=1)\n        timesteps = torch.cat([timestep_padding, timesteps], dim=1)\n    \n    # Attention mask ìƒì„±\n    attention_mask = torch.cat([\n        torch.zeros(1, padding_len, device=device),\n        torch.ones(1, seq_len, device=device)\n    ], dim=1) if padding_len > 0 else torch.ones(1, seq_len, device=device)\n    \n    # Forward pass\n    with torch.no_grad():\n        if DT_AVAILABLE:\n            _, action_preds, _ = model(\n                states, actions, None, returns_to_go, timesteps, attention_mask\n            )\n        else:\n            _, action_preds, _ = model(\n                states, actions, returns_to_go, timesteps\n            )\n    \n    # ë§ˆì§€ë§‰ timestepì˜ action ë°˜í™˜\n    return action_preds[0, -1]\n\n# ============================================================\n# ğŸ§ª get_action í…ŒìŠ¤íŠ¸\n# ============================================================\nprint(\"ğŸ§ª get_action í•¨ìˆ˜ í…ŒìŠ¤íŠ¸\")\nprint(\"=\"*60)\n\n# ë”ë¯¸ íˆìŠ¤í† ë¦¬ ìƒì„±\ntest_states = torch.randn(1, 5, STATE_DIM).to(device)  # 5 ìŠ¤í…ì˜ íˆìŠ¤í† ë¦¬\ntest_actions = torch.randn(1, 5, ACT_DIM).to(device)   # 5 ìŠ¤í…ì˜ action\ntest_rtg = torch.ones(1, 5, 1).to(device) * 1.8        # ëª©í‘œ RTG = 1800/1000\ntest_timesteps = torch.arange(5).unsqueeze(0).to(device)\n\nprint(f\"ì…ë ¥ íˆìŠ¤í† ë¦¬ ê¸¸ì´: 5 ìŠ¤í…\")\nprint(f\"ëª©í‘œ RTG: 1800 (ì •ê·œí™”: 1.8)\")\n\n# Action ì˜ˆì¸¡\npredicted_action = get_action(\n    model, test_states, test_actions, test_rtg, test_timesteps, device\n)\n\nprint(f\"\\nğŸ¯ ì˜ˆì¸¡ëœ Action: {predicted_action.cpu().numpy()}\")\nprint(f\"   â””â”€ shape: {predicted_action.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ® ì—í”¼ì†Œë“œ ë¡¤ì•„ì›ƒ ì‹œë®¬ë ˆì´ì…˜\n# ============================================================\n# ì‹¤ì œ í™˜ê²½ ì—†ì´ ì¶”ë¡  ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.\n# (ì‹¤ì œ í™˜ê²½ì´ ìˆë‹¤ë©´ env.step()ìœ¼ë¡œ next_stateì™€ rewardë¥¼ ë°›ìŠµë‹ˆë‹¤)\n#\n# ì´ ì‹œë®¬ë ˆì´ì…˜ì€ ì¶”ë¡  ë£¨í”„ì˜ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.\n# ============================================================\n\ndef simulate_episode(model, target_return, max_ep_len=50, scale=1000, device='cpu'):\n    \"\"\"\n    í™˜ê²½ ì—†ì´ ì—í”¼ì†Œë“œë¥¼ ì‹œë®¬ë ˆì´ì…˜\n    (ì‹¤ì œ í™˜ê²½ì´ ìˆë‹¤ë©´ envì—ì„œ stateì™€ rewardë¥¼ ë°›ìŒ)\n    \n    Parameters:\n    -----------\n    model : nn.Module\n        í•™ìŠµëœ Decision Transformer\n    target_return : float\n        ëª©í‘œ return (ì˜ˆ: 1800)\n    max_ep_len : int\n        ìµœëŒ€ ì—í”¼ì†Œë“œ ê¸¸ì´\n    scale : float\n        RTG ì •ê·œí™” ìŠ¤ì¼€ì¼\n    device : torch.device\n        ì—°ì‚° ì¥ì¹˜\n        \n    Returns:\n    --------\n    tuple: (rtg_history, action_history, reward_history)\n    \"\"\"\n    state_dim, act_dim = STATE_DIM, ACT_DIM\n    \n    # ==========================================\n    # Step 1: ì´ˆê¸°í™”\n    # ==========================================\n    # ì´ˆê¸° state (ì‹¤ì œë¡œëŠ” env.reset()ì—ì„œ ë°›ìŒ)\n    states = torch.randn(1, 1, state_dim).to(device)\n    \n    # ë¹ˆ action íˆìŠ¤í† ë¦¬ (ì•„ì§ í–‰ë™ ì—†ìŒ)\n    actions = torch.zeros(1, 0, act_dim).to(device)\n    \n    # ì´ˆê¸° RTG = ëª©í‘œ return\n    rtg = torch.tensor([[[target_return / scale]]]).to(device)\n    \n    # ì´ˆê¸° timestep\n    timesteps = torch.tensor([[0]]).to(device)\n    \n    # ê¸°ë¡ìš©\n    rtg_history = [target_return]\n    action_history = []\n    reward_history = []\n    \n    print(f\"ğŸ® ì—í”¼ì†Œë“œ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘\")\n    print(f\"   ëª©í‘œ Return: {target_return}\")\n    print(\"=\"*60)\n    \n    # ==========================================\n    # Step 2-5: ì¶”ë¡  ë£¨í”„\n    # ==========================================\n    num_steps_to_show = min(10, max_ep_len)  # ì¶œë ¥í•  ìŠ¤í… ìˆ˜\n    \n    for t in range(num_steps_to_show):\n        # ----- Step 2: Action ì˜ˆì¸¡ -----\n        action = get_action(model, states, actions, rtg, timesteps, device)\n        action_history.append(action.cpu().numpy())\n        \n        # ----- Step 3: í™˜ê²½ì—ì„œ ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜) -----\n        # ì‹¤ì œ: next_state, reward, done, _ = env.step(action)\n        fake_reward = np.random.uniform(0, 5)  # ê°€ìƒì˜ reward\n        reward_history.append(fake_reward)\n        \n        # ----- Step 4: RTG ì—…ë°ì´íŠ¸ -----\n        new_rtg = rtg_history[-1] - fake_reward\n        rtg_history.append(new_rtg)\n        \n        # ì§„í–‰ ìƒí™© ì¶œë ¥\n        action_str = np.array2string(action.cpu().numpy(), precision=2, suppress_small=True)\n        print(f\"Step {t:>2}: RTG={rtg_history[-2]:>7.1f} â†’ Action={action_str}\")\n        print(f\"         reward={fake_reward:>5.2f} â†’ new RTG={new_rtg:>7.1f}\")\n        \n        # ----- Step 5: íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ -----\n        # ìƒˆë¡œìš´ state (ì‹¤ì œë¡œëŠ” envì—ì„œ ë°›ìŒ)\n        new_state = torch.randn(1, 1, state_dim).to(device)\n        \n        # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€\n        states = torch.cat([states, new_state], dim=1)\n        actions = torch.cat([actions, action.unsqueeze(0).unsqueeze(0)], dim=1)\n        rtg = torch.cat([rtg, torch.tensor([[[new_rtg / scale]]]).to(device)], dim=1)\n        timesteps = torch.cat([timesteps, torch.tensor([[t + 1]]).to(device)], dim=1)\n        \n        print()\n    \n    return rtg_history, action_history, reward_history\n\n# ============================================================\n# ğŸ® ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰\n# ============================================================\nrtg_hist, act_hist, reward_hist = simulate_episode(\n    model, \n    target_return=1800, \n    device=device\n)\n\nprint(\"=\"*60)\nprint(f\"ğŸ“Š ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼:\")\nprint(f\"   ì´ reward íšë“: {sum(reward_hist):.2f}\")\nprint(f\"   í‰ê·  reward: {np.mean(reward_hist):.2f}\")\nprint(f\"   ìµœì¢… RTG: {rtg_hist[-1]:.1f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ğŸ“ˆ RTG ë³€í™” ì‹œê°í™”\n# ============================================================\n# ì¶”ë¡  ê³¼ì •ì—ì„œ RTGê°€ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ ì‹œê°í™”í•©ë‹ˆë‹¤.\n#\n# ê´€ì°° í¬ì¸íŠ¸:\n#   - RTGëŠ” rewardë¥¼ ë°›ì„ ë•Œë§ˆë‹¤ ê°ì†Œ\n#   - ì´ìƒì ìœ¼ë¡œëŠ” ì—í”¼ì†Œë“œ ëì—ì„œ RTG â‰ˆ 0\n#   - RTGê°€ ìŒìˆ˜ê°€ ë˜ë©´ ëª©í‘œ ì´ˆê³¼ ë‹¬ì„±!\n# ============================================================\n\nprint(\"ğŸ“ˆ RTG ë³€í™” ì‹œê°í™”\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 1. RTG ë³€í™”\nax1 = axes[0]\nax1.plot(rtg_hist, 'o-', linewidth=2, markersize=8, color='orange')\nax1.axhline(y=0, color='green', linestyle='--', linewidth=2, label='Goal (RTG=0)')\nax1.fill_between(range(len(rtg_hist)), rtg_hist, 0, alpha=0.3, color='orange')\n\nax1.set_xlabel('Step', fontsize=12)\nax1.set_ylabel('Return-to-Go', fontsize=12)\nax1.set_title('RTG Evolution During Inference', fontsize=14)\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# RTG ê°’ í‘œì‹œ\nfor i, rtg in enumerate(rtg_hist):\n    if i % 2 == 0:  # 2ê°œ ê±´ë„ˆ í‘œì‹œ\n        ax1.annotate(f'{rtg:.0f}', (i, rtg), textcoords=\"offset points\", \n                     xytext=(0, 10), ha='center', fontsize=8)\n\n# 2. Reward ë³€í™”\nax2 = axes[1]\nax2.bar(range(len(reward_hist)), reward_hist, color='steelblue', alpha=0.7, edgecolor='black')\nax2.axhline(y=np.mean(reward_hist), color='red', linestyle='--', linewidth=2,\n            label=f'Mean: {np.mean(reward_hist):.2f}')\nax2.set_xlabel('Step', fontsize=12)\nax2.set_ylabel('Reward', fontsize=12)\nax2.set_title('Instantaneous Rewards', fontsize=14)\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# í•µì‹¬ ì¸ì‚¬ì´íŠ¸\nprint(f\"\\nğŸ“Œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:\")\nprint(f\"   â€¢ ì‹œì‘ RTG: {rtg_hist[0]:.0f} (ëª©í‘œ)\")\nprint(f\"   â€¢ ìµœì¢… RTG: {rtg_hist[-1]:.0f}\")\nprint(f\"   â€¢ RTG ê°ì†ŒëŸ‰: {rtg_hist[0] - rtg_hist[-1]:.0f} (= íšë“í•œ ì´ reward)\")\nprint()\nif rtg_hist[-1] <= 0:\n    print(\"   ğŸ‰ ëª©í‘œ ë‹¬ì„±! (RTG â‰¤ 0)\")\nelse:\n    print(f\"   ğŸ“ ëª©í‘œê¹Œì§€ ë‚¨ì€ reward: {rtg_hist[-1]:.0f}\")\nprint()\nprint(\"ğŸ’¡ ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì—í”¼ì†Œë“œ ëì—ì„œ RTGê°€ 0ì— ê°€ê¹Œì›Œì•¼\")\nprint(\"   ëª©í‘œ returnì„ ë‹¬ì„±í•œ ê²ƒì…ë‹ˆë‹¤.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ“ Phase 3 ì™„ë£Œ!\n",
    "\n",
    "## ğŸ“š ì´ Phaseì—ì„œ ë°°ìš´ ë‚´ìš©\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph Summary[\"ğŸ“ Phase 3 í•™ìŠµ ìš”ì•½\"]\n",
    "        direction TB\n",
    "        subgraph S1[\"1. ë°ì´í„°ì…‹ êµ¬ì¡°\"]\n",
    "            D1[\"trajectories = list of episodes\"]\n",
    "            D2[\"ê° traj = observations + actions + rewards\"]\n",
    "        end\n",
    "\n",
    "        subgraph S2[\"2. RTG ê³„ì‚°\"]\n",
    "            R1[\"RTG_t = r_t + RTG_{t+1}\"]\n",
    "            R2[\"ì—­ìˆœ ê³„ì‚° discount_cumsum\"]\n",
    "        end\n",
    "\n",
    "        subgraph S3[\"3. ëª¨ë¸ ì•„í‚¤í…ì²˜\"]\n",
    "            M1[\"ì…ë ¥: (RTG, State, Action) ì‹œí€€ìŠ¤\"]\n",
    "            M2[\"Embedding â†’ GPT-2 â†’ predict_action\"]\n",
    "        end\n",
    "\n",
    "        subgraph S4[\"4. í•™ìŠµ ê³¼ì •\"]\n",
    "            T1[\"Loss = MSE(predicted, target action)\"]\n",
    "            T2[\"= Supervised Learning!\"]\n",
    "        end\n",
    "\n",
    "        subgraph S5[\"5. ì¶”ë¡  ê³¼ì •\"]\n",
    "            I1[\"ëª©í‘œ RTG ì„¤ì • â†’ Action ì˜ˆì¸¡\"]\n",
    "            I2[\"â†’ í™˜ê²½ ì‹¤í–‰ â†’ RTG ì—…ë°ì´íŠ¸ â†’ ë°˜ë³µ\"]\n",
    "        end\n",
    "\n",
    "        S1 --> S2 --> S3 --> S4 --> S5\n",
    "    end\n",
    "\n",
    "    style S1 fill:#e3f2fd\n",
    "    style S2 fill:#fff3e0\n",
    "    style S3 fill:#e8f5e9\n",
    "    style S4 fill:#fce4ec\n",
    "    style S5 fill:#f3e5f5\n",
    "```\n",
    "\n",
    "## ğŸ”‘ í•µì‹¬ ê°œë… í•œëˆˆì— ë³´ê¸°\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Training[\"ğŸ“š í•™ìŠµ (Offline)\"]\n",
    "        direction TB\n",
    "        TR1[\"ê³¼ê±° ë°ì´í„°ì…‹\"] --> TR2[\"RTG ê³„ì‚°\"]\n",
    "        TR2 --> TR3[\"(R, s, a) ì‹œí€€ìŠ¤ êµ¬ì„±\"]\n",
    "        TR3 --> TR4[\"Transformer Forward\"]\n",
    "        TR4 --> TR5[\"MSE Loss â†’ Backprop\"]\n",
    "    end\n",
    "\n",
    "    subgraph Inference[\"ğŸ¯ ì¶”ë¡  (Online)\"]\n",
    "        direction TB\n",
    "        IN1[\"ëª©í‘œ Return ì„¤ì •\"] --> IN2[\"State ê´€ì¸¡\"]\n",
    "        IN2 --> IN3[\"Action ì˜ˆì¸¡\"]\n",
    "        IN3 --> IN4[\"í™˜ê²½ì—ì„œ ì‹¤í–‰\"]\n",
    "        IN4 --> IN5[\"RTG -= reward\"]\n",
    "        IN5 -.-> IN2\n",
    "    end\n",
    "\n",
    "    Training -->|\"í•™ìŠµëœ ëª¨ë¸\"| Inference\n",
    "\n",
    "    style Training fill:#e3f2fd\n",
    "    style Inference fill:#fff8e1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Phase 3 ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "| ì™„ë£Œ | í•­ëª© |\n",
    "|:---:|:---|\n",
    "| â˜ | í™˜ê²½ ì„¤ì • ë° ê²½ë¡œ êµ¬ì„± ì™„ë£Œ |\n",
    "| â˜ | D4RL ë°ì´í„°ì…‹ êµ¬ì¡° ì´í•´ |\n",
    "| â˜ | RTG ê³„ì‚° (`discount_cumsum`) êµ¬í˜„ |\n",
    "| â˜ | DecisionTransformer ëª¨ë¸ êµ¬ì¡° ì´í•´ |\n",
    "| â˜ | Forward pass ë™ì‘ í™•ì¸ |\n",
    "| â˜ | ë°°ì¹˜ ìƒì„± (`get_batch`) ì´í•´ |\n",
    "| â˜ | í•™ìŠµ ë£¨í”„ êµ¬í˜„ ë° ì‹¤í–‰ |\n",
    "| â˜ | ì¶”ë¡  ì‹œ RTG ì—…ë°ì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ ì´í•´ |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    P3[\"âœ… Phase 3\\nGym í™˜ê²½ ì‹¤ìŠµ\\nì™„ë£Œ!\"] --> P4[\"ğŸ¯ Phase 4\\nAtari í™˜ê²½ ì‹¤ìŠµ\"]\n",
    "\n",
    "    subgraph Next[\"Phase 4ì—ì„œ ë°°ìš¸ ë‚´ìš©\"]\n",
    "        direction TB\n",
    "        N1[\"ğŸ–¼ï¸ ì´ë¯¸ì§€ ê¸°ë°˜ í™˜ê²½ (84Ã—84 í”½ì…€)\"]\n",
    "        N2[\"ğŸ® DQN-replay ë°ì´í„°ì…‹\"]\n",
    "        N3[\"ğŸ‘¾ Atari ê²Œì„ (Breakout, Pong ë“±)\"]\n",
    "        N4[\"ğŸ—ï¸ ë” í° ëª¨ë¸ + CNN ì¸ì½”ë”\"]\n",
    "    end\n",
    "\n",
    "    P4 --> Next\n",
    "\n",
    "    style P3 fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px\n",
    "    style P4 fill:#ffeb3b,stroke:#f57f17,stroke-width:2px\n",
    "    style Next fill:#fff3e0\n",
    "```\n",
    "\n",
    "**Phase 4: Atari í™˜ê²½ ì‹¤ìŠµ** â†’ [phase4_atari_practice.ipynb](./phase4_atari_practice.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ ì¶”ê°€ í•™ìŠµ ìë£Œ\n",
    "\n",
    "1. **ì½”ë“œ ë¶„ì„**: `gym/decision_transformer/models/decision_transformer.py` ì½ì–´ë³´ê¸°\n",
    "2. **ì‹¤ì œ ì‹¤í–‰**: D4RL ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ í›„ `experiment.py` ì‹¤í–‰\n",
    "3. **ë…¼ë¬¸**: [Decision Transformer](https://arxiv.org/abs/2106.01345) ì›ë¬¸ ì½ê¸°\n",
    "\n",
    "---\n",
    "\n",
    "> ğŸ‰ **ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!** Phase 3ì—ì„œ Decision Transformerì˜ ì‹¤ì œ êµ¬í˜„ì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤.  \n",
    "> ì´ì œ Atari í™˜ê²½ì—ì„œì˜ ì‘ìš©ì„ ë°°ìš¸ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}