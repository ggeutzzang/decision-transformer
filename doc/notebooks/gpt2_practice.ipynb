{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 ì‹¤ìŠµ: Decision Transformerë¥¼ ìœ„í•œ ê¸°ì´ˆ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ GPT-2ì˜ í•µì‹¬ êµ¬ì¡°ë¥¼ PyTorchë¡œ ì§ì ‘ êµ¬í˜„í•˜ë©° ë°°ì›ë‹ˆë‹¤.\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "\n",
    "1. Self-Attention ë©”ì»¤ë‹ˆì¦˜ ì´í•´\n",
    "2. Transformer Block êµ¬ì¡° íŒŒì•…\n",
    "3. Causal Maskingì˜ ì¤‘ìš”ì„± ì´í•´\n",
    "4. ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± ê²½í—˜\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ“¦ íŒ¨í‚¤ì§€ ì„í¬íŠ¸\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "print(f\"ğŸ PyTorch: {torch.__version__}\")\n",
    "print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
    "print()\n",
    "print(\"==================================================\")\n",
    "ì¤€ë¹„ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. GPT-2 ê°œìš”\n\n### 1.1 ì „ì²´ ì•„í‚¤í…ì²˜\n\n```mermaid\nflowchart TB\n    subgraph Input[\"ğŸ“¥ ì…ë ¥\"]\n        Tokens[\"Token IDs<br/>(B, T)\"]\n    end\n\n    subgraph Embedding[\"1ï¸âƒ£ ì„ë² ë”©\"]\n        TE[\"Token Embedding<br/>(vocab_size â†’ n_embd)\"]\n        PE[\"Positional Embedding<br/>(block_size â†’ n_embd)\"]\n        Add[\"ì„ë² ë”© í•©ì‚°\"]\n        Drop[\"Dropout\"]\n    end\n\n    subgraph Blocks[\"2ï¸âƒ£ Transformer Blocks Ã— N\"]\n        direction TB\n        B1[\"Block 1\"]\n        B2[\"Block 2\"]\n        B3[\"...\"]\n        B4[\"Block N\"]\n    end\n\n    subgraph Head[\"3ï¸âƒ£ ì¶œë ¥ í—¤ë“œ\"]\n        LN[\"LayerNorm\"]\n        Linear[\"Linear<br/>(n_embd â†’ vocab_size)\"]\n    end\n\n    subgraph Output[\"ğŸ“¤ ì¶œë ¥\"]\n        Logits[\"Logits<br/>(B, T, vocab_size)\"]\n        NextToken[\"Next Token\"]\n    end\n\n    Tokens --> TE\n    Tokens --> PE\n    TE --> Add\n    PE --> Add\n    Add --> Drop --> Blocks --> LN --> Linear --> Logits --> NextToken\n\n    style Blocks fill:#e3f2fd\n    style NextToken fill:#c8e6c9\n    style Add fill:#fff9c4\n```\n\n### 1.2 GPT-2ëŠ” ë¬´ì—‡ì¸ê°€?\n\n**GPT-2** = **G**enerative **P**re-trained **T**ransformer-2\n\n```mermaid\nmindmap\n    root((GPT-2))\n        Generative[\"Generative<br/>ë‹¤ìŒ í† í° ìƒì„±\"]\n        Pretrained[\"Pre-trained<br/>ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„°\"]\n        Transformer[\"Transformer<br/>Self-attention ê¸°ë°˜\"]\n        Autoregressive[\"Autoregressive<br/>ì´ì „ í† í°ë§Œ ì°¸ì¡°\"]\n```\n\n- **Generative**: ë‹¤ìŒ í† í°ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸\n- **Pre-trained**: ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì‚¬ì „ í•™ìŠµ\n- **Transformer**: Self-attention ê¸°ë°˜ ì•„í‚¤í…ì²˜\n- **Autoregressive**: Causal maskingìœ¼ë¡œ ì´ì „ í† í°ë§Œ ì°¸ì¡°\n\n### 1.3 ë™ì‘ ì˜ˆì‹œ\n\n```\nì…ë ¥: \"The cat sat on the\"\nì¶œë ¥: \"mat\" (ê°€ì¥ ë†’ì€ í™•ë¥ )\n\ní™•ë¥  ë¶„í¬:\nP(mat | The, cat, sat, on, the)    = 0.23  â† ì„ íƒ\nP(floor | The, cat, sat, on, the)  = 0.15\nP(bed | The, cat, sat, on, the)    = 0.10\nP(couch | The, cat, sat, on, the)  = 0.08\n...\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Self-Attention êµ¬í˜„\n\n### 2.1 Attentionì˜ ì§ê´€ì  ì´í•´\n\n```mermaid\nflowchart LR\n    subgraph Question[\"Query: 'cat'ì´ ë¬´ì—‡ì„ ì§‘ì¤‘í•´ì„œ ë³´ëŠ”ê°€?\"]\n        Q[\"cat (Query)\"]\n    end\n\n    subgraph Keys[\"Keys (ë¬¸ë§¥ ë‹¨ì–´ë“¤)\"]\n        K1[\"The\"]\n        K2[\"cat\"]\n        K3[\"sat\"]\n        K4[\"on\"]\n        K5[\"the\"]\n        K6[\"mat\"]\n    end\n\n    subgraph Attention[\"Attention Weights\"]\n        A1[\"0.1\"]\n        A2[\"0.5 â­\"]\n        A3[\"0.3\"]\n        A4[\"0.05\"]\n        A5[\"0.05\"]\n        A6[\"0.0\"]\n    end\n\n    Q --> A1\n    Q --> A2\n    Q --> A3\n    Q --> A4\n    Q --> A5\n    Q --> A6\n\n    K1 -.-> A1\n    K2 -.-> A2\n    K3 -.-> A3\n    K4 -.-> A4\n    K5 -.-> A5\n    K6 -.-> A6\n\n    style Q fill:#ffcdd2\n    style A2 fill:#c8e6c9\n    style K2 fill:#c8e6c9\n```\n\n**í•µì‹¬ ì•„ì´ë””ì–´**: Attention = ê°€ì¤‘ì¹˜ í•© (Weighted Sum)\n- Query: \"ë‚´ê°€ ë¬´ì—‡ì„ ì°¾ëŠ”ê°€?\"\n- Key: \"ê° í† í°ì´ ë¬´ì—‡ì„ ì œê³µí•˜ëŠ”ê°€?\"\n- Value: \"ì‹¤ì œ ë‚´ìš©æ˜¯ä»€ä¹ˆ?\"\n\n### 2.2 Scaled Dot-Product Attention\n\n```mermaid\nflowchart TB\n    subgraph Input[\"ì…ë ¥\"]\n        Q[\"Query Matrix<br/>Q (B, n_head, T, d_k)\"]\n        K[\"Key Matrix<br/>K (B, n_head, T, d_k)\"]\n        V[\"Value Matrix<br/>V (B, n_head, T, d_k)\"]\n    end\n\n    subgraph Step1[\"Step 1: Similarity\"]\n        MatMul[\"Q @ K^T<br/>(B, n_head, T, T)\"]\n    end\n\n    subgraph Step2[\"Step 2: Scale\"]\n        Scale[\"/ âˆšd_k<br/>ìŠ¤ì¼€ì¼ë§\"]\n    end\n\n    subgraph Step3[\"Step 3: Mask (Optional)\"]\n        Mask[\"Causal Mask<br/>í•˜ì‚¼ê° ë§ˆìŠ¤í¬\"]\n    end\n\n    subgraph Step4[\"Step 4: Normalize\"]\n        Softmax[\"Softmax<br/>í™•ë¥  ë¶„í¬\"]\n    end\n\n    subgraph Step5[\"Step 5: Weighted Sum\"]\n        AttnOut[\"@ V<br/>(B, n_head, T, d_k)\"]\n    end\n\n    Q --> MatMul\n    K --> MatMul\n    MatMul --> Scale --> Mask --> Softmax --> AttnOut\n    V --> AttnOut\n\n    style MatMul fill:#ffcdd2\n    style Softmax fill:#c8e6c9\n    style AttnOut fill:#fff9c4\n```\n\n**ê³µì‹**: \n```\nAttention(Q, K, V) = softmax(Q @ K^T / âˆšd_k) @ V\n```\n\n### 2.3 ì™œ Multi-Headì¸ê°€?\n\n```mermaid\nflowchart TB\n    subgraph Single[\"Single Head\"]\n        S1[\"í•˜ë‚˜ì˜ Attention<br/>ëª¨ë“  ì •ë³´ë¥¼ í•œ ê³³ì—\"]\n        SOut[\"ì¶œë ¥\"]\n    end\n\n    subgraph Multi[\"Multi-Head\"]\n        M1[\"Head 1: ë¬¸ë²• ì •ë³´\"]\n        M2[\"Head 2: ì˜ë¯¸ ì •ë³´\"]\n        M3[\"Head 3: ìœ„ì¹˜ ì •ë³´\"]\n        M4[\"Head 4: ...\"]\n        MOut[\"ê²°í•©: ë‹¤ì–‘í•œ ê´€ì \"]\n    end\n\n    subgraph Compare[\"ë¹„êµ\"]\n        C1[\"Single: í•˜ë‚˜ì˜ í‘œí˜„ ê³µê°„\"]\n        C2[\"Multi: ì—¬ëŸ¬ í‘œí˜„ ê³µê°„<br/>ë” í’ë¶€í•œ í‘œí˜„\"]\n    end\n\n    S1 --> SOut\n    M1 --> MOut\n    M2 --> MOut\n    M3 --> MOut\n    M4 --> MOut\n\n    SOut --> C1\n    MOut --> C2\n\n    style Single fill:#ffcdd2\n    style Multi fill:#c8e6c9\n```\n\n**ì¥ì **:\n- **ë‹¤ì–‘í•œ í‘œí˜„**: ê° Headê°€ ë‹¤ë¥¸ ê´€ì ì—ì„œ ì •ë³´ í¬ì°©\n- **ë³‘ë ¬ ì²˜ë¦¬**: ë™ì‹œì— ì—¬ëŸ¬ Attention ê³„ì‚°\n- **í‘œí˜„ë ¥ í–¥ìƒ**: ë” ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥\n\n### 2.4 Multi-Head Attention êµ¬ì¡°\n\n```mermaid\nflowchart TB\n    subgraph Input[\"ì…ë ¥: x (B, T, C)\"]\n        X[\"x\"]\n    end\n\n    subgraph Projections[\"Q, K, V Projections\"]\n        Q[\"Linear â†’ Q<br/>(B, T, C)\"]\n        K[\"Linear â†’ K<br/>(B, T, C)\"]\n        V[\"Linear â†’ V<br/>(B, T, C)\"]\n    end\n\n    subgraph Split[\"Multi-Head ë¶„í• \"]\n        S1[\"Q â†’ (B, n_head, T, head_dim)\"]\n        S2[\"K â†’ (B, n_head, T, head_dim)\"]\n        S3[\"V â†’ (B, n_head, T, head_dim)\"]\n        Note1[\"head_dim = C / n_head\"]\n    end\n\n    subgraph Attn[\"Attention ê³„ì‚°\"]\n        MatMul[\"Q @ K^T<br/>(B, n_head, T, T)\"]\n        Scale[\"/ âˆšd_k\"]\n        Mask[\"Causal Mask\"]\n        Softmax[\"Softmax\"]\n        AttnOut[\"@ V<br/>(B, n_head, T, head_dim)\"]\n    end\n\n    subgraph Concat[\"Multi-Head ê²°í•©\"]\n        Concat[\"Transpose + Reshape<br/>(B, n_head, T, head_dim)<br/>â†’ (B, T, C)\"]\n    end\n\n    subgraph Output[\"ì¶œë ¥ Projection\"]\n        Proj[\"Linear(C â†’ C)\"]\n        Dropout[\"Dropout\"]\n    end\n\n    X --> Q\n    X --> K\n    X --> V\n\n    Q --> S1\n    K --> S2\n    V --> S3\n\n    S1 --> MatMul\n    S2 --> MatMul\n    S3 --> AttnOut\n\n    MatMul --> Scale --> Mask --> Softmax --> AttnOut\n    AttnOut --> Concat --> Proj --> Dropout\n\n    style MatMul fill:#ffcdd2\n    style AttnOut fill:#c8e6c9\n    style Softmax fill:#fff9c4\n    style Concat fill:#e3f2fd\n```\n\n**Shape ë³€í™” ì˜ˆì‹œ** (n_head=4, C=128):\n- ì…ë ¥: `(B, T, 128)`\n- Q, K, V: `(B, T, 128)`\n- Split í›„: `(B, 4, T, 32)` â† head_dim = 128/4 = 32\n- Attention í›„: `(B, 4, T, 32)`\n- Concat í›„: `(B, T, 128)`\n\n### 2.5 Multi-Head Attentionì˜ íš¨ê³¼\n\n```mermaid\nflowchart LR\n    subgraph Sentence[\"ë¬¸ì¥: 'The cat sat on the mat'\"]\n        W1[\"cat\"]\n        W2[\"mat\"]\n    end\n\n    subgraph Heads[\"ê° Headì˜ ì—­í• \"]\n        H1[\"Head 1:<br/>ë¬¸ë²• ê´€ê³„<br/>cat â†” sat\"]\n        H2[\"Head 2:<br/>ëŒ€ëª…ì‚¬ ì°¸ì¡°<br/>The â†” cat\"]\n        H3[\"Head 3:<br/>ì „ì¹˜ì‚¬ ê´€ê³„<br/>on â†” mat\"]\n        H4[\"Head 4:<br/>ì˜ë¯¸ ê´€ê³„<br/>cat â†” mat\"]\n    end\n\n    subgraph Result[\"ê²°ê³¼\"]\n        R1[\"í†µí•©ëœ í‘œí˜„:<br/>ë¬¸ë²• + ì˜ë¯¸ + êµ¬ì¡°\"]\n    end\n\n    W1 --> H1\n    W1 --> H2\n    W1 --> H4\n    W2 --> H3\n    W2 --> H4\n\n    H1 --> R1\n    H2 --> R1\n    H3 --> R1\n    H4 --> R1\n\n    style Heads fill:#e3f2fd\n    style Result fill:#c8e6c9\n```\n\n### 2.6 Causal Masking í•„ìš”æ€§"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ§  Attention ë©”ì»¤ë‹ˆì¦˜ ì‹œê°í™”\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ì˜ˆì‹œ ë¬¸ì¥: \"The cat sat on the mat\"\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "# ê°€ì§œ attention weights (ì‹¤ì œë¡œëŠ” ëª¨ë¸ì´ í•™ìŠµ)\n",
    "# ê° í–‰ì€ í•´ë‹¹ ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ì— ì–¼ë§ˆë‚˜ attentioní•˜ëŠ”ì§€\n",
    "attention_weights = torch.tensor([\n",
    "    [0.6, 0.3, 0.1, 0.0, 0.0, 0.0],  # \"The\"ëŠ” \"cat\"ì— ë§ì´ attend\n",
    "    [0.2, 0.5, 0.2, 0.1, 0.0, 0.0],  # \"cat\"ì€ ë³¸ì¸ê³¼ \"sat\"ì— attend\n",
    "    [0.1, 0.3, 0.4, 0.1, 0.1, 0.0],  # \"sat\"ì€ ì£¼ë³€ì— attend\n",
    "    [0.0, 0.1, 0.2, 0.4, 0.2, 0.1],  # \"on\"ì€ ì£¼ë³€ì— attend\n",
    "    [0.0, 0.0, 0.1, 0.2, 0.5, 0.2],  # \"the\"ëŠ” \"mat\"ì— attend\n",
    "    [0.0, 0.0, 0.1, 0.1, 0.3, 0.5],  # \"mat\"ì€ ë³¸ì¸ê³¼ ì´ì „ ë‹¨ì–´ì— attend\n",
    "])\n",
    "\n",
    "# Causal masking ì ìš© (ë¯¸ë˜ ë‹¨ì–´ëŠ” ë³¼ ìˆ˜ ì—†ìŒ)\n",
    "causal_mask = torch.tril(torch.ones(6, 6))\n",
    "attention_weights = attention_weights * causal_mask\n",
    "attention_weights = attention_weights / attention_weights.sum(dim=1, keepdim=True)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention_weights.numpy(), \n",
    "            xticklabels=tokens, \n",
    "            yticklabels=tokens,\n",
    "            cmap='Blues',\n",
    "            annot=True,\n",
    "            fmt='.2f')\n",
    "plt.title('Attention Pattern (Causal Masking Applied)')\n",
    "plt.xlabel('Key (attend to)')\n",
    "plt.ylabel('Query (current token)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Attention Pattern í•´ì„:\")\n",
    "print(\"  - ê° í–‰ì€ í•´ë‹¹ ë‹¨ì–´ê°€ ë¬¸ë§¥ì—ì„œ ì–´ë–¤ ë‹¨ì–´ì— ì§‘ì¤‘í•˜ëŠ”ì§€\")\n",
    "print(\"  - Causal maskingìœ¼ë¡œ ë¯¸ë˜ ë‹¨ì–´ëŠ” 0 (ë³¼ ìˆ˜ ì—†ìŒ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Self-Attention êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# ğŸ”§ Causal Self-Attention êµ¬í˜„\n# ============================================================\n\nclass CausalSelfAttention(nn.Module):\n    \"\"\"\n    GPT-2ì˜ Causal Self-Attention\n    \n    í•µì‹¬: ê° í† í°ì´ ìì‹ ê³¼ ì´ì „ í† í°ë§Œ ë³¼ ìˆ˜ ìˆìŒ\n    \"\"\"\n    def __init__(self, n_embd, n_head, block_size, attn_pdrop=0.1, resid_pdrop=0.1):\n        super().__init__()\n        assert n_embd % n_head == 0\n        \n        self.n_head = n_head\n        self.n_embd = n_embd\n        \n        # Q, K, V projections\n        self.key = nn.Linear(n_embd, n_embd)\n        self.query = nn.Linear(n_embd, n_embd)\n        self.value = nn.Linear(n_embd, n_embd)\n        \n        # Dropout\n        self.attn_drop = nn.Dropout(attn_pdrop)\n        self.resid_drop = nn.Dropout(resid_pdrop)\n        \n        # Output projection\n        self.proj = nn.Linear(n_embd, n_embd)\n        \n        # Causal mask (í•˜ì‚¼ê° í–‰ë ¬)\n        self.register_buffer(\n            \"mask\",\n            torch.tril(torch.ones(block_size, block_size))\n                 .view(1, 1, block_size, block_size)\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: (B, T, C) - Batch, Time, Channels\n        Returns:\n            y: (B, T, C)\n        \"\"\"\n        B, T, C = x.size()\n        \n        # Q, K, V ê³„ì‚°\n        k = self.key(x)   # (B, T, C)\n        q = self.query(x) # (B, T, C)\n        v = self.value(x) # (B, T, C)\n        \n        # Multi-headë¡œ ë¶„í• \n        head_dim = C // self.n_head\n        k = k.view(B, T, self.n_head, head_dim).transpose(1, 2)  # (B, n_head, T, head_dim)\n        q = q.view(B, T, self.n_head, head_dim).transpose(1, 2)\n        v = v.view(B, T, self.n_head, head_dim).transpose(1, 2)\n        \n        # Attention scores\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        \n        # Causal masking\n        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_drop(att)\n        \n        # Attention output\n        y = att @ v  # (B, n_head, T, head_dim)\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        \n        # Output projection\n        y = self.resid_drop(self.proj(y))\n        \n        return y\n\n# í…ŒìŠ¤íŠ¸\nB, T, C = 2, 10, 32\nattn = CausalSelfAttention(n_embd=C, n_head=4, block_size=128)\nx = torch.randn(B, T, C)\ny = attn(x)\n\nprint(f\"âœ… Attention í…ŒìŠ¤íŠ¸ í†µê³¼!\")\nprint(f\"   ì…ë ¥ shape: {x.shape}\")\nprint(f\"   ì¶œë ¥ shape: {y.shape}\")\nprint(f\"   Head ìˆ˜: {attn.n_head}\")\nprint(f\"   Head ì°¨ì›: {C // attn.n_head}\")\n\n# ============================================================\n# ğŸ” Multi-Head Attention ë‚´ë¶€ ë“¤ì—¬ë‹¤ë³´ê¸°\n# ============================================================\n\ndef inspect_multihead_attention():\n    \"\"\"Multi-Head Attentionì˜ ë‚´ë¶€ ë™ì‘ì„ ìì„¸íˆ í™•ì¸\"\"\"\n    \n    # ì„¤ì •\n    B, T, C = 1, 5, 12  # ë°°ì¹˜ 1, ì‹œí€€ìŠ¤ 5, ì°¨ì› 12\n    n_head = 3\n    head_dim = C // n_head\n    \n    # ëª¨ë¸ ìƒì„±\n    attn_layer = CausalSelfAttention(n_embd=C, n_head=n_head, block_size=128)\n    \n    # ì…ë ¥\n    x = torch.randn(B, T, C)\n    \n    # ê° ë‹¨ê³„ë³„ë¡œ í™•ì¸\n    with torch.no_grad():\n        # Q, K, V ê³„ì‚°\n        k = attn_layer.key(x)   # (B, T, C)\n        q = attn_layer.query(x) # (B, T, C)\n        v = attn_layer.value(x) # (B, T, C)\n        \n        print(\"ğŸ“Š Q, K, V Projection:\")\n        print(f\"   Q shape: {q.shape}\")  # (1, 5, 12)\n        print(f\"   K shape: {k.shape}\")\n        print(f\"   V shape: {v.shape}\")\n        \n        # Multi-head ë¶„í• \n        k = k.view(B, T, n_head, head_dim).transpose(1, 2)  # (B, n_head, T, head_dim)\n        q = q.view(B, T, n_head, head_dim).transpose(1, 2)\n        v = v.view(B, T, n_head, head_dim).transpose(1, 2)\n        \n        print(f\"\\nğŸ“Š Multi-Head ë¶„í•  í›„:\")\n        print(f\"   Q shape: {q.shape}\")  # (1, 3, 5, 4) - 3ê°œ head, ê° head_dim=4\n        print(f\"   K shape: {k.shape}\")\n        print(f\"   V shape: {v.shape}\")\n        print(f\"   ê° Head ì°¨ì›: {head_dim}\")\n        \n        # Attention scores (ê° headë³„)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        print(f\"\\nğŸ“Š Attention Scores (before masking): {att.shape}\")  # (1, 3, 5, 5)\n        print(f\"   ê° Headê°€ ê° í† í° ìŒì˜ similarity ê³„ì‚°\")\n        \n        # Causal masking\n        mask = attn_layer.mask[:, :, :T, :T]\n        att_masked = att.masked_fill(mask == 0, float('-inf'))\n        att_probs = F.softmax(att_masked, dim=-1)\n        \n        print(f\"\\nğŸ“Š Attention Probabilities (after masking & softmax):\")\n        for h in range(n_head):\n            print(f\"\\n   Head {h+1}ì˜ Attention íŒ¨í„´:\")\n            for t in range(T):\n                probs = att_probs[0, h, t, :t+1].numpy()  # í˜„ì¬ í† í°ê¹Œì§€\n                print(f\"      í† í° {t} â†’ {[f'{p:.2f}' for p in probs]}\")\n        \n        # Weighted sum\n        y = att_probs @ v  # (1, 3, 5, 4)\n        print(f\"\\nğŸ“Š Weighted Sum (ê° Headì˜ ì¶œë ¥): {y.shape}\")\n        \n        # Head ê²°í•©\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        print(f\"ğŸ“Š Head ê²°í•© í›„: {y.shape}\")\n        \n        # Output projection\n        y_final = attn_layer.proj(attn_layer.resid_drop(y))\n        print(f\"ğŸ“Š ìµœì¢… ì¶œë ¥: {y_final.shape}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ğŸ” Multi-Head Attention ë‚´ë¶€ ë¶„ì„\")\nprint(\"=\"*60)\ninspect_multihead_attention()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ”§ Causal Self-Attention êµ¬í˜„\n",
    "# ============================================================\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-2ì˜ Causal Self-Attention\n",
    "    \n",
    "    í•µì‹¬: ê° í† í°ì´ ìì‹ ê³¼ ì´ì „ í† í°ë§Œ ë³¼ ìˆ˜ ìˆìŒ\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head, block_size, attn_pdrop=0.1, resid_pdrop=0.1):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        \n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        \n",
    "        # Q, K, V projections\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "        # Dropout\n",
    "        self.attn_drop = nn.Dropout(attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(resid_pdrop)\n",
    "        \n",
    "        # Output projection\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "        # Causal mask (í•˜ì‚¼ê° í–‰ë ¬)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(block_size, block_size))\n",
    "                 .view(1, 1, block_size, block_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, T, C) - Batch, Time, Channels\n",
    "        Returns:\n",
    "            y: (B, T, C)\n",
    "        \"\"\"\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # Q, K, V ê³„ì‚°\n",
    "        k = self.key(x)   # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        \n",
    "        # Multi-headë¡œ ë¶„í• \n",
    "        head_dim = C // self.n_head\n",
    "        k = k.view(B, T, self.n_head, head_dim).transpose(1, 2)  # (B, n_head, T, head_dim)\n",
    "        q = q.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        \n",
    "        # Causal masking\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        \n",
    "        # Attention output\n",
    "        y = att @ v  # (B, n_head, T, head_dim)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        \n",
    "        return y\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "B, T, C = 2, 10, 32\n",
    "attn = CausalSelfAttention(n_embd=C, n_head=4, block_size=128)\n",
    "x = torch.randn(B, T, C)\n",
    "y = attn(x)\n",
    "\n",
    "print(f\"âœ… Attention í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "print(f\"   ì…ë ¥ shape: {x.shape}\")\n",
    "print(f\"   ì¶œë ¥ shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Transformer Block\n\n### 3.1 Block êµ¬ì¡°\n\n```mermaid\nflowchart TB\n    subgraph Block[\"Transformer Block\"]\n        direction TB\n        \n        subgraph Layer1[\"Layer 1: Attention\"]\n            LN1[\"LayerNorm(x)\"]\n            Attn[\"Self-Attention(x)\"]\n            Res1[\"x + Attn(LN1(x))<br/>Residual Connection\"]\n        end\n        \n        subgraph Layer2[\"Layer 2: MLP\"]\n            LN2[\"LayerNorm(x)\"]\n            MLP[\"MLP(x)<br/>Linear â†’ GELU â†’ Linear\"]\n            Res2[\"x + MLP(LN2(x))<br/>Residual Connection\"]\n        end\n    end\n\n    Input[\"x (B, T, C)\"] --> LN1\n    LN1 --> Attn --> Res1 --> LN2 --> MLP --> Res2 --> Output[\"y (B, T, C)\"]\n\n    style Attn fill:#ffcdd2\n    style MLP fill:#c8e6c9\n    style Res1 fill:#fff9c4\n    style Res2 fill:#fff9c4\n```\n\n### 3.2 Pre-LN vs Post-LN\n\n```mermaid\nflowchart LR\n    subgraph PreLN[\"Pre-LN (GPT-2 ìŠ¤íƒ€ì¼)\"]\n        direction TB\n        P1[\"x\"]\n        P2[\"LN(x)\"]\n        P3[\"SubLayer(LN(x))\"]\n        P4[\"x + SubLayer(LN(x))\"]\n        \n        P1 --> P2 --> P3 --> P4\n    end\n\n    subgraph PostLN[\"Post-LN (ì›ë³¸ Transformer)\"]\n        direction TB\n        L1[\"x\"]\n        L2[\"SubLayer(x)\"]\n        L3[\"LN(x + SubLayer(x))\"]\n        \n        L1 --> L2 --> L3\n    end\n\n    style PreLN fill:#c8e6c9\n    style PostLN fill:#ffcdd2\n```\n\n**GPT-2ëŠ” Pre-LN ì‚¬ìš©** â†’ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ\n\n### 3.3 MLP (Feed Forward Network)\n\n```mermaid\nflowchart LR\n    subgraph MLP[\"Feed Forward Network\"]\n        In[\"x (B, T, C)\"]\n        \n        L1[\"Linear<br/>C â†’ 4Ã—C\"]\n        Act[\"GELU Activation\"]\n        L2[\"Linear<br/>4Ã—C â†’ C\"]\n        Drop[\"Dropout\"]\n        \n        Out[\"y (B, T, C)\"]\n    end\n\n    In --> L1 --> Act --> L2 --> Drop --> Out\n\n    style L1 fill:#e3f2fd\n    style L2 fill:#e3f2fd\n    style Act fill:#fff9c4\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ§± Transformer Block êµ¬í˜„\n",
    "# ============================================================\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Block: Attention + MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head, block_size, resid_pdrop=0.1, attn_pdrop=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, n_head, block_size, attn_pdrop, resid_pdrop)\n",
    "        \n",
    "        # MLP: 4ë°° í™•ì¥ í›„ ì¶•ì†Œ\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(resid_pdrop),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pre-LN + Residual (GPT-2 ìŠ¤íƒ€ì¼)\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "block = TransformerBlock(n_embd=32, n_head=4, block_size=128)\n",
    "x = torch.randn(2, 10, 32)\n",
    "y = block(x)\n",
    "\n",
    "print(f\"âœ… Transformer Block í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "print(f\"   ì¶œë ¥ shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GPT-2 ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ¤– GPT-2 ëª¨ë¸ êµ¬í˜„\n",
    "# ============================================================\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    ê°„ë‹¨í•œ GPT-2 ëª¨ë¸\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, block_size, n_layer=6, n_head=8, n_embd=512):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Token & Positional embeddings\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, block_size, n_embd))\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(n_embd, n_head, block_size)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm and head\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: (B, T) - Token IDs\n",
    "            targets: (B, T) - Target tokens (optional)\n",
    "        Returns:\n",
    "            logits: (B, T, vocab_size)\n",
    "            loss: scalar (optional)\n",
    "        \"\"\"\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.block_size\n",
    "        \n",
    "        # Embeddings\n",
    "        tok_emb = self.tok_emb(idx)  # (B, T, n_embd)\n",
    "        pos_emb = self.pos_emb[:, :T, :]  # (1, T, n_embd)\n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        x = self.blocks(x)\n",
    "        \n",
    "        # Final layer norm and head\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        # Loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=-1\n",
    "            )\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "model = GPT(vocab_size=100, block_size=32, n_layer=2, n_head=2, n_embd=64)\n",
    "idx = torch.randint(0, 100, (2, 10))\n",
    "logits, loss = model(idx)\n",
    "\n",
    "print(f\"âœ… GPT ëª¨ë¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
    "print(f\"   ì¶œë ¥ shape: {logits.shape}\")\n",
    "print(f\"   íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "\n",
    "### 5.1 ë¬¸ì ë‹¨ìœ„ í† í¬ë‚˜ì´ì €"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ“ ë¬¸ì ë‹¨ìœ„ í† í¬ë‚˜ì´ì €\n",
    "# ============================================================\n",
    "\n",
    "class CharTokenizer:\n",
    "    \"\"\"ê°„ë‹¨í•œ ë¬¸ì ë‹¨ìœ„ í† í¬ë‚˜ì´ì €\"\"\"\n",
    "    def __init__(self, text):\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.char2idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx2char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        self.vocab_size = len(self.chars)\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return torch.tensor([self.char2idx[ch] for ch in text])\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        if isinstance(tokens, torch.Tensor):\n",
    "            tokens = tokens.tolist()\n",
    "        return ''.join([self.idx2char[idx] for idx in tokens])\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "text = \"hello world\"\n",
    "tokenizer = CharTokenizer(text)\n",
    "\n",
    "print(f\"ğŸ“ ì–´íœ˜ í¬ê¸°: {tokenizer.vocab_size}\")\n",
    "print(f\"ğŸ“ ë¬¸ì: {tokenizer.chars}\")\n",
    "print(f\"ğŸ“ 'hello' ì¸ì½”ë”©: {tokenizer.encode('hello')}\")\n",
    "print(f\"ğŸ“ ë””ì½”ë”©: '{tokenizer.decode(tokenizer.encode('hello'))}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ² í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜\n",
    "# ============================================================\n",
    "\n",
    "def generate(model, tokenizer, prompt, max_tokens=100, temperature=1.0):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ ìƒì„±\n",
    "    \n",
    "    Args:\n",
    "        model: GPT ëª¨ë¸\n",
    "        tokenizer: í† í¬ë‚˜ì´ì €\n",
    "        prompt: ì‹œì‘ í…ìŠ¤íŠ¸\n",
    "        max_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜\n",
    "        temperature: ìƒ˜í”Œë§ ì˜¨ë„ (ë‚®ì„ìˆ˜ë¡ ë³´ìˆ˜ì )\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt).unsqueeze(0)  # (1, T)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_tokens):\n",
    "            # ë‹¤ìŒ í† í° ì˜ˆì¸¡\n",
    "            logits, _ = model(tokens)\n",
    "            logits = logits[0, -1, :] / temperature  # (vocab_size,)\n",
    "            \n",
    "            # ìƒ˜í”Œë§\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # í† í° ì¶”ê°€\n",
    "            tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            # EOS ì²´í¬ (ì„ íƒì‚¬í•­)\n",
    "            if next_token.item() == tokenizer.char2idx.get('.', -1):\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(tokens[0])\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ í›ˆë ¨\n",
    "def simple_training_example():\n",
    "    \"\"\"ê°„ë‹¨í•œ í›ˆë ¨ ì˜ˆì œ\"\"\"\n",
    "    # ë°ì´í„°\n",
    "    text = \"The cat sat on the mat. The dog sat on the log. \" * 100\n",
    "    tokenizer = CharTokenizer(text)\n",
    "    \n",
    "    # ëª¨ë¸\n",
    "    model = GPT(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        block_size=64,\n",
    "        n_layer=2,\n",
    "        n_head=2,\n",
    "        n_embd=64\n",
    "    )\n",
    "    \n",
    "    # ì˜µí‹°ë§ˆì´ì €\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # í›ˆë ¨\n",
    "    tokens = tokenizer.encode(text)\n",
    "    print(\"\\nğŸ¯ í›ˆë ¨ ì‹œì‘...\")\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        # ëœë¤ ë°°ì¹˜ ìƒ˜í”Œë§\n",
    "        start_idx = torch.randint(0, len(tokens) - 64, (1,)).item()\n",
    "        x = tokens[start_idx:start_idx + 64].unsqueeze(0)\n",
    "        y = tokens[start_idx + 1:start_idx + 65].unsqueeze(0)\n",
    "        \n",
    "        # Forward & Backward\n",
    "        logits, loss = model(x, targets=y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"   Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# ì‹¤í–‰\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ² í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "model, tokenizer = simple_training_example()\n",
    "\n",
    "# ìƒì„±\n",
    "generated = generate(model, tokenizer, \"The cat\", max_tokens=20, temperature=0.8)\n",
    "print(f\"\\nğŸ“ ìƒì„± ê²°ê³¼: '{generated}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Decision Transformerì™€ì˜ ì—°ê²°\n\n### 6.1 GPT-2 â†’ Decision Transformer ë³€í™˜\n\n```mermaid\nflowchart TB\n    subgraph GPT[\"GPT-2 (ì–¸ì–´ ëª¨ë¸ë§)\"]\n        direction TB\n        GIn[\"Token IDs<br/>(B, T)\"]\n        GEmb[\"Token Embedding<br/>(vocab â†’ n_embd)\"]\n        GTrans[\"Transformer<br/>Causal Attention\"]\n        GHead[\"Vocab Head<br/>(n_embd â†’ vocab)\"]\n        GOut[\"Next Token\"]\n    end\n\n    subgraph DT[\"Decision Transformer (RL)\"]\n        direction TB\n        DIn[\"(R, s, a) tuples<br/>(B, T, 3, dim)\"]\n        DEmb[\"3ê°œ Embedding<br/>RTG, State, Action\"]\n        DTrans[\"Transformer<br/>Causal Attention\"]\n        DHead[\"Action Head<br/>(n_embd â†’ act_dim)\"]\n        DOut[\"Next Action\"]\n    end\n\n    GIn --> GEmb --> GTrans --> GHead --> GOut\n    DIn --> DEmb --> DTrans --> DHead --> DOut\n\n    GEmb -.->|\"ë³€í™˜\"| DEmb\n    GHead -.->|\"ë³€í™˜\"| DHead\n\n    style GPT fill:#e3f2fd\n    style DT fill:#c8e6c9\n    style GTrans fill:#bbdefb\n    style DTrans fill:#a5d6a7\n```\n\n### 6.2 ì£¼ìš” ì°¨ì´ì  ë¹„êµ\n\n```mermaid\nflowchart LR\n    subgraph Aspect[\"ì¸¡ë©´\"]\n        A1[\"ì…ë ¥\"]\n        A2[\"ì„ë² ë”©\"]\n        A3[\"ìœ„ì¹˜ ì •ë³´\"]\n        A4[\"ì¶œë ¥\"]\n        A5[\"ì†ì‹¤\"]\n    end\n\n    subgraph GPT2[\"GPT-2\"]\n        G1[\"í† í° IDs\"]\n        G2[\"Token Embedding\"]\n        G3[\"Positional Embedding\"]\n        G4[\"ë‹¤ìŒ í† í° ë¶„í¬\"]\n        G5[\"Cross Entropy\"]\n    end\n\n    subgraph DT[\"Decision Transformer\"]\n        D1[\"(R, s, a) íŠœí”Œ\"]\n        D2[\"3ê°œ Embedding\"]\n        D3[\"Timestep Embedding\"]\n        D4[\"ë‹¤ìŒ Action (íšŒê·€/ë¶„ë¥˜)\"]\n        D5[\"MSE / Cross Entropy\"]\n    end\n\n    A1 --> G1\n    A1 --> D1\n    A2 --> G2\n    A2 --> D2\n    A3 --> G3\n    A3 --> D3\n    A4 --> G4\n    A4 --> D4\n    A5 --> G5\n    A5 --> D5\n\n    style GPT2 fill:#e3f2fd\n    style DT fill:#c8e6c9\n```\n\n### 6.3 DT ì‹œí€€ìŠ¤ êµ¬ì„± ìƒì„¸\n\n```mermaid\nflowchart TB\n    subgraph Input[\"ì›ë³¸ ë°ì´í„°\"]\n        RTG[\"RTG<br/>(B, T, 1)\"]\n        State[\"States<br/>(B, T, state_dim)\"]\n        Action[\"Actions<br/>(B, T, act_dim)\"]\n    end\n\n    subgraph Embed[\"ì„ë² ë”©\"]\n        RE[\"RTG Linear<br/>â†’ (B, T, hidden)\"]\n        SE[\"State Linear<br/>â†’ (B, T, hidden)\"]\n        AE[\"Action Linear<br/>â†’ (B, T, hidden)\"]\n    end\n\n    subgraph Timestep[\"Timestep ì¶”ê°€\"]\n        TE[\"Timestep Embedding<br/>â†’ (B, T, hidden)\"]\n        Add1[\"RTG + Timestep\"]\n        Add2[\"State + Timestep\"]\n        Add3[\"Action + Timestep\"]\n    end\n\n    subgraph Stack[\"ì‹œí€€ìŠ¤ ìŠ¤íƒ\"]\n        Stack[\"stack([R, s, a], dim=1)<br/>â†’ (B, 3, T, hidden)\"]\n    end\n\n    subgraph Reshape[\"ì¬ë°°ì¹˜\"]\n        Reshape[\"permute(0,2,1,3)<br/>reshape(B, 3*T, hidden)\"]\n    end\n\n    subgraph Output[\"ìµœì¢… ì‹œí€€ìŠ¤\"]\n        Seq[\"[Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚, aâ‚, ...]<br/>shape: (B, 3*T, hidden)\"]\n    end\n\n    RTG --> RE --> Add1\n    State --> SE --> Add2\n    Action --> AE --> Add3\n    TE --> Add1\n    TE --> Add2\n    TE --> Add3\n    Add1 --> Stack\n    Add2 --> Stack\n    Add3 --> Stack --> Reshape --> Seq\n\n    style Seq fill:#c8e6c9\n    style Reshape fill:#fff9c4\n```\n\n### 6.4 ì¶”ë¡  ì‹œ RTG ì—…ë°ì´íŠ¸\n\n```mermaid\nsequenceDiagram\n    participant User as ğŸ¯ ì‚¬ìš©ì\n    participant Model as ğŸ§  DT Model\n    participant Env as ğŸŒ í™˜ê²½\n    participant RTG as RTG ë³€ìˆ˜\n\n    Note over User,RTG: ì¶”ë¡  ì‹œì‘\n    User->>Model: target_return = 100\n    User->>RTG: rtg = 100\n\n    Model->>Env: actionâ‚€ = model(state, rtg=100)\n    Env-->>Model: reward = 10\n    Note over Model: rtg = 100 - 10 = 90\n\n    Model->>Env: actionâ‚ = model(state, rtg=90)\n    Env-->>Model: reward = 20\n    Note over Model: rtg = 90 - 20 = 70\n\n    Model->>Env: actionâ‚‚ = model(state, rtg=70)\n    Note over Model: ...ê³„ì†...\n\n    Note over Env: rtg â‰ˆ 0 â†’ ëª©í‘œ ë‹¬ì„±!\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ”„ Decision Transformer ìŠ¤íƒ€ì¼ ì‹œí€€ìŠ¤ êµ¬ì„±\n",
    "# ============================================================\n",
    "\n",
    "def create_dt_sequence(batch_size, seq_len, state_dim, act_dim, hidden_dim):\n",
    "    \"\"\"\n",
    "    Decision Transformerìš© ì‹œí€€ìŠ¤ êµ¬ì„± ì˜ˆì‹œ\n",
    "    \n",
    "    GPT-2: [token_0, token_1, token_2, ...]\n",
    "    DT:     [R_0, s_0, a_0, R_1, s_1, a_1, ...]\n",
    "    \"\"\"\n",
    "    # ê°€ì§œ ë°ì´í„°\n",
    "    rtg = torch.randn(batch_size, seq_len, 1)\n",
    "    states = torch.randn(batch_size, seq_len, state_dim)\n",
    "    actions = torch.randn(batch_size, seq_len, act_dim)\n",
    "    \n",
    "    # ì„ë² ë”© ë ˆì´ì–´\n",
    "    rtg_linear = nn.Linear(1, hidden_dim)\n",
    "    state_linear = nn.Linear(state_dim, hidden_dim)\n",
    "    action_linear = nn.Linear(act_dim, hidden_dim)\n",
    "    \n",
    "    # ì„ë² ë”©\n",
    "    rtg_emb = rtg_linear(rtg)       # (B, T, hidden)\n",
    "    state_emb = state_linear(states)\n",
    "    action_emb = action_linear(actions)\n",
    "    \n",
    "    # ì‹œí€€ìŠ¤ ìŠ¤íƒ: (B, 3, T, hidden)\n",
    "    stacked = torch.stack([rtg_emb, state_emb, action_emb], dim=1)\n",
    "    \n",
    "    # ì¬ë°°ì¹˜: (B, 3*T, hidden)\n",
    "    sequence = stacked.permute(0, 2, 1, 3).reshape(batch_size, 3 * seq_len, hidden_dim)\n",
    "    \n",
    "    print(f\"ğŸ“Š DT ì‹œí€€ìŠ¤ êµ¬ì„±:\")\n",
    "    print(f\"   RTG shape: {rtg.shape} -> ì„ë² ë”©: {rtg_emb.shape}\")\n",
    "    print(f\"   State shape: {states.shape} -> ì„ë² ë”©: {state_emb.shape}\")\n",
    "    print(f\"   Action shape: {actions.shape} -> ì„ë² ë”©: {action_emb.shape}\")\n",
    "    print(f\"   ìµœì¢… ì‹œí€€ìŠ¤: {sequence.shape}\")\n",
    "    print(f\"\\n   ì‹œí€€ìŠ¤ êµ¬ì¡°: [R_0, s_0, a_0, R_1, s_1, a_1, ...]\")\n",
    "    print(f\"   State ìœ„ì¹˜(1::3)ì—ì„œ Action ì˜ˆì¸¡!\")\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# ì‹¤í–‰\n",
    "seq = create_dt_sequence(batch_size=2, seq_len=5, state_dim=10, act_dim=4, hidden_dim=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìš”ì•½\n",
    "\n",
    "### âœ… í•™ìŠµí•œ ë‚´ìš©\n",
    "\n",
    "1. **Self-Attention**: ê° í† í°ì´ ë¬¸ë§³ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ë¥¸ í† í°ì— attend\n",
    "2. **Causal Masking**: ë¯¸ë˜ ì •ë³´ ëˆ„ì„¤ ë°©ì§€\n",
    "3. **Transformer Block**: Attention + MLP + Residual\n",
    "4. **GPT-2**: Token/Positional Embedding + Blocks + Head\n",
    "5. **DT ì—°ê²°**: (R, s, a) ì‹œí€€ìŠ¤ë¡œ RL ì¬êµ¬ì„±\n",
    "\n",
    "### ğŸ“š ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "- [ ] Decision Transformer ì‹¤ìŠµ (`phase3_gym_practice.ipynb`)\n",
    "- [ ] Atari í™˜ê²½ ì‹¤ìŠµ (`phase4_atari_practice.ipynb`)\n",
    "- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‹¤í—˜\n",
    "\n",
    "---\n",
    "\n",
    "### ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- [Language Models are Unsupervised Multitask Learners (GPT-2)](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)\n",
    "- [annotated transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}