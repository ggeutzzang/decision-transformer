{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ“ Phase 1: ë°°ê²½ ì§€ì‹\n\n> **ëª©í‘œ**: Decision Transformerë¥¼ ì´í•´í•˜ê¸° ìœ„í•œ **ê°•í™”í•™ìŠµ**ê³¼ **Transformer**ì˜ ê¸°ì´ˆ ê°œë…ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n\n---\n\n## ğŸ“š ì´ ë…¸íŠ¸ë¶ì—ì„œ ë°°ìš°ëŠ” ê²ƒ\n\n| ì„¹ì…˜ | í•µì‹¬ ê°œë… | ì™œ í•„ìš”í•œê°€? |\n|:---:|:---|:---|\n| **1** | ê°•í™”í•™ìŠµ ê¸°ì´ˆ | DTëŠ” RL ë¬¸ì œë¥¼ í‘¸ëŠ” ëª¨ë¸ì´ë¯€ë¡œ |\n| **2** | Transformer ê¸°ì´ˆ | DTì˜ í•µì‹¬ ì•„í‚¤í…ì²˜ì´ë¯€ë¡œ |\n| **3** | ì—°ìŠµ ë¬¸ì œ | ì´í•´ë„ í™•ì¸ |\n\n---\n\n## ğŸ—ºï¸ Decision Transformer ì „ì²´ ê·¸ë¦¼\n\n```mermaid\nflowchart TB\n    subgraph Traditional[\"ğŸ® ê¸°ì¡´ ê°•í™”í•™ìŠµ\"]\n        S1[\"ìƒíƒœ (State)\"] -->|\"ì´ ìƒí™©ì—ì„œ ë­˜ í•´ì•¼ í•´?\"| A1[\"í–‰ë™ (Action)\"]\n    end\n\n    subgraph DT[\"ğŸ¤– Decision Transformer\"]\n        RTG[\"ëª©í‘œ<br/>(RTG)\"] --> TF[\"Transformer\"]\n        State[\"ìƒíƒœ<br/>(State)\"] --> TF\n        TF -->|\"100ì ì„ ì–»ìœ¼ë ¤ë©´<br/>ë­˜ í•´ì•¼ í•´?\"| Action[\"í–‰ë™<br/>(Action)\"]\n    end\n\n    style RTG fill:#e1f5fe,stroke:#01579b\n    style State fill:#fff3e0,stroke:#e65100\n    style Action fill:#e8f5e9,stroke:#1b5e20\n    style TF fill:#f3e5f5,stroke:#4a148c\n```\n\n---\n\n## ğŸ“‹ ëª©ì°¨\n\n1. [ğŸ”§ í™˜ê²½ ì„¤ì •](#0-í™˜ê²½-ì„¤ì •)\n2. [ğŸ® ê°•í™”í•™ìŠµ ê¸°ì´ˆ](#1-ê°•í™”í•™ìŠµ-ê¸°ì´ˆ)\n   - í•µì‹¬ ìš©ì–´ (State, Action, Reward)\n   - Returnê³¼ Discount Factor\n   - Policy (ì •ì±…)\n   - Trajectory (ê¶¤ì )\n3. [ğŸ¤– Transformer ê¸°ì´ˆ](#2-transformer-ê¸°ì´ˆ)\n   - Self-Attention ë©”ì»¤ë‹ˆì¦˜\n   - Causal Masking\n   - Positional Encoding\n   - Autoregressive ìƒì„±\n4. [âœï¸ ì—°ìŠµ ë¬¸ì œ](#3-ì—°ìŠµ-ë¬¸ì œ)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# ğŸ”§ 0. í™˜ê²½ ì„¤ì •\n\nmatplotlibì—ì„œ í•œê¸€ì„ í‘œì‹œí•˜ë ¤ë©´ í°íŠ¸ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.\n\n> âš ï¸ **ì°¸ê³ **: í•œê¸€ í°íŠ¸ê°€ ì—†ì–´ë„ ë…¸íŠ¸ë¶ ì‹¤í–‰ì—ëŠ” ë¬¸ì œ ì—†ìŠµë‹ˆë‹¤. ê·¸ë˜í”„ ë ˆì´ë¸”ë§Œ ì˜ì–´ë¡œ í‘œì‹œë©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í•œê¸€ í°íŠ¸ ì„¤ì • ë° ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# í•œê¸€ í°íŠ¸ ì„¤ì •\ndef setup_korean_font():\n    \"\"\"ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì•„ ì„¤ì •\"\"\"\n    # ìì£¼ ì‚¬ìš©ë˜ëŠ” í•œê¸€ í°íŠ¸ ëª©ë¡\n    korean_fonts = [\n        'NanumGothic', 'NanumBarunGothic', 'Malgun Gothic', \n        'AppleGothic', 'Noto Sans CJK KR', 'Noto Sans KR',\n        'UnDotum', 'Baekmuk Gulim'\n    ]\n    \n    # ì‹œìŠ¤í…œì— ì„¤ì¹˜ëœ í°íŠ¸ ëª©ë¡\n    system_fonts = [f.name for f in fm.fontManager.ttflist]\n    \n    # ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ ì°¾ê¸°\n    for font in korean_fonts:\n        if font in system_fonts:\n            plt.rcParams['font.family'] = font\n            plt.rcParams['axes.unicode_minus'] = False\n            print(f\"Korean font set: {font}\")\n            return font\n    \n    # í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°\n    print(\"Warning: Korean font not found. Labels will be in English.\")\n    plt.rcParams['axes.unicode_minus'] = False\n    return None\n\n# í°íŠ¸ ì„¤ì • ì‹¤í–‰\nkorean_font = setup_korean_font()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# ğŸ® 1. ê°•í™”í•™ìŠµ ê¸°ì´ˆ\n\n## 1.1 í•µì‹¬ ìš©ì–´\n\nê°•í™”í•™ìŠµ(Reinforcement Learning)ì€ **ì—ì´ì „íŠ¸**ê°€ **í™˜ê²½**ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° **ë³´ìƒ**ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ëŠ” ë¶„ì•¼ì…ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    Agent[\"ğŸ¤– ì—ì´ì „íŠ¸ (Agent)<br/><i>í•™ìŠµí•˜ëŠ” ì£¼ì²´</i><br/><small>ì˜ˆ: ê²Œì„ í”Œë ˆì´ì–´</small>\"]\n    Env[\"ğŸŒ í™˜ê²½ (Environment)<br/><i>ìƒí˜¸ì‘ìš© ëŒ€ìƒ</i><br/><small>ì˜ˆ: ê²Œì„ ì„¸ê³„</small>\"]\n\n    Agent -->|\"í–‰ë™ (Action)\"| Env\n    Env -->|\"ìƒíƒœ (State)\"| Agent\n    Env -->|\"ë³´ìƒ (Reward)\"| Agent\n\n    style Agent fill:#bbdefb,stroke:#1565c0\n    style Env fill:#c8e6c9,stroke:#2e7d32\n```\n\n### ğŸ“Œ í•µì‹¬ ìš©ì–´ ì •ë¦¬\n\n| ìš©ì–´ | ì˜ë¬¸ | ì„¤ëª… | ê²Œì„ ì˜ˆì‹œ |\n|:---:|:---:|:---|:---|\n| **ìƒíƒœ** | State (s) | í™˜ê²½ì˜ í˜„ì¬ ìƒí™© | ê²Œì„ í™”ë©´, ìºë¦­í„° ìœ„ì¹˜ |\n| **í–‰ë™** | Action (a) | ì—ì´ì „íŠ¸ê°€ ì·¨í•˜ëŠ” ë™ì‘ | ì í”„, ì´ë™, ê³µê²© |\n| **ë³´ìƒ** | Reward (r) | í–‰ë™ì˜ ê²°ê³¼ë¡œ ë°›ëŠ” í”¼ë“œë°± | +10 (ì½”ì¸), -1 (í”¼í•´) |\n| **ì •ì±…** | Policy (Ï€) | ìƒíƒœì—ì„œ í–‰ë™ì„ ì„ íƒí•˜ëŠ” ê·œì¹™ | \"ì ì´ ë³´ì´ë©´ ê³µê²©\" |\n| **ì—í”¼ì†Œë“œ** | Episode | ì‹œì‘ë¶€í„° ì¢…ë£Œê¹Œì§€ í•œ ë²ˆì˜ ì‹¤í–‰ | ê²Œì„ í•œ íŒ |\n\n### ğŸ¯ ì§ê´€ì  ì´í•´\n\n> **ë¹„ìœ **: ê°•í™”í•™ìŠµì€ ì•„ì´ê°€ ê±·ê¸°ë¥¼ ë°°ìš°ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.\n> - **ìƒíƒœ**: í˜„ì¬ ìì„¸ (ì„œ ìˆìŒ, ë„˜ì–´ì§, ê¸°ìš¸ì–´ì§)\n> - **í–‰ë™**: ë‹¤ë¦¬ë¥¼ ì›€ì§ì´ëŠ” ë°©ì‹\n> - **ë³´ìƒ**: ì„±ê³µì ìœ¼ë¡œ í•œ ê±¸ìŒ (+1), ë„˜ì–´ì§ (-1)\n> - **ì •ì±…**: ì–´ë–¤ ìì„¸ì—ì„œ ì–´ë–»ê²Œ ë‹¤ë¦¬ë¥¼ ì›€ì§ì¼ì§€ì— ëŒ€í•œ ê·œì¹™\n\nì•„ë˜ ì½”ë“œì—ì„œ ê°„ë‹¨í•œ í™˜ê²½ì„ ë§Œë“¤ì–´ ì§ì ‘ í™•ì¸í•´ë´…ì‹œë‹¤! ğŸ‘‡"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# ğŸ® ê°„ë‹¨í•œ ê·¸ë¦¬ë“œ ì›”ë“œ í™˜ê²½ ë§Œë“¤ê¸°\n# ============================================================================\n# \n# ì´ ì½”ë“œëŠ” ì•„ì£¼ ê°„ë‹¨í•œ 1ì°¨ì› ê²Œì„ í™˜ê²½ì„ ë§Œë“­ë‹ˆë‹¤.\n# \n# ê²Œì„ ê·œì¹™:\n#   - ì—ì´ì „íŠ¸ëŠ” ìœ„ì¹˜ 0ì—ì„œ ì‹œì‘\n#   - ëª©í‘œëŠ” ìœ„ì¹˜ 5ì— ë„ë‹¬í•˜ëŠ” ê²ƒ\n#   - í–‰ë™: ì™¼ìª½(0) ë˜ëŠ” ì˜¤ë¥¸ìª½(1)ìœ¼ë¡œ ì´ë™\n#   - ë³´ìƒ: ëª©í‘œ ë„ë‹¬ ì‹œ +10, ê·¸ ì™¸ì—ëŠ” -1\n#\n# ì‹œê°í™”:\n#   ì‹œì‘                              ëª©í‘œ\n#     â†“                                 â†“\n#   [ 0 ]---[ 1 ]---[ 2 ]---[ 3 ]---[ 4 ]---[ 5 ]\n#     â—                                       â­\n#\n# ============================================================================\n\nimport random\n\nclass SimpleGridWorld:\n    \"\"\"\n    ê°„ë‹¨í•œ 1ì°¨ì› ê·¸ë¦¬ë“œ ì›”ë“œ í™˜ê²½\n    \n    ì—ì´ì „íŠ¸ê°€ ì™¼ìª½/ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™í•˜ì—¬ ëª©í‘œ ì§€ì ì— ë„ë‹¬í•˜ëŠ” ê²Œì„ì…ë‹ˆë‹¤.\n    \"\"\"\n    def __init__(self):\n        self.position = 0  # ì‹œì‘ ìœ„ì¹˜\n        self.goal = 5      # ëª©í‘œ ìœ„ì¹˜\n        \n    def reset(self):\n        \"\"\"ì—í”¼ì†Œë“œ ì‹œì‘: ìœ„ì¹˜ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”\"\"\"\n        self.position = 0\n        return self.position  # í˜„ì¬ ìƒíƒœ(state) ë°˜í™˜\n    \n    def step(self, action):\n        \"\"\"\n        í•œ ìŠ¤í… ì‹¤í–‰: í–‰ë™ì„ ë°›ì•„ì„œ í™˜ê²½ì„ ì—…ë°ì´íŠ¸\n        \n        Args:\n            action: 0 = ì™¼ìª½ìœ¼ë¡œ ì´ë™, 1 = ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™\n            \n        Returns:\n            state: ìƒˆë¡œìš´ ìœ„ì¹˜\n            reward: ë°›ì€ ë³´ìƒ\n            done: ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€\n        \"\"\"\n        # í–‰ë™ì— ë”°ë¼ ìœ„ì¹˜ ë³€ê²½\n        if action == 0:  # ì™¼ìª½\n            self.position = max(0, self.position - 1)  # 0ë³´ë‹¤ ì‘ì•„ì§€ì§€ ì•Šë„ë¡\n        else:  # ì˜¤ë¥¸ìª½\n            self.position = min(self.goal, self.position + 1)  # ëª©í‘œë¥¼ ë„˜ì§€ ì•Šë„ë¡\n        \n        # ë³´ìƒ ê³„ì‚°\n        if self.position == self.goal:\n            reward = 10   # ğŸ‰ ëª©í‘œ ë„ë‹¬! í° ë³´ìƒ\n            done = True   # ì—í”¼ì†Œë“œ ì¢…ë£Œ\n        else:\n            reward = -1   # ğŸ˜“ ì•„ì§ ë„ë‹¬ ëª»í•¨, ì‘ì€ í˜ë„í‹°\n            done = False  # ê³„ì† ì§„í–‰\n            \n        return self.position, reward, done\n\n# ============================================================================\n# ğŸ§ª í™˜ê²½ í…ŒìŠ¤íŠ¸í•˜ê¸°\n# ============================================================================\n\nprint(\"=\" * 60)\nprint(\"ğŸ® ê·¸ë¦¬ë“œ ì›”ë“œ í™˜ê²½ í…ŒìŠ¤íŠ¸\")\nprint(\"=\" * 60)\nprint()\n\n# í™˜ê²½ ìƒì„± ë° ì´ˆê¸°í™”\nenv = SimpleGridWorld()\nstate = env.reset()\n\nprint(f\"ğŸ“ ì‹œì‘ ìƒíƒœ: {state}\")\nprint(f\"ğŸ¯ ëª©í‘œ ìœ„ì¹˜: {env.goal}\")\nprint()\nprint(\"â–¶ï¸ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê³„ì† ì´ë™í•´ë´…ì‹œë‹¤!\")\nprint(\"-\" * 60)\n\n# ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê³„ì† ì´ë™\ntotal_reward = 0\nfor step in range(10):\n    action = 1  # ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™\n    state, reward, done = env.step(action)\n    total_reward += reward\n    \n    # í˜„ì¬ ìƒíƒœë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œ\n    grid = ['[ ]'] * 6\n    grid[state] = '[â—]'  # í˜„ì¬ ìœ„ì¹˜ í‘œì‹œ\n    grid_str = '---'.join(grid)\n    \n    print(f\"Step {step+1}: í–‰ë™={'â†’ì˜¤ë¥¸ìª½':8s} | ìœ„ì¹˜={state} | ë³´ìƒ={reward:+3d} | {grid_str}\")\n    \n    if done:\n        print()\n        print(\"ğŸ‰ ëª©í‘œ ë„ë‹¬!\")\n        break\n\nprint()\nprint(f\"ğŸ“Š ì´ ë³´ìƒ (Return): {total_reward}\")\nprint()\nprint(\"ğŸ’¡ í•´ì„: 5ë²ˆ ì´ë™(-5)í•˜ê³  ëª©í‘œ ë„ë‹¬(+10) = ì´ +6ì \")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 1.2 Returnê³¼ Discount Factor\n\n### ğŸ“– Return (ë°˜í™˜ê°’)ì´ë€?\n\n**Return**ì€ ì—í”¼ì†Œë“œì—ì„œ ì–»ì€ **ì´ ë³´ìƒì˜ í•©**ì…ë‹ˆë‹¤. ê¸°í˜¸ë¡œëŠ” **G**ë¡œ í‘œê¸°í•©ë‹ˆë‹¤.\n\n$$G_t = r_t + r_{t+1} + r_{t+2} + \\cdots + r_T$$\n\n> ğŸ’¡ **ì‰½ê²Œ ë§í•˜ë©´**: \"ê²Œì„ í•œ íŒì—ì„œ ì–»ì€ ì´ ì ìˆ˜\"\n\n---\n\n### ğŸ”¢ Discount Factor (í• ì¸ìœ¨)ì´ë€?\n\n**í• ì¸ìœ¨ Î³(ê°ë§ˆ)**ëŠ” **ë¯¸ë˜ ë³´ìƒì˜ ê°€ì¹˜ë¥¼ ì–¼ë§ˆë‚˜ ë‚®ì¶œì§€** ê²°ì •í•˜ëŠ” ê°’ì…ë‹ˆë‹¤ (0 ~ 1 ì‚¬ì´).\n\n$$G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}$$\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     í• ì¸ìœ¨(Î³)ì˜ ì˜ë¯¸                                â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                     â”‚\nâ”‚   Î³ = 1.0  â†’  \"ë¯¸ë˜ ë³´ìƒë„ ì§€ê¸ˆë§Œí¼ ì¤‘ìš”í•´\"                        â”‚\nâ”‚                ë¯¸ë˜ ë³´ìƒì„ í• ì¸ ì—†ì´ ê·¸ëŒ€ë¡œ ê³„ì‚°                    â”‚\nâ”‚                                                                     â”‚\nâ”‚   Î³ = 0.9  â†’  \"ë¯¸ë˜ ë³´ìƒì€ ì¡°ê¸ˆ ëœ ì¤‘ìš”í•´\" (ì¼ë°˜ì  ì„¤ì •)           â”‚\nâ”‚                10ìŠ¤í… í›„ ë³´ìƒì€ 0.9^10 â‰ˆ 0.35ë°°ë¡œ í• ì¸              â”‚\nâ”‚                                                                     â”‚\nâ”‚   Î³ = 0.5  â†’  \"ê°€ê¹Œìš´ ë¯¸ë˜ë§Œ ì‹ ê²½ ì¨\"                              â”‚\nâ”‚                3ìŠ¤í… í›„ ë³´ìƒì€ 0.5^3 = 0.125ë°°ë¡œ í• ì¸               â”‚\nâ”‚                                                                     â”‚\nâ”‚   Î³ = 0.0  â†’  \"ì§€ê¸ˆ ë‹¹ì¥ì˜ ë³´ìƒë§Œ ì¤‘ìš”í•´\"                          â”‚\nâ”‚                ë¯¸ë˜ ë³´ìƒì€ ì™„ì „íˆ ë¬´ì‹œ                              â”‚\nâ”‚                                                                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### ğŸ¤” ì™œ í• ì¸ìœ¨ì„ ì‚¬ìš©í•˜ë‚˜ìš”?\n\n1. **ìˆ˜í•™ì  ì•ˆì •ì„±**: ë¬´í•œíˆ ê¸´ ì—í”¼ì†Œë“œì—ì„œë„ Returnì´ ë°œì‚°í•˜ì§€ ì•ŠìŒ\n2. **í˜„ì‹¤ ë°˜ì˜**: ì§€ê¸ˆ ë°›ëŠ” 1000ì› vs 10ë…„ í›„ ë°›ëŠ” 1000ì› â†’ ì§€ê¸ˆì´ ë” ê°€ì¹˜ ìˆìŒ\n3. **ë¶ˆí™•ì‹¤ì„±**: ë¯¸ë˜ëŠ” ë¶ˆí™•ì‹¤í•˜ë¯€ë¡œ í˜„ì¬ ë³´ìƒì„ ë” ì‹ ë¢°\n\nì•„ë˜ ì½”ë“œì—ì„œ ì§ì ‘ í™•ì¸í•´ë´…ì‹œë‹¤! ğŸ‘‡"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# ğŸ“Š Return ê³„ì‚° ì˜ˆì‹œ\n# ============================================================================\n#\n# 5ê°œì˜ ìŠ¤í…ì—ì„œ ê°ê° [1, 2, 3, 4, 5]ì˜ ë³´ìƒì„ ë°›ì•˜ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n# í• ì¸ìœ¨(Î³)ì— ë”°ë¼ ì´ Returnì´ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ í™•ì¸í•´ë´…ì‹œë‹¤.\n#\n# ============================================================================\n\nrewards = [1, 2, 3, 4, 5]  # ê° ìŠ¤í…ì˜ ë³´ìƒ\n\nprint(\"=\" * 60)\nprint(\"ğŸ“Š Return ê³„ì‚° ì˜ˆì‹œ\")\nprint(\"=\" * 60)\nprint(f\"\\në³´ìƒ ì‹œí€€ìŠ¤: {rewards}\")\nprint()\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 1. ë‹¨ìˆœ Return (Î³=1): ëª¨ë“  ë³´ìƒì„ ê·¸ëŒ€ë¡œ í•©ì‚°\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nsimple_return = sum(rewards)\nprint(f\"1ï¸âƒ£ ë‹¨ìˆœ Return (Î³=1): {rewards[0]} + {rewards[1]} + {rewards[2]} + {rewards[3]} + {rewards[4]} = {simple_return}\")\nprint()\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 2. Discounted Return: ë¯¸ë˜ ë³´ìƒì— Î³^të¥¼ ê³±í•¨\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef calculate_discounted_return(rewards, gamma):\n    \"\"\"\n    í• ì¸ëœ Return ê³„ì‚°\n    \n    ê³µì‹: G = râ‚€ + Î³Â·râ‚ + Î³Â²Â·râ‚‚ + Î³Â³Â·râ‚ƒ + ...\n    \"\"\"\n    discounted = 0\n    for t, r in enumerate(rewards):\n        discounted += (gamma ** t) * r  # Î³^t Ã— r_t\n    return discounted\n\nprint(\"2ï¸âƒ£ Discounted Return (ë‹¤ì–‘í•œ Î³ ê°’):\")\nprint(\"-\" * 60)\nprint(f\"{'Î³':>6} | {'ê³„ì‚°ì‹':^35} | {'ê²°ê³¼':>8}\")\nprint(\"-\" * 60)\n\nfor gamma in [0.0, 0.5, 0.9, 0.99, 1.0]:\n    g = calculate_discounted_return(rewards, gamma)\n    \n    # ê³„ì‚° ê³¼ì • ë¬¸ìì—´ ìƒì„±\n    terms = [f\"{gamma**i:.2f}Ã—{r}\" for i, r in enumerate(rewards[:3])]\n    formula = \" + \".join(terms) + \" + ...\"\n    \n    print(f\"{gamma:>6.2f} | {formula:35} | {g:>8.2f}\")\n\nprint()\nprint(\"ğŸ’¡ ê´€ì°°:\")\nprint(\"   - Î³=0: ì²« ë²ˆì§¸ ë³´ìƒ(1)ë§Œ ê³„ì‚°ë¨\")\nprint(\"   - Î³=1: ëª¨ë“  ë³´ìƒì´ ë™ì¼í•œ ê°€ì¹˜ë¡œ í•©ì‚°ë¨\")\nprint(\"   - Î³ê°€ í´ìˆ˜ë¡ ë¯¸ë˜ ë³´ìƒë„ ì¤‘ìš”í•˜ê²Œ ë°˜ì˜ë¨\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# ğŸ“ˆ ì‹œê°í™”: Discount Factorì˜ íš¨ê³¼\n# ============================================================================\n#\n# ì´ ê·¸ë˜í”„ëŠ” ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ë³´ìƒì˜ \"ê°€ì¹˜(weight)\"ê°€\n# Î³ ê°’ì— ë”°ë¼ ì–´ë–»ê²Œ ê°ì†Œí•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.\n#\n# Yì¶• ê°’ì´ ë†’ì„ìˆ˜ë¡ í•´ë‹¹ ì‹œì ì˜ ë³´ìƒì´ ë” ì¤‘ìš”í•˜ê²Œ ë°˜ì˜ë©ë‹ˆë‹¤.\n#\n# ============================================================================\n\nimport numpy as np\n\ntimesteps = np.arange(20)  # 0~19 ìŠ¤í…\ngammas = [0.5, 0.9, 0.99, 1.0]  # ë‹¤ì–‘í•œ Î³ ê°’\n\nplt.figure(figsize=(12, 5))\n\n# ê° Î³ì— ëŒ€í•´ weight ê³¡ì„  ê·¸ë¦¬ê¸°\nfor gamma in gammas:\n    weights = gamma ** timesteps  # Î³^t ê³„ì‚°\n    plt.plot(timesteps, weights, label=f'Î³={gamma}', linewidth=2.5, marker='o', markersize=4)\n\n# ê·¸ë˜í”„ ê¾¸ë¯¸ê¸°\nplt.xlabel('Timestep (ë¯¸ë˜ë¡œ ê°ˆìˆ˜ë¡ â†’)', fontsize=12)\nplt.ylabel('Weight (ë³´ìƒì˜ í˜„ì¬ ê°€ì¹˜)', fontsize=12)\nplt.title('ğŸ” í• ì¸ìœ¨(Î³)ì— ë”°ë¥¸ ë¯¸ë˜ ë³´ìƒì˜ ê°€ì¹˜ ë³€í™”', fontsize=14)\nplt.legend(fontsize=11, loc='upper right')\nplt.grid(True, alpha=0.3)\n\n# ì„¤ëª… í…ìŠ¤íŠ¸ ì¶”ê°€\nplt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\nplt.text(15, 0.55, 'â† ê°€ì¹˜ê°€ ì ˆë°˜ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” ì§€ì ', fontsize=10, color='gray')\n\nplt.tight_layout()\nplt.show()\n\n# í•´ì„ ì„¤ëª…\nprint()\nprint(\"ğŸ“– ê·¸ë˜í”„ í•´ì„:\")\nprint(\"=\" * 60)\nprint(\"â€¢ Î³=1.0 (íŒŒë€ì„ ): ë¯¸ë˜ ë³´ìƒë„ í˜„ì¬ì™€ ë™ì¼í•œ ê°€ì¹˜\")\nprint(\"â€¢ Î³=0.99 (ì£¼í™©ì„ ): ë§¤ìš° ì²œì²œíˆ ê°ì†Œ, ë¨¼ ë¯¸ë˜ë„ ì¤‘ìš”\")\nprint(\"â€¢ Î³=0.9 (ì´ˆë¡ì„ ): 10ìŠ¤í… í›„ ë³´ìƒì€ ì•½ 35% ê°€ì¹˜\")\nprint(\"â€¢ Î³=0.5 (ë¹¨ê°„ì„ ): ê¸‰ê²©íˆ ê°ì†Œ, ê°€ê¹Œìš´ ë¯¸ë˜ë§Œ ì¤‘ìš”\")\nprint()\nprint(\"ğŸ’¡ Decision Transformerì—ì„œëŠ” ì£¼ë¡œ Î³=1.0ì„ ì‚¬ìš©í•©ë‹ˆë‹¤!\")\nprint(\"   (í• ì¸ ì—†ì´ ìˆœìˆ˜ Return-to-Goë¥¼ ì‚¬ìš©)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 1.3 Policy (ì •ì±…)\n\n### ğŸ“– Policyë€?\n\n**Policy Ï€(a|s)**ëŠ” **\"ì–´ë–¤ ìƒíƒœì—ì„œ ì–´ë–¤ í–‰ë™ì„ í•  ê²ƒì¸ê°€\"**ë¥¼ ê²°ì •í•˜ëŠ” ê·œì¹™ì…ë‹ˆë‹¤.\n\n> ğŸ’¡ **ì‰½ê²Œ ë§í•˜ë©´**: \"ê²Œì„ ê³µëµì§‘\" ë˜ëŠ” \"í–‰ë™ ë§¤ë‰´ì–¼\"\n\n---\n\n### ğŸ”€ ë‘ ê°€ì§€ ìœ í˜•ì˜ Policy\n\n```mermaid\nflowchart LR\n    subgraph Det[\"1ï¸âƒ£ Deterministic Policy<br/><small>ê²°ì •ì  ì •ì±…</small>\"]\n        DS[\"ìƒíƒœ S\"] -->|\"a = Ï€(s)<br/>í•­ìƒ ê°™ì€ ê²°ê³¼\"| DA[\"í–‰ë™ A\"]\n    end\n\n    subgraph Sto[\"2ï¸âƒ£ Stochastic Policy<br/><small>í™•ë¥ ì  ì •ì±…</small>\"]\n        SS[\"ìƒíƒœ S\"] -->|\"70%\"| SA1[\"ê³µê²©\"]\n        SS -->|\"30%\"| SA2[\"ë°©ì–´\"]\n    end\n\n    style Det fill:#e3f2fd,stroke:#1565c0\n    style Sto fill:#fce4ec,stroke:#c2185b\n    style DA fill:#bbdefb\n    style SA1 fill:#f8bbd9\n    style SA2 fill:#f8bbd9\n```\n\n**Deterministic**: ê°™ì€ ìƒíƒœ â†’ í•­ìƒ ê°™ì€ í–‰ë™ (ì˜ˆ: \"ì ì´ ë³´ì´ë©´ â†’ ë¬´ì¡°ê±´ ê³µê²©\")\n\n**Stochastic**: ê°™ì€ ìƒíƒœ â†’ í™•ë¥ ì ìœ¼ë¡œ í–‰ë™ ì„ íƒ (ì˜ˆ: \"ì ì´ ë³´ì´ë©´ â†’ 70% ê³µê²©, 30% ë°©ì–´\")\n\n### ğŸ° íƒí—˜(Exploration) vs ì°©ì·¨(Exploitation)\n\nê°•í™”í•™ìŠµì˜ í•µì‹¬ ë”œë ˆë§ˆ:\n- **ì°©ì·¨**: í˜„ì¬ ì•Œê³  ìˆëŠ” ìµœì„ ì˜ í–‰ë™ì„ ì„ íƒ\n- **íƒí—˜**: ìƒˆë¡œìš´ í–‰ë™ì„ ì‹œë„í•´ì„œ ë” ì¢‹ì€ ë°©ë²• ë°œê²¬\n\n> **Îµ-Greedy ì •ì±…**ì´ ì´ ê· í˜•ì„ ë§ì¶¥ë‹ˆë‹¤:\n> - Îµ í™•ë¥ ë¡œ **ë¬´ì‘ìœ„ í–‰ë™** (íƒí—˜)\n> - (1-Îµ) í™•ë¥ ë¡œ **ìµœì„ ì˜ í–‰ë™** (ì°©ì·¨)\n\nì•„ë˜ ì½”ë“œì—ì„œ ë‹¤ì–‘í•œ ì •ì±…ì„ ë¹„êµí•´ë´…ì‹œë‹¤! ğŸ‘‡"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# ğŸ¯ ë‹¤ì–‘í•œ Policy êµ¬í˜„ ë° ë¹„êµ\n# ============================================================================\n#\n# ì„¸ ê°€ì§€ ë‹¤ë¥¸ ì •ì±…ì„ êµ¬í˜„í•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤:\n#   1. Random Policy: ë¬´ì‘ìœ„ë¡œ í–‰ë™ ì„ íƒ\n#   2. Greedy Policy: í•­ìƒ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™ (ëª©í‘œ ë°©í–¥)\n#   3. Îµ-Greedy Policy: ëŒ€ë¶€ë¶„ ì˜¤ë¥¸ìª½, ê°€ë” ë¬´ì‘ìœ„\n#\n# ============================================================================\n\nimport numpy as np\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# ğŸ² Policy 1: Random Policy (ë¬´ì‘ìœ„ ì •ì±…)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef random_policy(state):\n    \"\"\"\n    50% í™•ë¥ ë¡œ ì™¼ìª½, 50% í™•ë¥ ë¡œ ì˜¤ë¥¸ìª½\n    íƒí—˜ì€ ë§ì´ í•˜ì§€ë§Œ íš¨ìœ¨ì ì´ì§€ ì•ŠìŒ\n    \"\"\"\n    return np.random.choice([0, 1])  # 0: ì™¼ìª½, 1: ì˜¤ë¥¸ìª½\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# ğŸ¯ Policy 2: Greedy Policy (íƒìš•ì  ì •ì±…)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef greedy_policy(state):\n    \"\"\"\n    í•­ìƒ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™ (ëª©í‘œê°€ ì˜¤ë¥¸ìª½ì— ìˆìœ¼ë¯€ë¡œ)\n    ìµœì„ ì˜ í–‰ë™ë§Œ ì„ íƒ, íƒí—˜ ì—†ìŒ\n    \"\"\"\n    return 1  # í•­ìƒ ì˜¤ë¥¸ìª½\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# ğŸ° Policy 3: Îµ-Greedy Policy (ì…ì‹¤ë¡ -íƒìš• ì •ì±…)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef epsilon_greedy_policy(state, epsilon=0.1):\n    \"\"\"\n    Îµ í™•ë¥ ë¡œ ë¬´ì‘ìœ„ í–‰ë™ (íƒí—˜)\n    (1-Îµ) í™•ë¥ ë¡œ ìµœì„ ì˜ í–‰ë™ (ì°©ì·¨)\n    \n    Args:\n        state: í˜„ì¬ ìƒíƒœ\n        epsilon: íƒí—˜ í™•ë¥  (ê¸°ë³¸ 10%)\n    \"\"\"\n    if np.random.random() < epsilon:\n        return np.random.choice([0, 1])  # íƒí—˜: ë¬´ì‘ìœ„\n    else:\n        return 1  # ì°©ì·¨: ì˜¤ë¥¸ìª½ìœ¼ë¡œ\n\n# ============================================================================\n# ğŸ“Š ì •ì±… ì„±ëŠ¥ ë¹„êµ\n# ============================================================================\n\ndef run_episode(env, policy, max_steps=20):\n    \"\"\"ì£¼ì–´ì§„ ì •ì±…ìœ¼ë¡œ ì—í”¼ì†Œë“œ 1íšŒ ì‹¤í–‰\"\"\"\n    state = env.reset()\n    total_reward = 0\n    for _ in range(max_steps):\n        action = policy(state)\n        state, reward, done = env.step(action)\n        total_reward += reward\n        if done:\n            break\n    return total_reward\n\n# ê° ì •ì±… 100ë²ˆì”© í…ŒìŠ¤íŠ¸\nenv = SimpleGridWorld()\nn_episodes = 100\n\npolicies = [\n    (\"ğŸ² Random\", random_policy),\n    (\"ğŸ¯ Greedy\", greedy_policy),\n    (\"ğŸ° Îµ-Greedy (Îµ=0.1)\", lambda s: epsilon_greedy_policy(s, 0.1))\n]\n\nprint(\"=\" * 60)\nprint(\"ğŸ“Š Policy ì„±ëŠ¥ ë¹„êµ (ê° 100íšŒ ì‹¤í–‰)\")\nprint(\"=\" * 60)\nprint()\n\nresults = {}\nfor name, policy in policies:\n    returns = [run_episode(env, policy) for _ in range(n_episodes)]\n    mean_return = np.mean(returns)\n    std_return = np.std(returns)\n    results[name] = (mean_return, std_return)\n    \n    # ê²°ê³¼ ì¶œë ¥\n    bar_length = int(mean_return)  # ë§‰ëŒ€ ê·¸ë˜í”„ ê¸¸ì´\n    bar = \"â–ˆ\" * max(0, bar_length)\n    print(f\"{name:25s}: í‰ê·  Return = {mean_return:6.2f} (Â±{std_return:.2f})\")\n    print(f\"                           {bar}\")\n    print()\n\nprint(\"-\" * 60)\nprint(\"ğŸ’¡ ë¶„ì„:\")\nprint(\"   â€¢ Random: í—¤ë§¤ê¸° ë•Œë¬¸ì— Returnì´ ë‚®ìŒ\")\nprint(\"   â€¢ Greedy: í•­ìƒ ìµœë‹¨ ê²½ë¡œ, ê°€ì¥ ë†’ì€ Return\")\nprint(\"   â€¢ Îµ-Greedy: Greedyì™€ ë¹„ìŠ·í•˜ì§€ë§Œ ê°€ë” íƒí—˜ìœ¼ë¡œ ì†ì‹¤\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 1.4 Trajectory (ê¶¤ì )\n\n### ğŸ“– Trajectoryë€?\n\n**Trajectory(Ï„)**ëŠ” í•œ ì—í”¼ì†Œë“œ ë™ì•ˆì˜ **ëª¨ë“  ê²½í—˜ì„ ìˆœì„œëŒ€ë¡œ ê¸°ë¡í•œ ê²ƒ**ì…ë‹ˆë‹¤.\n\n$$\\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \\ldots, s_T, a_T, r_T)$$\n\n```mermaid\nflowchart LR\n    subgraph T0[\"t=0\"]\n        S0[\"s=0\"]\n    end\n    subgraph T1[\"t=1\"]\n        S1[\"s=1\"]\n    end\n    subgraph T2[\"t=2\"]\n        S2[\"s=2\"]\n    end\n    subgraph T3[\"t=3\"]\n        S3[\"s=3\"]\n    end\n    subgraph T4[\"t=4\"]\n        S4[\"s=4\"]\n    end\n    subgraph T5[\"t=5\"]\n        S5[\"s=5 ğŸ¯\"]\n    end\n\n    S0 -->|\"a=â†’<br/>r=-1\"| S1 -->|\"a=â†’<br/>r=-1\"| S2 -->|\"a=â†’<br/>r=-1\"| S3 -->|\"a=â†’<br/>r=-1\"| S4 -->|\"a=â†’<br/>r=+10\"| S5\n\n    style S0 fill:#fff3e0\n    style S1 fill:#fff3e0\n    style S2 fill:#fff3e0\n    style S3 fill:#fff3e0\n    style S4 fill:#fff3e0\n    style S5 fill:#c8e6c9,stroke:#2e7d32\n```\n\n**Trajectory ë°ì´í„°**: `(0, â†’, -1, 1, â†’, -1, 2, â†’, -1, 3, â†’, -1, 4, â†’, +10)`\n\n### ğŸ¯ Decision Transformerì™€ì˜ ì—°ê²°\n\n> â­ **í•µì‹¬**: Decision TransformerëŠ” ì´ Trajectoryë¥¼ **ì‹œí€€ìŠ¤**ë¡œ ë³´ê³ ,\n> GPTì²˜ëŸ¼ **ë‹¤ìŒì— ì˜¬ ê²ƒì„ ì˜ˆì¸¡**í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤!\n\n| ê¸°ì¡´ ë°©ì‹ | Decision Transformer |\n|:---:|:---:|\n| State â†’ Action ë§¤í•‘ í•™ìŠµ | Trajectory ì „ì²´ë¥¼ ì‹œí€€ìŠ¤ë¡œ í•™ìŠµ |\n| \"ì´ ìƒíƒœì—ì„œ ë­˜ í•´ì•¼ í•´?\" | \"ì´ ì‹œí€€ìŠ¤ ë‹¤ìŒì— ë­ê°€ ì™€ì•¼ í•´?\" |\n\nì•„ë˜ ì½”ë“œì—ì„œ ì‹¤ì œ Trajectoryë¥¼ ìˆ˜ì§‘í•´ë´…ì‹œë‹¤! ğŸ‘‡"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# ğŸ“ Trajectory ìˆ˜ì§‘í•˜ê¸°\n# ============================================================================\n#\n# ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ê²½í—˜ì„ ê¸°ë¡í•©ë‹ˆë‹¤.\n# ì´ ë°ì´í„°ê°€ ë‚˜ì¤‘ì— Decision Transformer í•™ìŠµì— ì‚¬ìš©ë©ë‹ˆë‹¤!\n#\n# ============================================================================\n\ndef collect_trajectory(env, policy, max_steps=20):\n    \"\"\"\n    ì£¼ì–´ì§„ ì •ì±…ìœ¼ë¡œ ì—í”¼ì†Œë“œë¥¼ ì‹¤í–‰í•˜ê³  trajectoryë¥¼ ìˆ˜ì§‘\n    \n    Returns:\n        trajectory: {'states': [...], 'actions': [...], 'rewards': [...]}\n    \"\"\"\n    trajectory = {\n        'states': [],   # ë°©ë¬¸í•œ ìƒíƒœë“¤\n        'actions': [],  # ì·¨í•œ í–‰ë™ë“¤\n        'rewards': []   # ë°›ì€ ë³´ìƒë“¤\n    }\n    \n    state = env.reset()\n    \n    for step in range(max_steps):\n        # 1. í˜„ì¬ ìƒíƒœ ê¸°ë¡\n        trajectory['states'].append(state)\n        \n        # 2. ì •ì±…ì— ë”°ë¼ í–‰ë™ ì„ íƒ ë° ê¸°ë¡\n        action = policy(state)\n        trajectory['actions'].append(action)\n        \n        # 3. í™˜ê²½ì—ì„œ í–‰ë™ ì‹¤í–‰\n        next_state, reward, done = env.step(action)\n        \n        # 4. ë³´ìƒ ê¸°ë¡\n        trajectory['rewards'].append(reward)\n        \n        # 5. ë‹¤ìŒ ìƒíƒœë¡œ ì´ë™\n        state = next_state\n        \n        if done:\n            break\n    \n    return trajectory\n\n# ============================================================================\n# ğŸ§ª Trajectory ìˆ˜ì§‘ ë° ë¶„ì„\n# ============================================================================\n\nenv = SimpleGridWorld()\ntraj = collect_trajectory(env, greedy_policy)\n\nprint(\"=\" * 60)\nprint(\"ğŸ“ ìˆ˜ì§‘ëœ Trajectory\")\nprint(\"=\" * 60)\nprint()\n\n# í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ì¶œë ¥\nprint(f\"{'Step':^6} | {'State':^7} | {'Action':^12} | {'Reward':^7}\")\nprint(\"-\" * 42)\n\naction_names = {0: 'â† ì™¼ìª½', 1: 'â†’ ì˜¤ë¥¸ìª½'}\nfor t in range(len(traj['states'])):\n    s = traj['states'][t]\n    a = traj['actions'][t]\n    r = traj['rewards'][t]\n    print(f\"{t:^6} | {s:^7} | {action_names[a]:^12} | {r:^+7}\")\n\nprint()\nprint(\"=\" * 60)\nprint(\"ğŸ“Š Trajectory ìš”ì•½\")\nprint(\"=\" * 60)\nprint(f\"  â€¢ States:  {traj['states']}\")\nprint(f\"  â€¢ Actions: {traj['actions']}\")\nprint(f\"  â€¢ Rewards: {traj['rewards']}\")\nprint(f\"  â€¢ Total Return: {sum(traj['rewards'])}\")\nprint()\n\n# Decision Transformer ì—°ê²°\nprint(\"ğŸ”— Decision Transformerì—ì„œëŠ” ì´ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í• ê¹Œìš”?\")\nprint()\nprint(\"   ì›ë³¸: (sâ‚€, aâ‚€, râ‚€, sâ‚, aâ‚, râ‚, ...)\")\nprint(\"   DT:   (Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚, aâ‚, ...)\")\nprint()\nprint(\"   ì—¬ê¸°ì„œ Rì€ 'Return-to-Go' = ê·¸ ì‹œì ë¶€í„°ì˜ ì´ ë³´ìƒ\")\nprint(\"   â†’ Phase 2ì—ì„œ ìì„¸íˆ ë°°ì›ë‹ˆë‹¤!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# ğŸ¤– 2. Transformer ê¸°ì´ˆ\n\n> **ëª©í‘œ**: Decision Transformerì˜ í•µì‹¬ ì•„í‚¤í…ì²˜ì¸ Transformerì˜ ê¸°ë³¸ ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.\n\n### ì™œ Transformerë¥¼ ì•Œì•„ì•¼ í•˜ë‚˜ìš”?\n\n**Decision Transformer = Transformer + ê°•í™”í•™ìŠµ**\n\nGPTê°€ \"ë‹¨ì–´ ì‹œí€€ìŠ¤\"ë¥¼ ì²˜ë¦¬í•˜ë“¯ì´, Decision TransformerëŠ” \"(ëª©í‘œ, ìƒíƒœ, í–‰ë™) ì‹œí€€ìŠ¤\"ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph GPT[\"ğŸ“ GPT: ë‹¨ì–´ ì‹œí€€ìŠ¤ â†’ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡\"]\n        W1[\"The\"] --> W2[\"cat\"] --> W3[\"sat\"] --> W4[\"on\"] --> W5[\"???\"]\n    end\n\n    subgraph DT[\"ğŸ¤– DT: (R,s,a) ì‹œí€€ìŠ¤ â†’ ë‹¤ìŒ í–‰ë™ ì˜ˆì¸¡\"]\n        R0[\"Râ‚€\"] --> S0[\"sâ‚€\"] --> A0[\"aâ‚€\"] --> R1[\"Râ‚\"] --> S1[\"sâ‚\"] --> A1[\"???\"]\n    end\n\n    style W5 fill:#ffeb3b,stroke:#f57f17\n    style A1 fill:#ffeb3b,stroke:#f57f17\n    style R0 fill:#e1f5fe,stroke:#01579b\n    style R1 fill:#e1f5fe,stroke:#01579b\n    style S0 fill:#fff3e0,stroke:#e65100\n    style S1 fill:#fff3e0,stroke:#e65100\n    style A0 fill:#e8f5e9,stroke:#1b5e20\n```\n\nì´ ì„¹ì…˜ì—ì„œ ë°°ìš¸ í•µì‹¬ ê°œë…:\n\n| ê°œë… | ì—­í•  | DTì—ì„œì˜ ì ìš© |\n|:---:|:---|:---|\n| **Self-Attention** | ì‹œí€€ìŠ¤ ë‚´ ê´€ê³„ íŒŒì•… | ê³¼ê±° ìƒíƒœë“¤ ì°¸ì¡° |\n| **Causal Masking** | ë¯¸ë˜ ì •ë³´ ì°¨ë‹¨ | ë¯¸ë˜ í–‰ë™/ìƒíƒœ ëª» ë´„ |\n| **Positional Encoding** | ìˆœì„œ ì •ë³´ ì œê³µ | ì‹œê°„ ë‹¨ê³„ êµ¬ë¶„ |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2.1 Self-Attention ë©”ì»¤ë‹ˆì¦˜\n\n### ğŸ“– Self-Attentionì´ë€?\n\n**Self-Attention**ì€ ì‹œí€€ìŠ¤ ë‚´ì˜ ê° ìš”ì†Œê°€ **ë‹¤ë¥¸ ëª¨ë“  ìš”ì†Œë¥¼ ì°¸ì¡°**í•˜ì—¬ ì •ë³´ë¥¼ ëª¨ìœ¼ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.\n\n> ğŸ’¡ **ë¹„ìœ **: íšŒì˜ì—ì„œ ë°œí‘œìê°€ ì§ˆë¬¸ì„ ë°›ì„ ë•Œ,\n> - **Query (Q)**: \"ë‚´ê°€ ì•Œê³  ì‹¶ì€ ê²ƒ\" (ì§ˆë¬¸)\n> - **Key (K)**: \"ê° ì‚¬ëŒì´ ê°€ì§„ ì •ë³´ì˜ ì œëª©\" (ì „ë¬¸ ë¶„ì•¼)\n> - **Value (V)**: \"ê° ì‚¬ëŒì´ ê°€ì§„ ì •ë³´ì˜ ë‚´ìš©\" (ì‹¤ì œ ì§€ì‹)\n>\n> ë°œí‘œì(Query)ëŠ” ê° ì°¸ì„ì(Key)ë¥¼ ë³´ê³  \"ì´ ì‚¬ëŒì´ ë‚´ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆê² ë‹¤!\"ë¼ê³  íŒë‹¨í•˜ë©´ ê·¸ ì‚¬ëŒì˜ ì§€ì‹(Value)ì„ ë” ë§ì´ ì°¸ê³ í•©ë‹ˆë‹¤.\n\n### ğŸ”¢ ìˆ˜í•™ì  í‘œí˜„\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\n```mermaid\nflowchart LR\n    Q[\"Q (Query)\"] --> Step1\n    K[\"K (Key)\"] --> Step1\n\n    Step1[\"1ï¸âƒ£ QKáµ€<br/><small>ìœ ì‚¬ë„ ê³„ì‚°</small>\"] --> Step2[\"2ï¸âƒ£ Ã·âˆšd<sub>k</sub><br/><small>ìŠ¤ì¼€ì¼ë§</small>\"] --> Step3[\"3ï¸âƒ£ Softmax<br/><small>í™•ë¥  ë³€í™˜</small>\"] --> Step4[\"4ï¸âƒ£ Ã—V<br/><small>ê°€ì¤‘í•©</small>\"]\n\n    V[\"V (Value)\"] --> Step4\n    Step4 --> Out[\"ì¶œë ¥\"]\n\n    style Step1 fill:#e3f2fd,stroke:#1565c0\n    style Step2 fill:#e8f5e9,stroke:#2e7d32\n    style Step3 fill:#fff3e0,stroke:#e65100\n    style Step4 fill:#fce4ec,stroke:#c2185b\n    style Q fill:#bbdefb\n    style K fill:#bbdefb\n    style V fill:#bbdefb\n```\n\n**ê³„ì‚° ë‹¨ê³„ ì„¤ëª…:**\n1. **ìœ ì‚¬ë„ ê³„ì‚° (QK^T)**: ê° Queryê°€ ê° Keyì™€ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€ ì ìˆ˜ ê³„ì‚°\n2. **ìŠ¤ì¼€ì¼ë§ (Ã·âˆšd_k)**: ì ìˆ˜ê°€ ë„ˆë¬´ ì»¤ì§€ì§€ ì•Šë„ë¡ ì¡°ì ˆ\n3. **Softmax**: ì ìˆ˜ë¥¼ 0~1 ì‚¬ì´ í™•ë¥ ë¡œ ë³€í™˜ (í•© = 1)\n4. **ê°€ì¤‘í•© (Ã—V)**: í™•ë¥ ì— ë”°ë¼ ê° Valueë¥¼ ì„ì–´ì„œ ìµœì¢… ì¶œë ¥\n\nì•„ë˜ ì½”ë“œì—ì„œ ì§ì ‘ êµ¬í˜„í•´ë´…ì‹œë‹¤! ğŸ‘‡"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# ğŸ§  Self-Attention êµ¬í˜„í•˜ê¸°\n# ============================================================================\n#\n# ê°€ì¥ ë‹¨ìˆœí•œ í˜•íƒœì˜ Self-Attentionì„ ì§ì ‘ êµ¬í˜„í•©ë‹ˆë‹¤.\n# ì‹¤ì œë¡œëŠ” Q, K, Vë¥¼ ë‹¤ë¥´ê²Œ ë§Œë“¤ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ë™ì¼í•˜ê²Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n#\n# ============================================================================\n\nimport torch\nimport torch.nn.functional as F\n\ndef simple_self_attention(x):\n    \"\"\"\n    ê°„ë‹¨í•œ Self-Attention êµ¬í˜„\n    \n    Args:\n        x: ì…ë ¥ í…ì„œ (seq_len, d_model)\n           - seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´ (ì˜ˆ: 4ê°œì˜ í† í°)\n           - d_model: ê° í† í°ì˜ ì°¨ì› (ì˜ˆ: 8ì°¨ì› ë²¡í„°)\n    \n    Returns:\n        output: Attention ê²°ê³¼ (seq_len, d_model)\n        attention_weights: ê° í† í°ì´ ë‹¤ë¥¸ í† í°ì„ ì–¼ë§ˆë‚˜ ì°¸ì¡°í•˜ëŠ”ì§€ (seq_len, seq_len)\n    \"\"\"\n    # Self-Attentionì—ì„œëŠ” Q, K, Vê°€ ëª¨ë‘ ê°™ì€ ì…ë ¥ì—ì„œ ë‚˜ì˜´\n    Q = x  # Query: \"ë‚´ê°€ ì°¾ëŠ” ì •ë³´\"\n    K = x  # Key: \"ë‚´ê°€ ê°€ì§„ ì •ë³´ì˜ ë¼ë²¨\"\n    V = x  # Value: \"ë‚´ê°€ ê°€ì§„ ì •ë³´ì˜ ë‚´ìš©\"\n    \n    d_k = x.shape[-1]  # ì°¨ì› í¬ê¸° (ìŠ¤ì¼€ì¼ë§ì— ì‚¬ìš©)\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # Step 1: Attention Score ê³„ì‚° (QK^T)\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # ê²°ê³¼ shape: (seq_len, seq_len)\n    # scores[i][j] = \"í† í° iê°€ í† í° jë¥¼ ì–¼ë§ˆë‚˜ ì°¸ì¡°í• ì§€\"\n    scores = torch.matmul(Q, K.T) / (d_k ** 0.5)  # ìŠ¤ì¼€ì¼ë§\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # Step 2: Softmaxë¡œ í™•ë¥  ë¶„í¬ ë³€í™˜\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # ê° í–‰ì˜ í•© = 1 (í™•ë¥  ë¶„í¬)\n    attention_weights = F.softmax(scores, dim=-1)\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # Step 3: Valueì˜ ê°€ì¤‘í•© ê³„ì‚°\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    output = torch.matmul(attention_weights, V)\n    \n    return output, attention_weights\n\n# ============================================================================\n# ğŸ§ª í…ŒìŠ¤íŠ¸í•˜ê¸°\n# ============================================================================\n\ntorch.manual_seed(42)  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •\n\n# 4ê°œì˜ í† í°, ê° 8ì°¨ì›ì¸ ì…ë ¥ ìƒì„±\nx = torch.randn(4, 8)\n\nprint(\"=\" * 60)\nprint(\"ğŸ§  Self-Attention ë™ì‘ ì˜ˆì‹œ\")\nprint(\"=\" * 60)\nprint()\nprint(f\"ğŸ“¥ ì…ë ¥: {x.shape[0]}ê°œ í† í° Ã— {x.shape[1]}ì°¨ì›\")\nprint()\n\n# Attention ê³„ì‚°\noutput, weights = simple_self_attention(x)\n\nprint(\"ğŸ“Š Attention Weights (ê° í† í°ì´ ë‹¤ë¥¸ í† í°ì„ ì°¸ì¡°í•˜ëŠ” ë¹„ìœ¨):\")\nprint(\"-\" * 60)\nprint()\n\n# í–‰ë ¬ì„ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥\nprint(\"        \", end=\"\")\nfor j in range(4):\n    print(f\"  í† í°{j}  \", end=\"\")\nprint()\nprint(\"       +\" + \"-\" * 36 + \"+\")\n\nfor i in range(4):\n    print(f\"í† í°{i}  |\", end=\"\")\n    for j in range(4):\n        print(f\"  {weights[i,j]:.2f}  \", end=\"\")\n    print(f\"| = {weights[i].sum():.2f}\")\n\nprint(\"       +\" + \"-\" * 36 + \"+\")\nprint()\nprint(\"ğŸ’¡ í•´ì„: ê° í–‰ì˜ í•© = 1.0 (í™•ë¥  ë¶„í¬)\")\nprint(\"   weights[i][j] = í† í° iê°€ í† í° jë¥¼ ì°¸ì¡°í•˜ëŠ” ë¹„ìœ¨\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# ğŸ“Š Attention Weights ì‹œê°í™”\n# ============================================================================\n#\n# Attention weightsë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.\n# ìƒ‰ì´ ì§„í• ìˆ˜ë¡ ë” ë§ì´ ì°¸ì¡°í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n#\n# ============================================================================\n\nplt.figure(figsize=(8, 6))\n\n# íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°\nim = plt.imshow(weights.detach().numpy(), cmap='Blues', vmin=0, vmax=1)\nplt.colorbar(im, label='Attention Weight')\n\n# ë ˆì´ë¸” ì„¤ì •\nplt.xlabel('Key (ì°¸ì¡°ë˜ëŠ” í† í°)', fontsize=12)\nplt.ylabel('Query (ì°¸ì¡°í•˜ëŠ” í† í°)', fontsize=12)\nplt.title('ğŸ§  Self-Attention Weights ì‹œê°í™”', fontsize=14)\n\n# ì¶• ëˆˆê¸ˆ\ntokens = ['Token 0', 'Token 1', 'Token 2', 'Token 3']\nplt.xticks(range(4), tokens)\nplt.yticks(range(4), tokens)\n\n# ê° ì…€ì— ê°’ í‘œì‹œ\nfor i in range(4):\n    for j in range(4):\n        color = 'white' if weights[i,j] > 0.5 else 'black'\n        plt.text(j, i, f'{weights[i,j]:.2f}', ha='center', va='center', \n                fontsize=12, color=color, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# í•´ì„ ì„¤ëª…\nprint()\nprint(\"ğŸ“– ê·¸ë˜í”„ í•´ì„ ë°©ë²•:\")\nprint(\"=\" * 60)\nprint()\nprint(\"  â€¢ í–‰(Row) = Query: ì •ë³´ë¥¼ ìš”ì²­í•˜ëŠ” í† í°\")\nprint(\"  â€¢ ì—´(Column) = Key: ì°¸ì¡°ë˜ëŠ” í† í°\")\nprint(\"  â€¢ ê°’ = ì°¸ì¡° ë¹„ìœ¨ (0~1, ê° í–‰ì˜ í•© = 1)\")\nprint()\nprint(\"  ì˜ˆì‹œ: weights[1][0] = 0.30ì´ë©´\")\nprint(\"        â†’ 'Token 1ì´ Token 0ì„ 30% ë¹„ìœ¨ë¡œ ì°¸ì¡°í•œë‹¤'\")\nprint()\nprint(\"ğŸ”— Decision Transformerì—ì„œëŠ”:\")\nprint(\"   í˜„ì¬ ìƒíƒœê°€ ê³¼ê±°ì˜ ì—¬ëŸ¬ ìƒíƒœë“¤ì„ ì–¼ë§ˆë‚˜ ì°¸ì¡°í• ì§€ ê²°ì •!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2.2 Causal Masking (ì¸ê³¼ì  ë§ˆìŠ¤í‚¹)\n\n### ğŸ“– Causal Maskingì´ë€?\n\nGPT ê°™ì€ **Autoregressive(ìê¸°íšŒê·€) ëª¨ë¸**ì—ì„œëŠ” **ë¯¸ë˜ ì •ë³´ë¥¼ ë³¼ ìˆ˜ ì—†ì–´ì•¼** í•©ë‹ˆë‹¤.\n\n> ğŸ’¡ **ë¹„ìœ **: ì‹œí—˜ ë¬¸ì œë¥¼ ìˆœì„œëŒ€ë¡œ í’€ ë•Œ, ì•„ì§ ì•ˆ í‘¼ ë’· ë¬¸ì œì˜ ë‹µì„ ë¯¸ë¦¬ ë³¼ ìˆ˜ ì—†ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n### ğŸ”’ ì™œ í•„ìš”í•œê°€?\n\n```mermaid\nflowchart TB\n    Sentence[\"ğŸ“ ë¬¸ì¥: The cat sat on the ___\"]\n\n    subgraph Bad[\"âŒ ì¼ë°˜ Attention\"]\n        B1[\"'on' ì˜ˆì¸¡ ì‹œ<br/>ë¯¸ë˜ í† í°ë„ ì°¸ì¡°\"] --> B2[\"ì¹˜íŒ…!<br/>í•™ìŠµ ì‹¤íŒ¨ ğŸ˜±\"]\n    end\n\n    subgraph Good[\"âœ… Causal Attention\"]\n        G1[\"'on' ì˜ˆì¸¡ ì‹œ<br/>ê³¼ê±°ë§Œ ì°¸ì¡°<br/>(The, cat, sat)\"] --> G2[\"ì •ìƒ í•™ìŠµ!<br/>ì„±ê³µ ğŸ‰\"]\n    end\n\n    Sentence --> Bad\n    Sentence --> Good\n\n    style Bad fill:#ffebee,stroke:#c62828\n    style Good fill:#e8f5e9,stroke:#2e7d32\n    style B2 fill:#ffcdd2\n    style G2 fill:#c8e6c9\n```\n\n### ğŸ”¢ Causal Mask êµ¬ì¡°\n\nCausal MaskëŠ” **í•˜ì‚¼ê° í–‰ë ¬**ì…ë‹ˆë‹¤:\n\n|  | Token 0 | Token 1 | Token 2 | Token 3 |\n|:---:|:---:|:---:|:---:|:---:|\n| **Token 0** | âœ… 1 | âŒ 0 | âŒ 0 | âŒ 0 |\n| **Token 1** | âœ… 1 | âœ… 1 | âŒ 0 | âŒ 0 |\n| **Token 2** | âœ… 1 | âœ… 1 | âœ… 1 | âŒ 0 |\n| **Token 3** | âœ… 1 | âœ… 1 | âœ… 1 | âœ… 1 |\n\n- **1 = ë³¼ ìˆ˜ ìˆìŒ**: ìê¸° ìì‹ ê³¼ ì´ì „ í† í°ë“¤\n- **0 = ë³¼ ìˆ˜ ì—†ìŒ**: ë¯¸ë˜ í† í°ë“¤ (ë§ˆìŠ¤í‚¹ë¨)\n\nì•„ë˜ ì½”ë“œì—ì„œ êµ¬í˜„í•˜ê³  ë¹„êµí•´ë´…ì‹œë‹¤! ğŸ‘‡"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# ğŸ”’ Causal Self-Attention êµ¬í˜„í•˜ê¸°\n# ============================================================================\n#\n# ë¯¸ë˜ í† í°ì„ ë³¼ ìˆ˜ ì—†ë„ë¡ ë§ˆìŠ¤í‚¹ì„ ì ìš©í•©ë‹ˆë‹¤.\n# ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ëŠ” attention scoreê°€ -âˆê°€ ë˜ì–´ softmax í›„ 0ì´ ë©ë‹ˆë‹¤.\n#\n# ============================================================================\n\ndef causal_self_attention(x):\n    \"\"\"\n    Causal (Masked) Self-Attention êµ¬í˜„\n    \n    í•µì‹¬: ë¯¸ë˜ í† í°ì˜ attention scoreë¥¼ -âˆë¡œ ë§Œë“¤ì–´ì„œ\n          softmax í›„ 0ì´ ë˜ê²Œ í•©ë‹ˆë‹¤.\n    \n    Args:\n        x: ì…ë ¥ í…ì„œ (seq_len, d_model)\n    \n    Returns:\n        output: Attention ê²°ê³¼\n        attention_weights: ë§ˆìŠ¤í‚¹ì´ ì ìš©ëœ attention weights\n        mask: ì‚¬ìš©ëœ causal mask\n    \"\"\"\n    seq_len = x.shape[0]\n    d_k = x.shape[-1]\n    \n    Q, K, V = x, x, x\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # Step 1: Attention scores ê³„ì‚°\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    scores = torch.matmul(Q, K.T) / (d_k ** 0.5)\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # Step 2: Causal Mask ìƒì„± (í•˜ì‚¼ê° í–‰ë ¬)\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # torch.tril = í•˜ì‚¼ê° í–‰ë ¬ (lower triangular)\n    mask = torch.tril(torch.ones(seq_len, seq_len))\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # Step 3: ë§ˆìŠ¤í¬ ì ìš© (ë¯¸ë˜ ìœ„ì¹˜ëŠ” -âˆë¡œ)\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # mask == 0ì¸ ìœ„ì¹˜(ë¯¸ë˜)ë¥¼ -âˆë¡œ ì„¤ì •\n    # softmax(-âˆ) = 0ì´ ë¨!\n    scores = scores.masked_fill(mask == 0, float('-inf'))\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # Step 4: Softmax ì ìš©\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    attention_weights = F.softmax(scores, dim=-1)\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # Step 5: Valueì˜ ê°€ì¤‘í•©\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    output = torch.matmul(attention_weights, V)\n    \n    return output, attention_weights, mask\n\n# ============================================================================\n# ğŸ§ª í…ŒìŠ¤íŠ¸í•˜ê¸°\n# ============================================================================\n\nx = torch.randn(4, 8)\noutput, causal_weights, mask = causal_self_attention(x)\n\nprint(\"=\" * 60)\nprint(\"ğŸ”’ Causal Mask\")\nprint(\"=\" * 60)\nprint()\nprint(\"1 = ë³¼ ìˆ˜ ìˆìŒ, 0 = ë³¼ ìˆ˜ ì—†ìŒ (ë§ˆìŠ¤í‚¹)\")\nprint()\nprint(mask)\nprint()\n\nprint(\"=\" * 60)\nprint(\"ğŸ“Š Causal Attention Weights\")\nprint(\"=\" * 60)\nprint()\nprint(\"ìƒì‚¼ê° ë¶€ë¶„(ë¯¸ë˜)ì´ ëª¨ë‘ 0ì¸ ê²ƒì„ í™•ì¸í•˜ì„¸ìš”!\")\nprint()\n\n# í–‰ë ¬ ì¶œë ¥\nprint(\"        \", end=\"\")\nfor j in range(4):\n    print(f\"  í† í°{j}  \", end=\"\")\nprint()\nprint(\"       +\" + \"-\" * 36 + \"+\")\n\nfor i in range(4):\n    print(f\"í† í°{i}  |\", end=\"\")\n    for j in range(4):\n        val = causal_weights[i,j].item()\n        if val == 0:\n            print(f\"   --   \", end=\"\")  # ë§ˆìŠ¤í‚¹ëœ ë¶€ë¶„\n        else:\n            print(f\"  {val:.2f}  \", end=\"\")\n    print(f\"| = {causal_weights[i].sum():.2f}\")\n\nprint(\"       +\" + \"-\" * 36 + \"+\")\nprint()\nprint(\"ğŸ’¡ '--'ë¡œ í‘œì‹œëœ ë¶€ë¶„: ë¯¸ë˜ í† í° (ë³¼ ìˆ˜ ì—†ìŒ)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# ğŸ“Š Standard vs Causal Attention ë¹„êµ ì‹œê°í™”\n# ============================================================================\n#\n# ë‘ ê°€ì§€ Attention ë°©ì‹ì˜ ì°¨ì´ë¥¼ ë‚˜ë€íˆ ë¹„êµí•©ë‹ˆë‹¤.\n# í•µì‹¬ ì°¨ì´: Causal Attentionì€ ìƒì‚¼ê° ë¶€ë¶„ì´ 0!\n#\n# ============================================================================\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# ì™¼ìª½: Standard (Full) Attention\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n_, standard_weights = simple_self_attention(x)\n\nim1 = axes[0].imshow(standard_weights.detach().numpy(), cmap='Blues', vmin=0, vmax=0.5)\naxes[0].set_title('ğŸ“Š Standard Self-Attention\\n(ëª¨ë“  í† í°ì„ ë³¼ ìˆ˜ ìˆìŒ)', fontsize=12)\naxes[0].set_xlabel('Key (ì°¸ì¡°ë˜ëŠ” í† í°)')\naxes[0].set_ylabel('Query (ì°¸ì¡°í•˜ëŠ” í† í°)')\nplt.colorbar(im1, ax=axes[0])\n\n# ê°’ í‘œì‹œ\nfor i in range(4):\n    for j in range(4):\n        axes[0].text(j, i, f'{standard_weights[i,j]:.2f}', ha='center', va='center', fontsize=10)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# ì˜¤ë¥¸ìª½: Causal Attention\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nim2 = axes[1].imshow(causal_weights.detach().numpy(), cmap='Blues', vmin=0, vmax=0.5)\naxes[1].set_title('ğŸ”’ Causal Self-Attention\\n(ë¯¸ë˜ í† í°ì€ ë³¼ ìˆ˜ ì—†ìŒ)', fontsize=12)\naxes[1].set_xlabel('Key (ì°¸ì¡°ë˜ëŠ” í† í°)')\naxes[1].set_ylabel('Query (ì°¸ì¡°í•˜ëŠ” í† í°)')\nplt.colorbar(im2, ax=axes[1])\n\n# ê°’ í‘œì‹œ\nfor i in range(4):\n    for j in range(4):\n        val = causal_weights[i,j].item()\n        text = f'{val:.2f}' if val > 0 else 'ğŸš«'\n        axes[1].text(j, i, text, ha='center', va='center', fontsize=10)\n\n# ì¶• ë ˆì´ë¸”\nfor ax in axes:\n    ax.set_xticks(range(4))\n    ax.set_yticks(range(4))\n    ax.set_xticklabels([f'Token {i}' for i in range(4)])\n    ax.set_yticklabels([f'Token {i}' for i in range(4)])\n\nplt.tight_layout()\nplt.show()\n\n# í•´ì„ ì„¤ëª…\nprint()\nprint(\"=\" * 60)\nprint(\"ğŸ“– ë‘ ë°©ì‹ì˜ ì°¨ì´\")\nprint(\"=\" * 60)\nprint()\nprint(\"  Standard Attention:\")\nprint(\"    â€¢ ëª¨ë“  í† í°ì´ ì„œë¡œë¥¼ ë³¼ ìˆ˜ ìˆìŒ\")\nprint(\"    â€¢ ë¬¸ì¥ ë¶„ë¥˜, ë²ˆì—­ì˜ ì¸ì½”ë” ë“±ì—ì„œ ì‚¬ìš©\")\nprint()\nprint(\"  Causal Attention:\")\nprint(\"    â€¢ ê° í† í°ì€ ìì‹ ê³¼ ì´ì „ í† í°ë§Œ ë³¼ ìˆ˜ ìˆìŒ\")\nprint(\"    â€¢ GPT, Decision Transformer ë“±ì—ì„œ ì‚¬ìš©\")\nprint()\nprint(\"ğŸ”— Decision Transformerì—ì„œ:\")\nprint(\"   í˜„ì¬ ì‹œì ì—ì„œ ë¯¸ë˜ì˜ ìƒíƒœ/í–‰ë™ì„ ë³¼ ìˆ˜ ì—†ë„ë¡ ë§ˆìŠ¤í‚¹!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2.3 Positional Encoding (ìœ„ì¹˜ ì¸ì½”ë”©)\n\n### ğŸ“– ì™œ ìœ„ì¹˜ ì •ë³´ê°€ í•„ìš”í•œê°€?\n\nTransformerì˜ Attentionì€ **ìˆœì„œë¥¼ ëª¨ë¦…ë‹ˆë‹¤**! ì…ë ¥ ìˆœì„œë¥¼ ë°”ê¿”ë„ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜µë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph Input[\"ğŸ“ ë‘ ê°€ì§€ ì…ë ¥\"]\n        I1[\"ì…ë ¥1: ê³ ì–‘ì´ê°€, ì¥ë¥¼, ì¡ì•˜ë‹¤\"]\n        I2[\"ì…ë ¥2: ì¥ë¥¼, ê³ ì–‘ì´ê°€, ì¡ì•˜ë‹¤\"]\n    end\n\n    subgraph NoPos[\"âŒ Attentionë§Œ ì‚¬ìš©\"]\n        NP[\"ë‘ ì…ë ¥ì´ ê°™ê²Œ ì²˜ë¦¬ë¨!<br/><small>ìˆœì„œ ì •ë³´ ì—†ìŒ</small>\"]\n    end\n\n    subgraph WithPos[\"âœ… + Positional Encoding\"]\n        WP[\"ê° ë‹¨ì–´ì— ìœ„ì¹˜ ì •ë³´ ì¶”ê°€<br/><small>1ë²ˆì§¸, 2ë²ˆì§¸, 3ë²ˆì§¸...</small>\"]\n        WR[\"ìˆœì„œê°€ ë‹¤ë¥´ë©´ ë‹¤ë¥´ê²Œ ì²˜ë¦¬!\"]\n        WP --> WR\n    end\n\n    Input --> NoPos\n    Input --> WithPos\n\n    style NoPos fill:#ffebee,stroke:#c62828\n    style WithPos fill:#e8f5e9,stroke:#2e7d32\n    style NP fill:#ffcdd2\n    style WR fill:#c8e6c9\n```\n\n### ğŸ”¢ ë‘ ê°€ì§€ ë°©ì‹\n\n| ë°©ì‹ | ì„¤ëª… | íŠ¹ì§• | ì‚¬ìš© ëª¨ë¸ |\n|:---:|:---|:---|:---:|\n| **Sinusoidal** | sin/cos í•¨ìˆ˜ë¡œ ê³ ì • íŒ¨í„´ ìƒì„± | í•™ìŠµ ë¶ˆí•„ìš”, ê¸´ ì‹œí€€ìŠ¤ ì¼ë°˜í™” ê°€ëŠ¥ | ì›ë³¸ Transformer |\n| **Learned** | ê° ìœ„ì¹˜ë§ˆë‹¤ í•™ìŠµ ê°€ëŠ¥í•œ ë²¡í„° | ë°ì´í„°ì—ì„œ ìµœì  íŒ¨í„´ í•™ìŠµ | **GPT, DT** |\n\n### ğŸ’¡ Decision Transformerì—ì„œì˜ ì‚¬ìš©\n\n> Decision TransformerëŠ” **Learned Positional Embedding**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n> í•˜ì§€ë§Œ ì¼ë°˜ì ì¸ \"ì‹œí€€ìŠ¤ ë‚´ ìœ„ì¹˜\"ê°€ ì•„ë‹ˆë¼ **timestep(ì‹œê°„ ë‹¨ê³„)**ì„ ì„ë² ë”©í•©ë‹ˆë‹¤!\n\nì•„ë˜ ì½”ë“œì—ì„œ ë‘ ë°©ì‹ì„ ëª¨ë‘ êµ¬í˜„í•´ë´…ì‹œë‹¤! ğŸ‘‡"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sinusoidal_positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding (\"Attention Is All You Need\" paper)\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(seq_len, d_model)\n",
    "    position = torch.arange(0, seq_len).unsqueeze(1).float()\n",
    "    \n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                         (-math.log(10000.0) / d_model))\n",
    "    \n",
    "    pe[:, 0::2] = torch.sin(position * div_term)  # even dimensions\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)  # odd dimensions\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# ì‹œê°í™”\n",
    "pe = sinusoidal_positional_encoding(50, 64)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(pe.T, aspect='auto', cmap='RdBu')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Position (in sequence)')\n",
    "plt.ylabel('Dimension')\n",
    "plt.title('Sinusoidal Positional Encoding')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Learned Positional Embedding (GPT ìŠ¤íƒ€ì¼)\nimport torch.nn as nn\n\nclass LearnedPositionalEmbedding(nn.Module):\n    def __init__(self, max_seq_len, d_model):\n        \"\"\"\n        Args:\n            max_seq_len: ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ìš©ëŸ‰)\n            d_model: ì„ë² ë”© ì°¨ì›\n        \"\"\"\n        super().__init__()\n        # í•™ìŠµ ê°€ëŠ¥í•œ ì„ë² ë”© ë²¡í„°\n        # nn.Embedding(100, 64) â†’ 100ê°œ ìœ„ì¹˜ Ã— 64ì°¨ì›ì˜ í…Œì´ë¸” ìƒì„±\n        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n\n    def forward(self, x):\n        seq_len = x.shape[0]  # ì‹¤ì œ ì…ë ¥ ê¸¸ì´\n        positions = torch.arange(seq_len)  # [0, 1, 2, ..., seq_len-1]\n        return self.pos_embedding(positions)\n\n# í…ŒìŠ¤íŠ¸\n# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n# â”‚ LearnedPositionalEmbedding(100, 64) íŒŒë¼ë¯¸í„° ì„¤ëª…:              â”‚\n# â”‚                                                                 â”‚\n# â”‚  100 = max_seq_len (ìµœëŒ€ ìš©ëŸ‰)                                  â”‚\n# â”‚      â†’ \"ìµœëŒ€ 100ê°œ ìœ„ì¹˜ê¹Œì§€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì„ë² ë”© í…Œì´ë¸”\"       â”‚\n# â”‚      â†’ ë‚´ë¶€ì ìœ¼ë¡œ (100 Ã— 64) í¬ê¸°ì˜ í•™ìŠµ ê°€ëŠ¥í•œ í–‰ë ¬ ìƒì„±       â”‚\n# â”‚                                                                 â”‚\n# â”‚  64 = d_model (ì„ë² ë”© ì°¨ì›)                                     â”‚\n# â”‚      â†’ ê° ìœ„ì¹˜ëŠ” 64ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„ë¨                           â”‚\n# â”‚                                                                 â”‚\n# â”‚  x = torch.randn(10, 64)                                        â”‚\n# â”‚      â†’ 10 = í˜„ì¬ ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ (í…ŒìŠ¤íŠ¸ìš©)                    â”‚\n# â”‚      â†’ 100ê°œ ì¤‘ 10ê°œ ìœ„ì¹˜ë§Œ ì‹¤ì œë¡œ ì‚¬ìš©                         â”‚\n# â”‚                                                                 â”‚\n# â”‚  ë¹„ìœ : 100ì¹¸ì§œë¦¬ ì£¼ì°¨ì¥ì— 10ëŒ€ë§Œ ì£¼ì°¨í•œ ìƒí™©                    â”‚\n# â”‚      - ì£¼ì°¨ì¥ ìš©ëŸ‰: 100ì¹¸ (max_seq_len)                         â”‚\n# â”‚      - í˜„ì¬ ì£¼ì°¨ëœ ì°¨: 10ëŒ€ (ì‹¤ì œ ì…ë ¥ ê¸¸ì´)                    â”‚\n# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\npos_emb = LearnedPositionalEmbedding(100, 64)  # ìµœëŒ€ 100 ìœ„ì¹˜, ê° 64ì°¨ì›\nx = torch.randn(10, 64)  # ì‹¤ì œë¡œëŠ” 10ê°œ í† í°ë§Œ í…ŒìŠ¤íŠ¸\npe = pos_emb(x)\n\nprint(f\"ì…ë ¥ shape: {x.shape}\")\nprint(f\"Position embedding shape: {pe.shape}\")\nprint(f\"\\nìµœì¢… ì…ë ¥ = í† í° ì„ë² ë”© + ìœ„ì¹˜ ì„ë² ë”©\")\nprint(f\"(x + pe).shape = {(x + pe).shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 GPTì˜ Autoregressive ìƒì„±\n",
    "\n",
    "GPTëŠ” ì´ì „ í† í°ë“¤ì„ ë³´ê³  ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "ì…ë ¥: [<start>, The, cat, sat]\n",
    "ì˜ˆì¸¡: [  The,   cat, sat,  on]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ Autoregressive ìƒì„± ì‹œë®¬ë ˆì´ì…˜\n",
    "vocab = ['<start>', 'the', 'cat', 'sat', 'on', 'mat', '<end>']\n",
    "\n",
    "# ê°€ìƒì˜ ë‹¤ìŒ í† í° í™•ë¥  (ì‹¤ì œë¡œëŠ” ëª¨ë¸ì´ ì˜ˆì¸¡)\n",
    "fake_next_token_probs = {\n",
    "    '<start>': {'the': 0.9, 'cat': 0.05, 'sat': 0.05},\n",
    "    'the': {'cat': 0.7, 'mat': 0.3},\n",
    "    'cat': {'sat': 0.9, 'on': 0.1},\n",
    "    'sat': {'on': 0.95, '<end>': 0.05},\n",
    "    'on': {'the': 0.3, 'mat': 0.7},\n",
    "    'mat': {'<end>': 1.0},\n",
    "}\n",
    "\n",
    "def generate_sequence(start_token='<start>', max_len=10):\n",
    "    sequence = [start_token]\n",
    "    current = start_token\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        if current not in fake_next_token_probs:\n",
    "            break\n",
    "            \n",
    "        probs = fake_next_token_probs[current]\n",
    "        tokens = list(probs.keys())\n",
    "        probabilities = list(probs.values())\n",
    "        \n",
    "        # í™•ë¥ ì  ìƒ˜í”Œë§\n",
    "        next_token = np.random.choice(tokens, p=probabilities)\n",
    "        sequence.append(next_token)\n",
    "        current = next_token\n",
    "        \n",
    "        if next_token == '<end>':\n",
    "            break\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# ì—¬ëŸ¬ ë¬¸ì¥ ìƒì„±\n",
    "print(\"Autoregressive ìƒì„± ì˜ˆì‹œ:\")\n",
    "for i in range(5):\n",
    "    seq = generate_sequence()\n",
    "    print(f\"  {i+1}. {' '.join(seq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ì—°ìŠµ ë¬¸ì œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì—°ìŠµ 1: Return ê³„ì‚°\n",
    "\n",
    "ë‹¤ìŒ rewardsê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê° timestepì—ì„œì˜ Returnì„ ê³„ì‚°í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [2, 4, 1, 3, 5]\n",
    "\n",
    "# TODO: ê° timestepì—ì„œì˜ Return ê³„ì‚° (Î³=1)\n",
    "# returns[0] = 2 + 4 + 1 + 3 + 5 = ?\n",
    "# returns[1] = 4 + 1 + 3 + 5 = ?\n",
    "# ...\n",
    "\n",
    "returns = []\n",
    "for i in range(len(rewards)):\n",
    "    # ì—¬ê¸°ì— ì½”ë“œ ì‘ì„±\n",
    "    pass\n",
    "\n",
    "# ì •ë‹µ í™•ì¸\n",
    "# print(f\"Returns: {returns}\")\n",
    "# ì˜ˆìƒ: [15, 13, 9, 8, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "returns = []\n",
    "for i in range(len(rewards)):\n",
    "    returns.append(sum(rewards[i:]))\n",
    "\n",
    "print(f\"Returns: {returns}\")  # [15, 13, 9, 8, 5]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì—°ìŠµ 2: Causal Mask ì´í•´\n",
    "\n",
    "5ê°œì˜ í† í°ì´ ìˆì„ ë•Œ, 3ë²ˆì§¸ í† í°(ì¸ë±ìŠ¤ 2)ì€ ì–´ë–¤ í† í°ë“¤ì„ ì°¸ì¡°í•  ìˆ˜ ìˆë‚˜ìš”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 5x5 causal mask ë§Œë“¤ê¸°\n",
    "seq_len = 5\n",
    "# mask = ?\n",
    "\n",
    "# ì •ë‹µ í™•ì¸\n",
    "# print(mask)\n",
    "# print(f\"\\ní† í° 2ê°€ ì°¸ì¡°í•  ìˆ˜ ìˆëŠ” í† í°: {?}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "mask = torch.tril(torch.ones(5, 5))\n",
    "print(mask)\n",
    "# tensor([[1., 0., 0., 0., 0.],\n",
    "#         [1., 1., 0., 0., 0.],\n",
    "#         [1., 1., 1., 0., 0.],  â† í† í° 2: 0, 1, 2 ì°¸ì¡° ê°€ëŠ¥\n",
    "#         [1., 1., 1., 1., 0.],\n",
    "#         [1., 1., 1., 1., 1.]])\n",
    "\n",
    "print(f\"í† í° 2ê°€ ì°¸ì¡°í•  ìˆ˜ ìˆëŠ” í† í°: 0, 1, 2 (ìê¸° ìì‹ ê¹Œì§€)\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì—°ìŠµ 3: Policy êµ¬í˜„\n",
    "\n",
    "ìƒíƒœê°€ ëª©í‘œ(5)ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê°ˆ í™•ë¥ ì´ ë†’ì€ policyë¥¼ êµ¬í˜„í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_policy(state):\n",
    "    \"\"\"\n",
    "    stateê°€ 5ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì˜¤ë¥¸ìª½(1)ìœ¼ë¡œ ê°ˆ í™•ë¥  ì¦ê°€\n",
    "    stateê°€ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì™¼ìª½(0)ìœ¼ë¡œ ê°ˆ í™•ë¥  ì¦ê°€\n",
    "    \"\"\"\n",
    "    # TODO: êµ¬í˜„\n",
    "    pass\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "# for s in range(6):\n",
    "#     actions = [smart_policy(s) for _ in range(100)]\n",
    "#     right_prob = sum(actions) / 100\n",
    "#     print(f\"State {s}: ì˜¤ë¥¸ìª½ í™•ë¥  = {right_prob:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "def smart_policy(state):\n",
    "    # stateê°€ í´ìˆ˜ë¡ ì˜¤ë¥¸ìª½ í™•ë¥  ë†’ìŒ\n",
    "    prob_right = state / 5.0  # 0~1 ì‚¬ì´ ê°’\n",
    "    return 1 if np.random.random() < prob_right else 0\n",
    "\n",
    "# ë˜ëŠ” ë” ê°„ë‹¨í•˜ê²Œ:\n",
    "def smart_policy_v2(state):\n",
    "    return 1 if state < 5 else 0  # ëª©í‘œì— ë„ë‹¬í•˜ë©´ ë©ˆì¶¤\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# ğŸ‰ Phase 1 ì™„ë£Œ!\n\n## ğŸ“š ë°°ìš´ ë‚´ìš© ìš”ì•½\n\n### ğŸ® ê°•í™”í•™ìŠµ ê¸°ì´ˆ\n| ê°œë… | ì„¤ëª… |\n|:---:|:---|\n| **State** | í™˜ê²½ì˜ í˜„ì¬ ìƒíƒœ |\n| **Action** | ì—ì´ì „íŠ¸ê°€ ì·¨í•˜ëŠ” í–‰ë™ |\n| **Reward** | í–‰ë™ì˜ ê²°ê³¼ë¡œ ë°›ëŠ” í”¼ë“œë°± |\n| **Policy** | ìƒíƒœì—ì„œ í–‰ë™ì„ ì„ íƒí•˜ëŠ” ê·œì¹™ |\n| **Return** | ì—í”¼ì†Œë“œì˜ ì´ ë³´ìƒ |\n| **Trajectory** | (s, a, r) ì‹œí€€ìŠ¤ - DTì˜ ì…ë ¥ ë°ì´í„°! |\n\n### ğŸ¤– Transformer ê¸°ì´ˆ\n| ê°œë… | ì„¤ëª… | DTì—ì„œì˜ ì—­í•  |\n|:---:|:---|:---|\n| **Self-Attention** | ì‹œí€€ìŠ¤ ë‚´ ê´€ê³„ íŒŒì•… | ê³¼ê±° ìƒíƒœ/í–‰ë™ ì°¸ì¡° |\n| **Causal Masking** | ë¯¸ë˜ ì •ë³´ ì°¨ë‹¨ | ì‹œê°„ ìˆœì„œ ìœ ì§€ |\n| **Positional Encoding** | ìœ„ì¹˜ ì •ë³´ ì œê³µ | timestep êµ¬ë¶„ |\n\n---\n\n## â¡ï¸ ë‹¤ìŒ ë‹¨ê³„\n\n**Phase 2: Decision Transformer í•µì‹¬ ê°œë…** â†’ `phase2_core_concepts.ipynb`\n\n- Return-to-Go (RTG) ê°œë…\n- ì‹œí€€ìŠ¤ êµ¬ì„± ë°©ì‹ (R, s, a)\n- ê¸°ì¡´ RLê³¼ì˜ ì°¨ì´ì \n\n---\n\n## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸\n\në‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°€ê¸° ì „ì— í™•ì¸í•˜ì„¸ìš”:\n\n- [ ] State, Action, Reward ê°œë…ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤\n- [ ] Returnê³¼ Discount factorë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤\n- [ ] Policyê°€ ë¬´ì—‡ì¸ì§€ ì´í•´í–ˆë‹¤\n- [ ] Trajectoryê°€ DTì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€ ì´í•´í–ˆë‹¤\n- [ ] Self-Attentionì˜ Q, K, V ê°œë…ì„ ì´í•´í–ˆë‹¤\n- [ ] Causal Maskingì´ ì™œ í•„ìš”í•œì§€ ì´í•´í–ˆë‹¤\n- [ ] Positional Encodingì˜ ì—­í• ì„ ì´í•´í–ˆë‹¤"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hawkeye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}