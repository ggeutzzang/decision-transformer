{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ¯ Phase 2: Decision Transformer í•µì‹¬ ê°œë…\n\n> **ëª©í‘œ**: Decision Transformerì˜ **í•µì‹¬ ì•„ì´ë””ì–´**ì¸ Return-to-Go(RTG)ì™€ ì‹œí€€ìŠ¤ êµ¬ì„± ë°©ì‹ì„ ì´í•´í•©ë‹ˆë‹¤.\n\n---\n\n## ğŸ“š ì´ ë…¸íŠ¸ë¶ì—ì„œ ë°°ìš°ëŠ” ê²ƒ\n\n| ì„¹ì…˜ | í•µì‹¬ ê°œë… | ì™œ ì¤‘ìš”í•œê°€? |\n|:---:|:---|:---|\n| **1** | Return-to-Go (RTG) | DTì˜ \"ëª©í‘œ ì„¤ì •\" ë°©ë²• |\n| **2** | ì‹œí€€ìŠ¤ êµ¬ì„± ë°©ì‹ | ì…ë ¥ ë°ì´í„° í˜•íƒœ ì´í•´ |\n| **3** | ê¸°ì¡´ RLê³¼ì˜ ì°¨ì´ì  | DTì˜ ë…íŠ¹í•œ ì ‘ê·¼ë²• ì´í•´ |\n| **4** | ì§ì ‘ êµ¬í˜„í•´ë³´ê¸° | ì½”ë“œë¡œ í™•ì¸ |\n\n---\n\n## ğŸŒŸ Decision Transformerì˜ í•µì‹¬ ì•„ì´ë””ì–´\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              Decision Transformerì˜ í˜ì‹ ì  ë°œìƒ                     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                     â”‚\nâ”‚   ğŸ¤” ê¸°ì¡´ RLì˜ ì§ˆë¬¸:                                                â”‚\nâ”‚      \"ì´ ìƒíƒœì—ì„œ ìµœì ì˜ í–‰ë™ì€ ë­ì§€?\"                              â”‚\nâ”‚      â†’ Value Functionì„ ì¶”ì •í•´ì•¼ í•¨ (ì–´ë µê³  ë¶ˆì•ˆì •)                â”‚\nâ”‚                                                                     â”‚\nâ”‚   ğŸ’¡ Decision Transformerì˜ ì§ˆë¬¸:                                   â”‚\nâ”‚      \"ì´ ìƒíƒœì—ì„œ 100ì ì„ ì–»ìœ¼ë ¤ë©´ ë­˜ í•´ì•¼ í•˜ì§€?\"                  â”‚\nâ”‚      â†’ ëª©í‘œ(RTG)ë¥¼ ì¡°ê±´ìœ¼ë¡œ í–‰ë™ ì˜ˆì¸¡ (Supervised Learning!)       â”‚\nâ”‚                                                                     â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\nâ”‚   â”‚                                                        â”‚        â”‚\nâ”‚   â”‚    ì…ë ¥: [ëª©í‘œ(RTG), ìƒíƒœ, ê³¼ê±°í–‰ë™ë“¤...]              â”‚        â”‚\nâ”‚   â”‚                     â†“                                  â”‚        â”‚\nâ”‚   â”‚              Transformer                               â”‚        â”‚\nâ”‚   â”‚                     â†“                                  â”‚        â”‚\nâ”‚   â”‚    ì¶œë ¥: ë‹¤ìŒ í–‰ë™(Action)                             â”‚        â”‚\nâ”‚   â”‚                                                        â”‚        â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\nâ”‚                                                                     â”‚\nâ”‚   í•µì‹¬: \"ë†’ì€ ì ìˆ˜ë¥¼ ì›í•˜ë©´ ë†’ì€ RTGë¥¼ ì…ë ¥í•˜ë©´ ë¨!\"                â”‚\nâ”‚                                                                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n## ğŸ“‹ ëª©ì°¨\n\n1. [ğŸ¯ Return-to-Go (RTG) ê°œë…](#1-return-to-go-rtg-ê°œë…)\n   - RTGë€ ë¬´ì—‡ì¸ê°€?\n   - RTG vs Return\n   - í•™ìŠµ vs ì¶”ë¡ ì—ì„œì˜ RTG\n2. [ğŸ“¦ ì‹œí€€ìŠ¤ êµ¬ì„± ë°©ì‹](#2-ì‹œí€€ìŠ¤-êµ¬ì„±-ë°©ì‹)\n   - (RTG, State, Action) íŠ¸ë¦¬í”Œ\n   - ì¸í„°ë¦¬ë¹™ êµ¬ì¡°\n   - ì˜ˆì¸¡ ìœ„ì¹˜ (Stateì—ì„œ Action ì˜ˆì¸¡)\n3. [ğŸ”„ ê¸°ì¡´ RLê³¼ì˜ ì°¨ì´ì ](#3-ê¸°ì¡´-rlê³¼ì˜-ì°¨ì´ì )\n   - Bellman Equation vs Sequence Modeling\n   - ì¥ë‹¨ì  ë¹„êµ\n4. [ğŸ› ï¸ ì§ì ‘ êµ¬í˜„í•´ë³´ê¸°](#4-ì§ì ‘-êµ¬í˜„í•´ë³´ê¸°)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\n\n# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# í•œê¸€ í°íŠ¸ ì„¤ì •\ndef setup_korean_font():\n    \"\"\"ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì•„ ì„¤ì •\"\"\"\n    korean_fonts = [\n        'Noto Sans CJK KR', 'NanumSquare', 'NanumGothic',\n        'Malgun Gothic', 'AppleGothic'\n    ]\n    system_fonts = {f.name for f in fm.fontManager.ttflist}\n    \n    for font in korean_fonts:\n        if font in system_fonts:\n            plt.rcParams['font.family'] = font\n            plt.rcParams['axes.unicode_minus'] = False\n            return font\n    \n    # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„\n    for font in korean_fonts:\n        for sys_font in system_fonts:\n            if font.lower().replace(' ', '') in sys_font.lower().replace(' ', ''):\n                plt.rcParams['font.family'] = sys_font\n                plt.rcParams['axes.unicode_minus'] = False\n                return sys_font\n    \n    plt.rcParams['axes.unicode_minus'] = False\n    return None\n\nsetup_korean_font()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# ğŸ¯ 1. Return-to-Go (RTG) ê°œë…\n\nRTGëŠ” Decision Transformerì˜ **ê°€ì¥ ì¤‘ìš”í•œ ê°œë…**ì…ë‹ˆë‹¤!\n\n---\n\n## ğŸ“– RTGë€ ë¬´ì—‡ì¸ê°€?\n\n**Return-to-Go (RTG)**ëŠ” **\"í˜„ì¬ ì‹œì ë¶€í„° ì—í”¼ì†Œë“œ ëê¹Œì§€ ì–»ì„ ë³´ìƒì˜ í•©\"**ì…ë‹ˆë‹¤.\n\n$$\\hat{R}_t = \\sum_{t'=t}^{T} r_{t'} = r_t + r_{t+1} + r_{t+2} + \\cdots + r_T$$\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                       RTG ì§ê´€ì  ì´í•´                               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                     â”‚\nâ”‚   ì‹œê°„:     t=0      t=1      t=2      t=3      t=4    (ë)        â”‚\nâ”‚   ë³´ìƒ:     +1       +2       +3       +4       +5                  â”‚\nâ”‚                                                                     â”‚\nâ”‚   RTG[0] = 1+2+3+4+5 = 15  \"ì•ìœ¼ë¡œ 15ì ì„ ì–»ì„ ê²ƒ\"                 â”‚\nâ”‚   RTG[1] =   2+3+4+5 = 14  \"ì•ìœ¼ë¡œ 14ì ì„ ì–»ì„ ê²ƒ\"                 â”‚\nâ”‚   RTG[2] =     3+4+5 = 12  \"ì•ìœ¼ë¡œ 12ì ì„ ì–»ì„ ê²ƒ\"                 â”‚\nâ”‚   RTG[3] =       4+5 = 9   \"ì•ìœ¼ë¡œ 9ì ì„ ì–»ì„ ê²ƒ\"                  â”‚\nâ”‚   RTG[4] =         5 = 5   \"ì•ìœ¼ë¡œ 5ì ì„ ì–»ì„ ê²ƒ\"                  â”‚\nâ”‚                                                                     â”‚\nâ”‚   ğŸ“‰ RTGëŠ” ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ê°ì†Œí•©ë‹ˆë‹¤!                             â”‚\nâ”‚                                                                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n## ğŸ® RTGì˜ ì—­í• \n\n> **í•µì‹¬ ì•„ì´ë””ì–´**: RTGë¥¼ \"ëª©í‘œ ì ìˆ˜\"ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤!\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                                                                     â”‚\nâ”‚   \"RTG=100, í˜„ì¬ ìƒíƒœ=s\" â†’ ëª¨ë¸ â†’ \"100ì ì„ ìœ„í•œ í–‰ë™=a\"           â”‚\nâ”‚                                                                     â”‚\nâ”‚   ë†’ì€ RTG ì…ë ¥ â†’ ì¢‹ì€ í–‰ë™ ì¶œë ¥                                   â”‚\nâ”‚   ë‚®ì€ RTG ì…ë ¥ â†’ í‰ë²”í•œ í–‰ë™ ì¶œë ¥                                 â”‚\nâ”‚                                                                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\nì•„ë˜ ì½”ë“œì—ì„œ RTG ê³„ì‚°ì„ ì§ì ‘ í•´ë´…ì‹œë‹¤! ğŸ‘‡"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# RTG ê³„ì‚° ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ë³´ì—¬ì£¼ê¸°\n\ndef calculate_rtg_verbose(rewards):\n    \"\"\"\n    Return-to-Go ê³„ì‚° (ë‹¨ê³„ë³„ ì¶œë ¥)\n    \n    í•µì‹¬: ë’¤ì—ì„œë¶€í„° ëˆ„ì í•©ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n    ì™œ? RTG[t] = reward[t] + reward[t+1] + ... + reward[T]\n        ì¦‰, \"í˜„ì¬ë¶€í„° ëê¹Œì§€ì˜ í•©\"\n    \"\"\"\n    n = len(rewards)\n    rtg = [0] * n\n    \n    print(\"RTG ê³„ì‚° ê³¼ì • (ë’¤ì—ì„œë¶€í„°):\")\n    print(\"=\" * 60)\n    \n    cumulative = 0\n    for t in range(n - 1, -1, -1):  # ë’¤ì—ì„œë¶€í„° ìˆœíšŒ\n        cumulative += rewards[t]\n        rtg[t] = cumulative\n        \n        # ê³„ì‚° ê³¼ì • ì¶œë ¥\n        remaining = rewards[t:]\n        formula = \" + \".join(map(str, remaining))\n        print(f\"t={t}: RTG[{t}] = {formula} = {cumulative}\")\n    \n    return rtg\n\n# ì˜ˆì‹œ\nrewards = [1, 2, 3, 4, 5]\nprint(f\"ë³´ìƒ ì‹œí€€ìŠ¤: {rewards}\\n\")\nrtg = calculate_rtg_verbose(rewards)\nprint(f\"\\nìµœì¢… RTG: {rtg}\")"
  },
  {
   "cell_type": "code",
   "source": "# RTG vs Return ë¹„êµ\n# Return: ì—í”¼ì†Œë“œ ì „ì²´ì˜ ì´ ë³´ìƒ (í•˜ë‚˜ì˜ ìˆ«ì)\n# RTG: ê° ì‹œì ì—ì„œ ë‚¨ì€ ë³´ìƒì˜ í•© (ì‹œì ë§ˆë‹¤ ë‹¤ë¥¸ ê°’)\n\nrewards = [1, 2, 3, 4, 5]\n\n# Return (ì „ì²´ í•©)\ntotal_return = sum(rewards)\nprint(f\"Return (ì „ì²´ ë³´ìƒ í•©): {total_return}\")\nprint()\n\n# RTG (ê° ì‹œì ì—ì„œì˜ ë‚¨ì€ ë³´ìƒ)\nprint(\"RTG (ê° ì‹œì ì—ì„œ ë‚¨ì€ ë³´ìƒì˜ í•©):\")\nprint(\"-\" * 40)\nfor t in range(len(rewards)):\n    remaining_rewards = rewards[t:]\n    rtg_t = sum(remaining_rewards)\n    print(f\"t={t}: ë‚¨ì€ ë³´ìƒ = {remaining_rewards} â†’ RTG = {rtg_t}\")\n\nprint()\nprint(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\nprint(\"â”‚  í•µì‹¬ ì°¨ì´:                                      â”‚\")\nprint(\"â”‚  - Return: í•˜ë‚˜ì˜ ìˆ«ì (ì—í”¼ì†Œë“œ ëì— í™•ì •)      â”‚\")\nprint(\"â”‚  - RTG: ì‹œì ë§ˆë‹¤ ë‹¤ë¥¸ ê°’ (ì ì  ê°ì†Œ)             â”‚\")\nprint(\"â”‚                                                  â”‚\")\nprint(\"â”‚  RTGëŠ” 'ì•„ì§ ì–»ì–´ì•¼ í•  ë³´ìƒ'ì„ ì˜ë¯¸!             â”‚\")\nprint(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### RTGì˜ ì—­í• : í•™ìŠµ vs ì¶”ë¡ \n\n| ë‹¨ê³„ | RTGì˜ ì—­í•  | ê°’ì˜ ì¶œì²˜ |\n|-----|----------|---------|\n| **í•™ìŠµ** | ë°ì´í„°ì—ì„œ ê³„ì‚°ëœ ì‹¤ì œ RTG ì‚¬ìš© | ê³¼ê±° ë°ì´í„° |\n| **ì¶”ë¡ ** | ì›í•˜ëŠ” ëª©í‘œ returnì„ RTGë¡œ ì„¤ì • | ì‚¬ìš©ì ì§€ì • |\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  í•™ìŠµ ì‹œ:                                                   â”‚\nâ”‚    ê³¼ê±° ì—í”¼ì†Œë“œì—ì„œ ì‹¤ì œë¡œ ì–»ì—ˆë˜ RTG ì‚¬ìš©                 â”‚\nâ”‚    \"ì´ ìƒíƒœì—ì„œ RTG=50ì´ì—ˆì„ ë•Œ ì‹¤ì œë¡œ í•œ í–‰ë™ì€ ë­ì˜€ì§€?\"  â”‚\nâ”‚                                                             â”‚\nâ”‚  ì¶”ë¡  ì‹œ:                                                   â”‚\nâ”‚    ì›í•˜ëŠ” ëª©í‘œë¥¼ RTGë¡œ ì„¤ì •                                 â”‚\nâ”‚    \"ì´ ìƒíƒœì—ì„œ RTG=100ì„ ì–»ìœ¼ë ¤ë©´ ë­˜ í•´ì•¼ í•´?\"            â”‚\nâ”‚    â†’ ë†’ì€ RTGë¥¼ ì„¤ì •í•˜ë©´ ë” ì¢‹ì€ í–‰ë™ì„ ì¶œë ¥!              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ì¶”ë¡  ì‹œ RTG ì—…ë°ì´íŠ¸ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì‹œë®¬ë ˆì´ì…˜\n\ndef simulate_inference_detailed(target_return, actual_rewards):\n    \"\"\"\n    ì¶”ë¡  ì‹œ RTGê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ìƒì„¸ ì‹œë®¬ë ˆì´ì…˜\n    \n    í•µì‹¬ ê·œì¹™: ìƒˆ RTG = ì´ì „ RTG - ë°›ì€ ë³´ìƒ\n    \n    Args:\n        target_return: ì²˜ìŒì— ì„¤ì •í•œ ëª©í‘œ return\n        actual_rewards: ì‹¤ì œë¡œ ë°›ì€ ë³´ìƒë“¤\n    \"\"\"\n    print(\"=\" * 60)\n    print(f\"ëª©í‘œ Return: {target_return}\")\n    print(\"=\" * 60)\n    print()\n    \n    rtg = target_return\n    rtg_history = [rtg]\n    \n    for step, reward in enumerate(actual_rewards):\n        print(f\"â”Œâ”€ Step {step} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n        print(f\"â”‚  í˜„ì¬ RTG: {rtg}\")\n        print(f\"â”‚  â†’ ëª¨ë¸ì—ê²Œ 'RTG={rtg}ì„ ë‹¬ì„±í•˜ëŠ” í–‰ë™' ìš”ì²­\")\n        print(f\"â”‚  â†’ ëª¨ë¸ì´ í–‰ë™ ì¶œë ¥\")\n        print(f\"â”‚  â†’ í™˜ê²½ì—ì„œ í–‰ë™ ì‹¤í–‰\")\n        print(f\"â”‚  â†’ ë°›ì€ ë³´ìƒ: {reward}\")\n        print(f\"â”‚\")\n        \n        new_rtg = rtg - reward\n        print(f\"â”‚  RTG ì—…ë°ì´íŠ¸: {rtg} - {reward} = {new_rtg}\")\n        print(f\"â”‚  ('{reward}ì  ì–»ì—ˆìœ¼ë‹ˆ, ì•ìœ¼ë¡œ {new_rtg}ì  ë” í•„ìš”')\")\n        print(f\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n        print()\n        \n        rtg = new_rtg\n        rtg_history.append(rtg)\n    \n    total_reward = sum(actual_rewards)\n    print(\"=\" * 60)\n    print(f\"ìµœì¢… ê²°ê³¼:\")\n    print(f\"  - ëª©í‘œ: {target_return}\")\n    print(f\"  - ì‹¤ì œ íšë“: {total_reward}\")\n    print(f\"  - ë‚¨ì€ RTG: {rtg} (0ì´ë©´ ëª©í‘œ ë‹¬ì„±!)\")\n    print(f\"  - ë‹¬ì„±ë¥ : {total_reward/target_return*100:.1f}%\")\n    \n    return rtg_history\n\n# ì˜ˆì‹œ: ëª©í‘œ 100ì \ntarget = 100\nrewards = [20, 15, 25, 10, 30]\nrtg_history = simulate_inference_detailed(target, rewards)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RTG ë³€í™” ì‹œê°í™”\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "steps = range(len(rtg_history))\n",
    "plt.plot(steps, rtg_history, 'o-', linewidth=2, markersize=8, color='coral')\n",
    "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# ê° í¬ì¸íŠ¸ì— ê°’ í‘œì‹œ\n",
    "for i, rtg in enumerate(rtg_history):\n",
    "    plt.annotate(f'{rtg:.0f}', (i, rtg), textcoords=\"offset points\", \n",
    "                 xytext=(0, 10), ha='center')\n",
    "\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Return-to-Go')\n",
    "plt.title('ì¶”ë¡  ì‹œ RTG ë³€í™” (ëª©í‘œ: 100)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"í•µì‹¬: ë§¤ ìŠ¤í…ë§ˆë‹¤ RTG = RTG - reward ë¡œ ì—…ë°ì´íŠ¸\")\n",
    "print(\"      RTGëŠ” 'ì•„ì§ ì–»ì–´ì•¼ í•  ë³´ìƒ'ì„ ì˜ë¯¸í•©ë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 2.1 ì‹œí€€ìŠ¤ êµ¬ì¡°\n\nDecision TransformerëŠ” (RTG, State, Action) íŠ¸ë¦¬í”Œì„ ì‹œí€€ìŠ¤ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.\n\n```\nÏ„ = (RÌ‚â‚, sâ‚, aâ‚, RÌ‚â‚‚, sâ‚‚, aâ‚‚, ..., RÌ‚â‚œ, sâ‚œ, aâ‚œ)\n```\n\n- RÌ‚: Return-to-Go (\"ì•ìœ¼ë¡œ ì–»ê³  ì‹¶ì€ ë³´ìƒ\")\n- s: State (í˜„ì¬ ìƒíƒœ)\n- a: Action (ì·¨í•œ í–‰ë™)\n\n**ì™œ ì´ ìˆœì„œì¸ê°€?**\n\n```\nGPTì˜ ë‹¤ìŒ í† í° ì˜ˆì¸¡ ë°©ì‹ê³¼ ì—°ê²°:\n\n\"RTGì™€ Stateë¥¼ ë³´ë©´ â†’ Actionì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤\"\n\nì…ë ¥:  [Râ‚€, sâ‚€]  â†’  ì¶œë ¥: aâ‚€\nì…ë ¥:  [Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚]  â†’  ì¶œë ¥: aâ‚\n...\n\nì¦‰, \"ëª©í‘œ(RTG) + í˜„ì¬ ìƒíƒœ(s) â†’ í–‰ë™(a)\" ë¼ëŠ” ì¸ê³¼ ê´€ê³„!\n```"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ì‹œí€€ìŠ¤ êµ¬ì„± ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ë³´ì—¬ì£¼ê¸°\n\ndef calculate_rtg(rewards):\n    \"\"\"RTG ê³„ì‚° (ê°„ë‹¨ ë²„ì „)\"\"\"\n    rtg = []\n    cumulative = 0\n    for r in reversed(rewards):\n        cumulative += r\n        rtg.append(cumulative)\n    return list(reversed(rtg))\n\ndef create_dt_sequence_verbose(states, actions, rewards):\n    \"\"\"\n    Decision Transformer ì‹œí€€ìŠ¤ ìƒì„± (ìƒì„¸ ë²„ì „)\n    \n    ì…ë ¥: states, actions, rewards (ê°ê° ë¦¬ìŠ¤íŠ¸)\n    ì¶œë ¥: [RTGâ‚€, sâ‚€, aâ‚€, RTGâ‚, sâ‚, aâ‚, ...] í˜•íƒœì˜ ì‹œí€€ìŠ¤\n    \"\"\"\n    # 1. RTG ê³„ì‚°\n    rtgs = calculate_rtg(rewards)\n    \n    print(\"Step 1: RTG ê³„ì‚°\")\n    print(\"-\" * 50)\n    for t, (r, rtg) in enumerate(zip(rewards, rtgs)):\n        print(f\"  t={t}: reward={r}, RTG={rtg}\")\n    print()\n    \n    # 2. ì‹œí€€ìŠ¤ êµ¬ì„±\n    print(\"Step 2: ì‹œí€€ìŠ¤ êµ¬ì„± (ì¸í„°ë¦¬ë¹™)\")\n    print(\"-\" * 50)\n    \n    sequence = []\n    for t, (rtg, state, action) in enumerate(zip(rtgs, states, actions)):\n        # RTG, State, Action ìˆœì„œë¡œ ì¶”ê°€\n        sequence.append(('RTG', rtg))\n        sequence.append(('State', state))\n        sequence.append(('Action', action))\n        \n        print(f\"  t={t}: RTG={rtg} â†’ State={state} â†’ Action={action}\")\n    \n    print()\n    print(\"Step 3: ìµœì¢… ì‹œí€€ìŠ¤\")\n    print(\"-\" * 50)\n    for i, (token_type, value) in enumerate(sequence):\n        print(f\"  Position {i:2d}: [{token_type:6s}] = {value}\")\n    \n    return sequence\n\n# ì˜ˆì‹œ ë°ì´í„°\nstates = ['s0', 's1', 's2', 's3']\nactions = ['a0', 'a1', 'a2', 'a3']\nrewards = [1, 2, 3, 4]\n\nprint(\"=\" * 50)\nprint(\"ì…ë ¥ ë°ì´í„°:\")\nprint(f\"  States:  {states}\")\nprint(f\"  Actions: {actions}\")\nprint(f\"  Rewards: {rewards}\")\nprint(\"=\" * 50)\nprint()\n\nsequence = create_dt_sequence_verbose(states, actions, rewards)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dt_sequence(states, actions, rewards):\n",
    "    \"\"\"\n",
    "    Decision Transformer ì‹œí€€ìŠ¤ ìƒì„±\n",
    "    \"\"\"\n",
    "    rtgs = calculate_rtg(rewards)\n",
    "    \n",
    "    sequence = []\n",
    "    for rtg, state, action in zip(rtgs, states, actions):\n",
    "        sequence.append(('RTG', rtg))\n",
    "        sequence.append(('State', state))\n",
    "        sequence.append(('Action', action))\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# ì˜ˆì‹œ ë°ì´í„°\n",
    "states = ['s0', 's1', 's2', 's3']\n",
    "actions = ['a0', 'a1', 'a2', 'a3']\n",
    "rewards = [1, 2, 3, 4]\n",
    "\n",
    "sequence = create_dt_sequence(states, actions, rewards)\n",
    "\n",
    "print(\"Decision Transformer ì‹œí€€ìŠ¤:\")\n",
    "print(\"=\"*60)\n",
    "for i, (token_type, value) in enumerate(sequence):\n",
    "    print(f\"Position {i:2d}: [{token_type:6s}] = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ì¸ë±ì‹± ê·œì¹™ì„ ì½”ë“œë¡œ í™•ì¸\n\nsequence = ['R0', 's0', 'a0', 'R1', 's1', 'a1', 'R2', 's2', 'a2', 'R3', 's3', 'a3']\n\nprint(\"ì „ì²´ ì‹œí€€ìŠ¤:\")\nfor i, token in enumerate(sequence):\n    print(f\"  Position {i:2d}: {token}\")\n\nprint()\nprint(\"=\" * 50)\nprint(\"Python ìŠ¬ë¼ì´ì‹±ìœ¼ë¡œ ê° íƒ€ì… ì¶”ì¶œ:\")\nprint(\"=\" * 50)\n\n# RTG ìœ„ì¹˜ (0, 3, 6, 9, ...)\nrtg_positions = sequence[0::3]\nprint(f\"\\nRTG (0::3):    {rtg_positions}\")\nprint(f\"  â†’ ìœ„ì¹˜: {list(range(0, len(sequence), 3))}\")\n\n# State ìœ„ì¹˜ (1, 4, 7, 10, ...)\nstate_positions = sequence[1::3]\nprint(f\"\\nState (1::3):  {state_positions}\")\nprint(f\"  â†’ ìœ„ì¹˜: {list(range(1, len(sequence), 3))}\")\nprint(f\"  â†’ Action ì˜ˆì¸¡ì€ ì´ ìœ„ì¹˜ì—ì„œ!\")\n\n# Action ìœ„ì¹˜ (2, 5, 8, 11, ...)\naction_positions = sequence[2::3]\nprint(f\"\\nAction (2::3): {action_positions}\")\nprint(f\"  â†’ ìœ„ì¹˜: {list(range(2, len(sequence), 3))}\")\n\nprint()\nprint(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\nprint(\"â”‚  0::3 ì˜ë¯¸: 0ë²ˆë¶€í„° ì‹œì‘, 3ì¹¸ì”© ì í”„              â”‚\")\nprint(\"â”‚  1::3 ì˜ë¯¸: 1ë²ˆë¶€í„° ì‹œì‘, 3ì¹¸ì”© ì í”„              â”‚\")\nprint(\"â”‚  2::3 ì˜ë¯¸: 2ë²ˆë¶€í„° ì‹œì‘, 3ì¹¸ì”© ì í”„              â”‚\")\nprint(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")"
  },
  {
   "cell_type": "code",
   "source": "# Causal Maskingê³¼ í•¨ê»˜ ì´í•´í•˜ê¸°\n# ê° ìœ„ì¹˜ì—ì„œ ì–´ë–¤ ì •ë³´ë¥¼ ë³¼ ìˆ˜ ìˆëŠ”ì§€ í™•ì¸\n\nsequence = ['R0', 's0', 'a0', 'R1', 's1', 'a1', 'R2', 's2', 'a2']\n\nprint(\"ê° State ìœ„ì¹˜ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” ì •ë³´ (Causal Masking):\")\nprint(\"=\" * 60)\n\n# State ìœ„ì¹˜: 1, 4, 7\nstate_indices = [1, 4, 7]\n\nfor idx in state_indices:\n    visible = sequence[:idx+1]  # ìê¸° ìì‹ ê¹Œì§€ ë³¼ ìˆ˜ ìˆìŒ\n    action_to_predict = sequence[idx+1] if idx+1 < len(sequence) else \"?\"\n    \n    print(f\"\\nìœ„ì¹˜ {idx} ({sequence[idx]})ì—ì„œ:\")\n    print(f\"  ë³¼ ìˆ˜ ìˆëŠ” í† í°: {visible}\")\n    print(f\"  ë³¼ ìˆ˜ ì—†ëŠ” í† í°: {sequence[idx+1:]}\")\n    print(f\"  â†’ {visible[-2]}(RTG)ì™€ {visible[-1]}(State)ë¥¼ ë³´ê³ \")\n    print(f\"  â†’ {action_to_predict}ë¥¼ ì˜ˆì¸¡!\")\n\nprint()\nprint(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\nprint(\"â”‚  í•µì‹¬: State ìœ„ì¹˜ì—ì„œëŠ” í•´ë‹¹ ì‹œì ì˜ RTGì™€ Stateë§Œ    â”‚\")\nprint(\"â”‚        ë³¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ì´ ì •ë³´ë¡œ Actionì„ ì˜ˆì¸¡     â”‚\")\nprint(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ì‹œí€€ìŠ¤ êµ¬ì¡° ë° ì˜ˆì¸¡ ìœ„ì¹˜ ì‹œê°í™”\n\n# colors ì •ì˜ (ì‹œê°í™”ìš©)\ncolors = {'RTG': '#ff6b6b', 'State': '#4ecdc4', 'Action': '#45b7d1'}\n\n# ì‹œí€€ìŠ¤ ìƒì„± (cell-11ì—ì„œ ì •ì˜ëœ í•¨ìˆ˜ ì‚¬ìš©)\ndef create_dt_sequence(states, actions, rewards):\n    rtgs = calculate_rtg(rewards)\n    sequence = []\n    for rtg, state, action in zip(rtgs, states, actions):\n        sequence.append(('RTG', rtg))\n        sequence.append(('State', state))\n        sequence.append(('Action', action))\n    return sequence\n\nstates = ['s0', 's1', 's2', 's3']\nactions = ['a0', 'a1', 'a2', 'a3']\nrewards = [1, 2, 3, 4]\nsequence = create_dt_sequence(states, actions, rewards)\n\n# ì‹œê°í™”\nfig, ax = plt.subplots(figsize=(14, 4))\n\n# ì…ë ¥ ì‹œí€€ìŠ¤\nfor i, (token_type, value) in enumerate(sequence):\n    color = colors[token_type]\n    rect = plt.Rectangle((i, 1), 0.9, 1, facecolor=color, edgecolor='black', alpha=0.7)\n    ax.add_patch(rect)\n    ax.text(i + 0.45, 1.5, str(value), ha='center', va='center', fontsize=9)\n\n# ì˜ˆì¸¡ í™”ì‚´í‘œ (State ìœ„ì¹˜ì—ì„œ)\nstate_positions = [i for i, (t, _) in enumerate(sequence) if t == 'State']\nfor i, pos in enumerate(state_positions):\n    ax.annotate('', xy=(pos + 0.45, 0.7), xytext=(pos + 0.45, 0.9),\n                arrowprops=dict(arrowstyle='->', color='red', lw=2))\n    ax.text(pos + 0.45, 0.4, f'â†’a{i}', ha='center', va='center', \n            fontsize=11, fontweight='bold', color='red')\n\nax.set_xlim(-0.5, len(sequence) + 0.5)\nax.set_ylim(0, 2.5)\nax.set_xticks(range(len(sequence)))\nax.set_xlabel('Position')\nax.set_yticks([0.5, 1.5])\nax.set_yticklabels(['Prediction', 'Input'])\nax.set_title('State ìœ„ì¹˜(1::3)ì—ì„œ Action ì˜ˆì¸¡')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"State ìœ„ì¹˜: {state_positions} (ì¸ë±ìŠ¤ 1::3)\")\nprint(f\"ì´ ìœ„ì¹˜ë“¤ì—ì„œ ë‹¤ìŒ Actionì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Causal Maskingê³¼ í•¨ê»˜ ì´í•´í•˜ê¸°\n",
    "\n",
    "Causal masking ë•ë¶„ì— ê° ìœ„ì¹˜ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” ì •ë³´ê°€ ì œí•œë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê° State ìœ„ì¹˜ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” ì •ë³´\n",
    "print(\"ê° State ìœ„ì¹˜ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” ì •ë³´ (Causal Masking):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for t, pos in enumerate(state_positions):\n",
    "    visible = sequence[:pos+1]  # ìê¸° ìì‹ ê¹Œì§€ ë³¼ ìˆ˜ ìˆìŒ\n",
    "    visible_str = [f\"{v}\" for _, v in visible]\n",
    "    print(f\"\\ns{t} ìœ„ì¹˜ (pos={pos})ì—ì„œ a{t} ì˜ˆì¸¡:\")\n",
    "    print(f\"  ë³¼ ìˆ˜ ìˆëŠ” ì •ë³´: {visible_str}\")\n",
    "    print(f\"  â†’ R{t}ì™€ s{t}ë¥¼ ë³´ê³  a{t}ë¥¼ ì˜ˆì¸¡!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 3.1 ê¸°ì¡´ RL: Bellman Equation ê¸°ë°˜\n\nê¸°ì¡´ ê°•í™”í•™ìŠµì˜ í•µì‹¬ ì•„ì´ë””ì–´:\n\n```\n\"ìµœì ì˜ í–‰ë™ = ë¯¸ë˜ ë³´ìƒì˜ ê¸°ëŒ€ê°’ì„ ìµœëŒ€í™”í•˜ëŠ” í–‰ë™\"\n```\n\n**Q-Learning** (Value-based):\n$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n\n**Policy Gradient** (Policy-based):\n$$\\nabla J(\\theta) = \\mathbb{E}[\\nabla \\log \\pi_\\theta(a|s) \\cdot A(s, a)]$$\n\ní•µì‹¬ì€ **Value Function** (Q ë˜ëŠ” V)ì„ ì¶”ì •í•œë‹¤ëŠ” ê²ƒ!"
  },
  {
   "cell_type": "code",
   "source": "# Decision Transformer ì ‘ê·¼ë²• (ì˜ì‚¬ ì½”ë“œ)\n\nclass SimpleDTConcept:\n    \"\"\"\n    Decision Transformer ê°œë… ì„¤ëª…ìš© í´ë˜ìŠ¤\n    \n    í•µì‹¬ ì°¨ì´:\n    - Q-table/Value function ì—†ìŒ!\n    - ê·¸ëƒ¥ ì…ë ¥â†’ì¶œë ¥ ë§¤í•‘ì„ í•™ìŠµ\n    \"\"\"\n    def __init__(self):\n        # Transformer ëª¨ë¸ (ì—¬ê¸°ì„œëŠ” ê°œë…ë§Œ)\n        self.model = \"Transformer(RTG, State â†’ Action)\"\n    \n    def train(self, dataset):\n        \"\"\"\n        í•™ìŠµ: ê³¼ê±° ë°ì´í„°ì—ì„œ íŒ¨í„´ í•™ìŠµ\n        \n        ë°ì´í„°ì…‹: [(rtg, state, action), ...]\n        ì†ì‹¤: MSE(ì˜ˆì¸¡ action, ì‹¤ì œ action)\n        \n        Q-Learningê³¼ ë‹¬ë¦¬:\n        - TD error ê³„ì‚° ì—†ìŒ\n        - Bellman equation ì—†ìŒ\n        - ê·¸ëƒ¥ Supervised Learning!\n        \"\"\"\n        for rtg, state, action in dataset:\n            predicted_action = self.model(rtg, state)\n            loss = (predicted_action - action) ** 2  # MSE\n            # loss.backward() & optimizer.step()\n    \n    def select_action(self, state, target_return):\n        \"\"\"\n        ì¶”ë¡ : ëª©í‘œ returnê³¼ stateë¥¼ ì£¼ë©´ action ì¶œë ¥\n        \n        Q-Learning: argmax_a Q(s, a)  â† Qê°’ ê³„ì‚° í•„ìš”\n        DT:         model(target_return, state)  â† ê·¸ëƒ¥ forward pass\n        \"\"\"\n        return self.model(target_return, state)\n\n# ë‘ ì ‘ê·¼ë²• ë¹„êµ\nprint(\"=\" * 60)\nprint(\"ë‘ ì ‘ê·¼ë²• ë¹„êµ\")\nprint(\"=\" * 60)\n\ncomparison_steps = \"\"\"\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    Q-Learning                                â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  1. í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ê²½í—˜ ìˆ˜ì§‘                            â”‚\nâ”‚  2. Q(s,a) ê°’ì„ Bellman equationìœ¼ë¡œ ì—…ë°ì´íŠ¸                â”‚\nâ”‚  3. ì¶”ë¡ : argmax_a Q(s,a) ë¡œ í–‰ë™ ì„ íƒ                       â”‚\nâ”‚                                                              â”‚\nâ”‚  í•„ìš”í•œ ê²ƒ: Q-table ë˜ëŠ” Q-network, TD error ê³„ì‚°            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                Decision Transformer                          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  1. ê³¼ê±° ë°ì´í„°(ê¶¤ì ) ìˆ˜ì§‘ (Offline)                         â”‚\nâ”‚  2. (RTG, State) â†’ Action ë§¤í•‘ì„ í•™ìŠµ                        â”‚\nâ”‚  3. ì¶”ë¡ : model(target_RTG, state) ë¡œ í–‰ë™ ì¶œë ¥             â”‚\nâ”‚                                                              â”‚\nâ”‚  í•„ìš”í•œ ê²ƒ: Transformer, MSE loss                            â”‚\nâ”‚  í•„ìš” ì—†ëŠ” ê²ƒ: Value function, Bellman equation              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\"\"\"\nprint(comparison_steps)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.3 ì¥ë‹¨ì  ë¹„êµ\n\n**Decision Transformerì˜ ì¥ì :**\n\n```\n1. ë‹¨ìˆœí•¨\n   - Supervised learningìœ¼ë¡œ ê·€ê²°\n   - êµ¬í˜„ì´ ì‰½ê³  ë””ë²„ê¹…ì´ ì‰¬ì›€\n\n2. ì•ˆì •ì„±\n   - Value function ì¶”ì •ì˜ ë¶ˆì•ˆì •ì„± ì—†ìŒ\n   - Q-learningì˜ overestimation ë¬¸ì œ ì—†ìŒ\n\n3. ëª©í‘œ ì§€í–¥ì \n   - ì›í•˜ëŠ” returnì„ ì§ì ‘ ì§€ì • ê°€ëŠ¥\n   - \"100ì  ì§œë¦¬ í”Œë ˆì´\" vs \"50ì  ì§œë¦¬ í”Œë ˆì´\" ì„ íƒ ê°€ëŠ¥\n\n4. ì¥ê¸° ì˜ì¡´ì„±\n   - Transformerê°€ ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ì— ê°•í•¨\n   - ê³¼ê±° ë§ì€ ìŠ¤í…ì˜ ì •ë³´ í™œìš© ê°€ëŠ¥\n```\n\n**ë‹¨ì /í•œê³„:**\n\n```\n1. Offline ë°ì´í„° ì˜ì¡´\n   - í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” í–‰ë™ì€ ëª» ë°°ì›€\n   - ë°ì´í„° í’ˆì§ˆì´ ì„±ëŠ¥ì„ ê²°ì •\n\n2. ëª©í‘œ ì„¤ì •ì˜ ì–´ë ¤ì›€\n   - ì ì ˆí•œ target RTG ì„¤ì •ì´ í•„ìš”\n   - ë„ˆë¬´ ë†’ìœ¼ë©´ ì´ìƒí•œ í–‰ë™, ë„ˆë¬´ ë‚®ìœ¼ë©´ ì„±ëŠ¥ ì €í•˜\n\n3. ê³„ì‚° ë¹„ìš©\n   - TransformerëŠ” ì—°ì‚°ëŸ‰ì´ ë§ìŒ\n   - ì‹¤ì‹œê°„ ì ìš©ì— ì œì•½\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ê¸°ì¡´ RL: Bellman Equation\n",
    "\n",
    "**Q-Learning**:\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n",
    "\n",
    "**Policy Gradient**:\n",
    "$$\\nabla J(\\theta) = \\mathbb{E}[\\nabla \\log \\pi_\\theta(a|s) \\cdot A(s, a)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Decision Transformer: Sequence Modeling\n",
    "\n",
    "**í•µì‹¬ ì•„ì´ë””ì–´**:\n",
    "- Bellman equation ì—†ìŒ\n",
    "- Value function ì—†ìŒ\n",
    "- ë‹¨ìˆœíˆ **ì‹œí€€ìŠ¤ ì˜ˆì¸¡ ë¬¸ì œ**ë¡œ ë³€í™˜\n",
    "\n",
    "$$\\pi(a | s, RÌ‚) = \\text{Transformer}(RÌ‚_1, s_1, a_1, ..., RÌ‚_t, s_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¹„êµ í‘œ\n",
    "comparison = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚        í•­ëª©         â”‚       ê¸°ì¡´ RL          â”‚  Decision Transformer  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ í•™ìŠµ ëª©í‘œ           â”‚ Value function ì¶”ì •    â”‚ ë‹¤ìŒ Action ì˜ˆì¸¡       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ ì†ì‹¤ í•¨ìˆ˜           â”‚ TD error, PG loss      â”‚ Cross-entropy / MSE    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ ì¡°ê±´í™”              â”‚ Stateë§Œ                â”‚ State + RTG (ëª©í‘œ)     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ ì‹œê°„ì  ì˜ì¡´ì„±       â”‚ 1-step (Markov)        â”‚ ì „ì²´ ì»¨í…ìŠ¤íŠ¸ (K steps)â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ ì•„í‚¤í…ì²˜            â”‚ MLP, CNN               â”‚ Transformer            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ ë°ì´í„° í™œìš©         â”‚ Online / Offline       â”‚ Offline                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class SimpleEmbeddings(nn.Module):\n    \"\"\"\n    Decision Transformerì˜ ì„ë² ë”© ë ˆì´ì–´ (ê°„ë‹¨ ë²„ì „)\n    \n    ì—­í• : RTG, State, Actionì„ ë™ì¼í•œ ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜\n    \"\"\"\n    def __init__(self, state_dim, action_dim, hidden_size, max_timestep=1000):\n        super().__init__()\n        \n        # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        # â”‚  ê° ì…ë ¥ íƒ€ì…ë³„ ì„ë² ë”© ë ˆì´ì–´                       â”‚\n        # â”‚                                                     â”‚\n        # â”‚  RTG:    ìŠ¤ì¹¼ë¼(1) â†’ hidden_size ë²¡í„°              â”‚\n        # â”‚  State:  state_dim â†’ hidden_size ë²¡í„°              â”‚\n        # â”‚  Action: action_dim â†’ hidden_size ë²¡í„°             â”‚\n        # â”‚                                                     â”‚\n        # â”‚  ëª¨ë‘ ê°™ì€ hidden_sizeë¡œ ë³€í™˜í•´ì•¼ ì‹œí€€ìŠ¤ êµ¬ì„± ê°€ëŠ¥! â”‚\n        # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        self.embed_state = nn.Linear(state_dim, hidden_size)\n        self.embed_action = nn.Linear(action_dim, hidden_size)\n        self.embed_return = nn.Linear(1, hidden_size)  # RTGëŠ” ìŠ¤ì¹¼ë¼\n        \n        # Timestep ì„ë² ë”© (Phase 1ì—ì„œ ë°°ìš´ Learned Positional Embedding!)\n        self.embed_timestep = nn.Embedding(max_timestep, hidden_size)\n        \n        # Layer normalization (í•™ìŠµ ì•ˆì •í™”)\n        self.embed_ln = nn.LayerNorm(hidden_size)\n        \n    def forward(self, states, actions, returns_to_go, timesteps):\n        \"\"\"\n        Args:\n            states: (batch, seq_len, state_dim)\n            actions: (batch, seq_len, action_dim)\n            returns_to_go: (batch, seq_len, 1)\n            timesteps: (batch, seq_len)\n        Returns:\n            token_embeddings: (batch, seq_len*3, hidden_size)\n        \"\"\"\n        batch_size, seq_len = states.shape[0], states.shape[1]\n        \n        # Step 1: ê° ì…ë ¥ì„ hidden_size ì°¨ì›ìœ¼ë¡œ ë³€í™˜\n        state_emb = self.embed_state(states)      # (B, T, H)\n        action_emb = self.embed_action(actions)   # (B, T, H)\n        return_emb = self.embed_return(returns_to_go)  # (B, T, H)\n        \n        # Step 2: Timestep ì„ë² ë”© ì¶”ê°€ (ìœ„ì¹˜ ì •ë³´)\n        time_emb = self.embed_timestep(timesteps)  # (B, T, H)\n        \n        state_emb = state_emb + time_emb\n        action_emb = action_emb + time_emb\n        return_emb = return_emb + time_emb\n        \n        # Step 3: ì‹œí€€ìŠ¤ ì¸í„°ë¦¬ë¹™\n        # [Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚, aâ‚, ...] ìˆœì„œë¡œ ë°°ì¹˜\n        # (B, T, 3, H) â†’ (B, T*3, H)\n        stacked = torch.stack([return_emb, state_emb, action_emb], dim=2)\n        token_embeddings = stacked.reshape(batch_size, seq_len * 3, -1)\n        \n        return self.embed_ln(token_embeddings)\n\n# í…ŒìŠ¤íŠ¸: ì„ë² ë”© ë ˆì´ì–´ ë™ì‘ í™•ì¸\nprint(\"=\" * 60)\nprint(\"ì„ë² ë”© ë ˆì´ì–´ í…ŒìŠ¤íŠ¸\")\nprint(\"=\" * 60)\n\nembedder = SimpleEmbeddings(\n    state_dim=4,      # ìƒíƒœ ë²¡í„° ì°¨ì› (ì˜ˆ: CartPole)\n    action_dim=2,     # í–‰ë™ ë²¡í„° ì°¨ì› (ì˜ˆ: ì¢Œ/ìš°)\n    hidden_size=32    # ì„ë² ë”© ì°¨ì›\n)\n\n# ì„ë² ë”© ë ˆì´ì–´ íŒŒë¼ë¯¸í„° í™•ì¸\nprint(\"\\nì„ë² ë”© ë ˆì´ì–´ êµ¬ì¡°:\")\nfor name, param in embedder.named_parameters():\n    print(f\"  {name}: {param.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 ì„ë² ë”© ë ˆì´ì–´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Decision Transformerì˜ ì„ë² ë”© ë ˆì´ì–´ (ê°„ë‹¨ ë²„ì „)\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_size, max_timestep=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ê° ì…ë ¥ íƒ€ì…ë³„ ì„ë² ë”©\n",
    "        self.embed_state = nn.Linear(state_dim, hidden_size)\n",
    "        self.embed_action = nn.Linear(action_dim, hidden_size)\n",
    "        self.embed_return = nn.Linear(1, hidden_size)\n",
    "        \n",
    "        # Timestep ì„ë² ë”©\n",
    "        self.embed_timestep = nn.Embedding(max_timestep, hidden_size)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.embed_ln = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, states, actions, returns_to_go, timesteps):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            states: (batch, seq_len, state_dim)\n",
    "            actions: (batch, seq_len, action_dim)\n",
    "            returns_to_go: (batch, seq_len, 1)\n",
    "            timesteps: (batch, seq_len)\n",
    "        Returns:\n",
    "            token_embeddings: (batch, seq_len*3, hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = states.shape[0], states.shape[1]\n",
    "        \n",
    "        # ê° ì…ë ¥ ì„ë² ë”©\n",
    "        state_emb = self.embed_state(states)      # (B, T, H)\n",
    "        action_emb = self.embed_action(actions)   # (B, T, H)\n",
    "        return_emb = self.embed_return(returns_to_go)  # (B, T, H)\n",
    "        \n",
    "        # Timestep ì„ë² ë”© ì¶”ê°€\n",
    "        time_emb = self.embed_timestep(timesteps)  # (B, T, H)\n",
    "        \n",
    "        state_emb = state_emb + time_emb\n",
    "        action_emb = action_emb + time_emb\n",
    "        return_emb = return_emb + time_emb\n",
    "        \n",
    "        # ì‹œí€€ìŠ¤ ì¸í„°ë¦¬ë¹™: [R, s, a, R, s, a, ...]\n",
    "        # (B, T, 3, H) â†’ (B, T*3, H)\n",
    "        stacked = torch.stack([return_emb, state_emb, action_emb], dim=2)\n",
    "        token_embeddings = stacked.reshape(batch_size, seq_len * 3, -1)\n",
    "        \n",
    "        return self.embed_ln(token_embeddings)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "embedder = SimpleEmbeddings(\n",
    "    state_dim=4,\n",
    "    action_dim=2,\n",
    "    hidden_size=32\n",
    ")\n",
    "\n",
    "# ë”ë¯¸ ë°ì´í„°\n",
    "batch_size, seq_len = 2, 5\n",
    "states = torch.randn(batch_size, seq_len, 4)\n",
    "actions = torch.randn(batch_size, seq_len, 2)\n",
    "rtgs = torch.randn(batch_size, seq_len, 1)\n",
    "timesteps = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "embeddings = embedder(states, actions, rtgs, timesteps)\n",
    "\n",
    "print(f\"ì…ë ¥:\")\n",
    "print(f\"  States: {states.shape}\")\n",
    "print(f\"  Actions: {actions.shape}\")\n",
    "print(f\"  RTGs: {rtgs.shape}\")\n",
    "print(f\"  Timesteps: {timesteps.shape}\")\n",
    "print(f\"\\nì¶œë ¥ (ì¸í„°ë¦¬ë¹™ëœ ì‹œí€€ìŠ¤):\")\n",
    "print(f\"  Embeddings: {embeddings.shape}\")\n",
    "print(f\"  (batch=2, seq_len=5*3=15, hidden=32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Action ì˜ˆì¸¡ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_action_predictions(transformer_output, seq_len):\n",
    "    \"\"\"\n",
    "    Transformer ì¶œë ¥ì—ì„œ Action ì˜ˆì¸¡ë§Œ ì¶”ì¶œ\n",
    "    \n",
    "    ì‹œí€€ìŠ¤: [Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚, aâ‚, ...]\n",
    "    ìœ„ì¹˜:    0   1   2   3   4   5\n",
    "    \n",
    "    Action ì˜ˆì¸¡ì€ State ìœ„ì¹˜ (1, 4, 7, ... = 1::3)ì—ì„œ ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    # State ìœ„ì¹˜ì—ì„œ ì˜ˆì¸¡ ì¶”ì¶œ\n",
    "    action_preds = transformer_output[:, 1::3, :]  # (B, T, H)\n",
    "    return action_preds\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "# ê°€ìƒì˜ transformer ì¶œë ¥\n",
    "fake_output = torch.randn(2, 15, 32)  # (batch=2, seq=15, hidden=32)\n",
    "\n",
    "action_preds = extract_action_predictions(fake_output, seq_len=5)\n",
    "\n",
    "print(f\"Transformer ì¶œë ¥: {fake_output.shape}\")\n",
    "print(f\"Action ì˜ˆì¸¡ (1::3 ìœ„ì¹˜): {action_preds.shape}\")\n",
    "print(f\"\\nì¶”ì¶œëœ ìœ„ì¹˜: {list(range(1, 15, 3))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.3 ê°„ë‹¨í•œ Decision Transformer ëª¨ë¸\n\nì•„ë˜ëŠ” `SimpleDecisionTransformer`ì˜ ì „ì²´ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤:\n\n```mermaid\nflowchart TB\n    subgraph Inputs[\"ğŸ“¥ ì…ë ¥\"]\n        RTG[\"RTG<br/>(batch, seq, 1)\"]\n        State[\"State<br/>(batch, seq, state_dim)\"]\n        Action[\"Action<br/>(batch, seq, action_dim)\"]\n        Time[\"Timestep<br/>(batch, seq)\"]\n    end\n\n    subgraph Embedding[\"1ï¸âƒ£ ì„ë² ë”© ë ˆì´ì–´\"]\n        direction TB\n        RTG --> |\"Linear(1â†’H)\"| RTG_emb[\"RTG Embedding<br/>(batch, seq, H)\"]\n        State --> |\"Linear(state_dimâ†’H)\"| State_emb[\"State Embedding<br/>(batch, seq, H)\"]\n        Action --> |\"Linear(action_dimâ†’H)\"| Action_emb[\"Action Embedding<br/>(batch, seq, H)\"]\n        Time --> |\"Embedding(max_t, H)\"| Time_emb[\"Time Embedding<br/>(batch, seq, H)\"]\n        \n        RTG_emb --> |\"+\"| RTG_final[\"R + T\"]\n        Time_emb --> RTG_final\n        State_emb --> |\"+\"| State_final[\"S + T\"]\n        Time_emb --> State_final\n        Action_emb --> |\"+\"| Action_final[\"A + T\"]\n        Time_emb --> Action_final\n    end\n\n    subgraph Interleave[\"2ï¸âƒ£ ì‹œí€€ìŠ¤ ì¸í„°ë¦¬ë¹™\"]\n        RTG_final --> Stack\n        State_final --> Stack\n        Action_final --> Stack\n        Stack[\"torch.stack & reshape\"] --> Sequence[\"[Râ‚€,sâ‚€,aâ‚€,Râ‚,sâ‚,aâ‚,...]<br/>(batch, seqÃ—3, H)\"]\n        Sequence --> LN[\"LayerNorm\"]\n    end\n\n    subgraph Transformer[\"3ï¸âƒ£ Transformer Encoder\"]\n        LN --> Mask[\"Causal Mask ì ìš©<br/>(ë¯¸ë˜ í† í° ì°¨ë‹¨)\"]\n        Mask --> TF[\"TransformerEncoder<br/>Ã— n_layers\"]\n        TF --> Output[\"(batch, seqÃ—3, H)\"]\n    end\n\n    subgraph Prediction[\"4ï¸âƒ£ Action ì˜ˆì¸¡\"]\n        Output --> Extract[\"State ìœ„ì¹˜ ì¶”ì¶œ<br/>[:, 1::3, :]\"]\n        Extract --> Head[\"Linear(Hâ†’action_dim)\"]\n        Head --> ActionPred[\"Action Predictions<br/>(batch, seq, action_dim)\"]\n    end\n\n    style Inputs fill:#e1f5fe\n    style Embedding fill:#fff3e0\n    style Interleave fill:#f3e5f5\n    style Transformer fill:#e8f5e9\n    style Prediction fill:#ffebee\n```\n\n**í•µì‹¬ êµ¬ì„± ìš”ì†Œ:**\n\n| ë‹¨ê³„ | ë ˆì´ì–´ | ì—­í•  |\n|:---:|:---|:---|\n| 1ï¸âƒ£ | `embed_*` + `embed_timestep` | ê° ì…ë ¥ì„ hidden_size ì°¨ì›ìœ¼ë¡œ ë³€í™˜ + ì‹œê°„ ì •ë³´ ì¶”ê°€ |\n| 2ï¸âƒ£ | `torch.stack` + `reshape` | (R,s,a) íŠ¸ë¦¬í”Œì„ ì¸í„°ë¦¬ë¹™í•˜ì—¬ ì‹œí€€ìŠ¤ êµ¬ì„± |\n| 3ï¸âƒ£ | `TransformerEncoder` | Causal Attentionìœ¼ë¡œ ì‹œí€€ìŠ¤ ì²˜ë¦¬ |\n| 4ï¸âƒ£ | `predict_action` | State ìœ„ì¹˜(1::3)ì—ì„œ Action ì˜ˆì¸¡ |"
  },
  {
   "cell_type": "code",
   "source": "class SimpleDecisionTransformer(nn.Module):\n    \"\"\"\n    ê°„ë‹¨í•œ Decision Transformer ëª¨ë¸\n    \n    í•µì‹¬ êµ¬ì¡°:\n    1. ì„ë² ë”©: RTG, State, Action â†’ ë™ì¼ ì°¨ì› ë²¡í„°\n    2. Transformer: ì‹œí€€ìŠ¤ ì²˜ë¦¬ (Causal Masking)\n    3. ì˜ˆì¸¡ í—¤ë“œ: State ìœ„ì¹˜ì—ì„œ Action ì˜ˆì¸¡\n    \"\"\"\n    def __init__(self, state_dim, action_dim, hidden_size=64, n_heads=4, n_layers=2, max_timestep=1000):\n        super().__init__()\n        \n        self.hidden_size = hidden_size\n        \n        # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        # â”‚  1. ì„ë² ë”© ë ˆì´ì–´                                   â”‚\n        # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        self.embed_state = nn.Linear(state_dim, hidden_size)\n        self.embed_action = nn.Linear(action_dim, hidden_size)\n        self.embed_return = nn.Linear(1, hidden_size)\n        self.embed_timestep = nn.Embedding(max_timestep, hidden_size)\n        self.embed_ln = nn.LayerNorm(hidden_size)\n        \n        # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        # â”‚  2. Transformer (Causal Self-Attention)             â”‚\n        # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_size,\n            nhead=n_heads,\n            dim_feedforward=hidden_size * 4,\n            dropout=0.1,\n            activation='gelu',\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n        \n        # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        # â”‚  3. ì˜ˆì¸¡ í—¤ë“œ (Action ì˜ˆì¸¡)                         â”‚\n        # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        self.predict_action = nn.Linear(hidden_size, action_dim)\n    \n    def forward(self, states, actions, returns_to_go, timesteps):\n        \"\"\"\n        Args:\n            states: (batch, seq_len, state_dim)\n            actions: (batch, seq_len, action_dim)\n            returns_to_go: (batch, seq_len, 1)\n            timesteps: (batch, seq_len)\n        Returns:\n            action_preds: (batch, seq_len, action_dim)\n        \"\"\"\n        batch_size, seq_len = states.shape[0], states.shape[1]\n        \n        # Step 1: ì„ë² ë”©\n        state_emb = self.embed_state(states)\n        action_emb = self.embed_action(actions)\n        return_emb = self.embed_return(returns_to_go)\n        time_emb = self.embed_timestep(timesteps)\n        \n        # Timestep ì„ë² ë”© ì¶”ê°€\n        state_emb = state_emb + time_emb\n        action_emb = action_emb + time_emb\n        return_emb = return_emb + time_emb\n        \n        # Step 2: ì‹œí€€ìŠ¤ ì¸í„°ë¦¬ë¹™ [R, s, a, R, s, a, ...]\n        stacked = torch.stack([return_emb, state_emb, action_emb], dim=2)\n        sequence = stacked.reshape(batch_size, seq_len * 3, self.hidden_size)\n        sequence = self.embed_ln(sequence)\n        \n        # Step 3: Causal Mask ìƒì„±\n        # ê° í† í°ì€ ìê¸° ìì‹ ê³¼ ì´ì „ í† í°ë§Œ ë³¼ ìˆ˜ ìˆìŒ\n        seq_length = seq_len * 3\n        causal_mask = torch.triu(\n            torch.ones(seq_length, seq_length, device=sequence.device),\n            diagonal=1\n        ).bool()\n        \n        # Step 4: Transformer í†µê³¼\n        output = self.transformer(sequence, mask=causal_mask)\n        \n        # Step 5: State ìœ„ì¹˜(1::3)ì—ì„œ Action ì˜ˆì¸¡ ì¶”ì¶œ\n        state_outputs = output[:, 1::3, :]  # (batch, seq_len, hidden)\n        action_preds = self.predict_action(state_outputs)\n        \n        return action_preds\n\n# ëª¨ë¸ ìƒì„± ë° í…ŒìŠ¤íŠ¸\nprint(\"=\" * 60)\nprint(\"Simple Decision Transformer í…ŒìŠ¤íŠ¸\")\nprint(\"=\" * 60)\n\nmodel = SimpleDecisionTransformer(\n    state_dim=4,      # ì˜ˆ: CartPole ìƒíƒœ ì°¨ì›\n    action_dim=2,     # ì˜ˆ: ì¢Œ/ìš° í–‰ë™\n    hidden_size=32,\n    n_heads=2,\n    n_layers=2\n)\n\n# ë”ë¯¸ ë°ì´í„°\nbatch_size, seq_len = 2, 5\nstates = torch.randn(batch_size, seq_len, 4)\nactions = torch.randn(batch_size, seq_len, 2)\nrtgs = torch.randn(batch_size, seq_len, 1)\ntimesteps = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)\n\n# Forward pass\naction_preds = model(states, actions, rtgs, timesteps)\n\nprint(f\"\\nì…ë ¥:\")\nprint(f\"  States:    {states.shape}\")\nprint(f\"  Actions:   {actions.shape}\")\nprint(f\"  RTGs:      {rtgs.shape}\")\nprint(f\"  Timesteps: {timesteps.shape}\")\nprint(f\"\\nì¶œë ¥:\")\nprint(f\"  Action predictions: {action_preds.shape}\")\nprint(f\"\\nëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def train_step(model, optimizer, states, actions, rtgs, timesteps):\n    \"\"\"\n    Decision Transformer í•™ìŠµ ìŠ¤í… (ì˜ì‚¬ ì½”ë“œ)\n    \n    í•µì‹¬: Supervised Learningê³¼ ë™ì¼!\n    - ì…ë ¥: (RTG, State, Action) ì‹œí€€ìŠ¤\n    - ì¶œë ¥: Action ì˜ˆì¸¡\n    - ì†ì‹¤: ì˜ˆì¸¡ Action vs ì‹¤ì œ Action (MSE)\n    \"\"\"\n    model.train()\n    optimizer.zero_grad()\n    \n    # Forward pass\n    action_preds = model(states, actions, rtgs, timesteps)\n    \n    # Loss ê³„ì‚° (MSE: ì—°ì† í–‰ë™ ê³µê°„)\n    # ë˜ëŠ” CrossEntropy: ì´ì‚° í–‰ë™ ê³µê°„\n    loss = nn.MSELoss()(action_preds, actions)\n    \n    # Backward pass\n    loss.backward()\n    optimizer.step()\n    \n    return loss.item()\n\n# í•™ìŠµ ë£¨í”„ ì˜ˆì‹œ\nprint(\"=\" * 60)\nprint(\"í•™ìŠµ ë£¨í”„ ì˜ˆì‹œ\")\nprint(\"=\" * 60)\n\n# ëª¨ë¸ê³¼ ì˜µí‹°ë§ˆì´ì €\nmodel = SimpleDecisionTransformer(state_dim=4, action_dim=2, hidden_size=32)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# ë”ë¯¸ í•™ìŠµ ë°ì´í„°\nbatch_size, seq_len = 8, 10\ndummy_data = {\n    'states': torch.randn(batch_size, seq_len, 4),\n    'actions': torch.randn(batch_size, seq_len, 2),\n    'rtgs': torch.randn(batch_size, seq_len, 1),\n    'timesteps': torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)\n}\n\n# ê°„ë‹¨í•œ í•™ìŠµ ë£¨í”„\nprint(\"\\ní•™ìŠµ ì§„í–‰:\")\nfor epoch in range(5):\n    loss = train_step(\n        model, optimizer,\n        dummy_data['states'],\n        dummy_data['actions'],\n        dummy_data['rtgs'],\n        dummy_data['timesteps']\n    )\n    print(f\"  Epoch {epoch+1}: Loss = {loss:.4f}\")\n\nprint()\nprint(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\nprint(\"â”‚  í•µì‹¬ í¬ì¸íŠ¸:                                           â”‚\")\nprint(\"â”‚  1. Bellman equation ì—†ìŒ - ê·¸ëƒ¥ MSE Loss!              â”‚\")\nprint(\"â”‚  2. TD error ê³„ì‚° ì—†ìŒ - Supervised Learning!           â”‚\")\nprint(\"â”‚  3. Value function ì—†ìŒ - ì§ì ‘ Action ì˜ˆì¸¡!             â”‚\")\nprint(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 í•™ìŠµ ë£¨í”„ ìŠ¤ì¼€ì¹˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ì—°ìŠµ ë¬¸ì œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì—°ìŠµ 1: RTG ê³„ì‚°\n",
    "\n",
    "ë‹¤ìŒ ë³´ìƒ ì‹œí€€ìŠ¤ì— ëŒ€í•´ RTGë¥¼ ê³„ì‚°í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [10, -5, 20, 15, -3, 8]\n",
    "\n",
    "# TODO: RTG ê³„ì‚°\n",
    "rtg = []\n",
    "\n",
    "# ì •ë‹µ í™•ì¸\n",
    "# print(f\"RTG: {rtg}\")\n",
    "# ì˜ˆìƒ: [45, 35, 40, 20, 5, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "rtg = calculate_rtg(rewards)\n",
    "# ë˜ëŠ”\n",
    "rtg = [sum(rewards[i:]) for i in range(len(rewards))]\n",
    "print(f\"RTG: {rtg}\")  # [45, 35, 40, 20, 5, 8]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì—°ìŠµ 2: ì‹œí€€ìŠ¤ ì¸ë±ì‹±\n",
    "\n",
    "ì‹œí€€ìŠ¤ `[R0, s0, a0, R1, s1, a1, R2, s2, a2, R3, s3, a3]`ì—ì„œ:\n",
    "\n",
    "1. RTG ìœ„ì¹˜ëŠ”?\n",
    "2. State ìœ„ì¹˜ëŠ”?\n",
    "3. Action ìœ„ì¹˜ëŠ”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = ['R0', 's0', 'a0', 'R1', 's1', 'a1', 'R2', 's2', 'a2', 'R3', 's3', 'a3']\n",
    "\n",
    "# TODO: ê° íƒ€ì…ì˜ ìœ„ì¹˜ ì°¾ê¸°\n",
    "# rtg_positions = sequence[?::?]\n",
    "# state_positions = sequence[?::?]\n",
    "# action_positions = sequence[?::?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "rtg_positions = sequence[0::3]    # ['R0', 'R1', 'R2', 'R3']\n",
    "state_positions = sequence[1::3]  # ['s0', 's1', 's2', 's3']\n",
    "action_positions = sequence[2::3] # ['a0', 'a1', 'a2', 'a3']\n",
    "\n",
    "print(f\"RTG ìœ„ì¹˜ (0::3): {rtg_positions}\")\n",
    "print(f\"State ìœ„ì¹˜ (1::3): {state_positions}\")\n",
    "print(f\"Action ìœ„ì¹˜ (2::3): {action_positions}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì—°ìŠµ 3: ì¶”ë¡  ì‹œë®¬ë ˆì´ì…˜\n",
    "\n",
    "ëª©í‘œ returnì´ 50ì´ê³ , ë§¤ ìŠ¤í… rewardê°€ `[15, 20, 10]`ì¼ ë•Œ,\n",
    "ê° ìŠ¤í…ì—ì„œì˜ RTGë¥¼ ê³„ì‚°í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_return = 50\n",
    "rewards = [15, 20, 10]\n",
    "\n",
    "# TODO: ê° ìŠ¤í…ì—ì„œì˜ RTG ê³„ì‚°\n",
    "# rtg_step0 = ?\n",
    "# rtg_step1 = ?\n",
    "# rtg_step2 = ?\n",
    "# rtg_step3 = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "rtg = target_return\n",
    "print(f\"Step 0 ì‹œì‘: RTG = {rtg}\")  # 50\n",
    "\n",
    "for i, r in enumerate(rewards):\n",
    "    rtg = rtg - r\n",
    "    print(f\"Step {i} í›„ (reward={r}): RTG = {rtg}\")\n",
    "\n",
    "# Step 0 ì‹œì‘: RTG = 50\n",
    "# Step 0 í›„ (reward=15): RTG = 35\n",
    "# Step 1 í›„ (reward=20): RTG = 15\n",
    "# Step 2 í›„ (reward=10): RTG = 5\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# ğŸ‰ Phase 2 ì™„ë£Œ!\n\n## ğŸ“š ë°°ìš´ ë‚´ìš© ìš”ì•½\n\n### ğŸ¯ Return-to-Go (RTG)\n| ê°œë… | ì„¤ëª… |\n|:---:|:---|\n| **ì •ì˜** | í˜„ì¬ ì‹œì ë¶€í„° ëê¹Œì§€ì˜ ë³´ìƒ í•© |\n| **í•™ìŠµ ì‹œ** | ê³¼ê±° ë°ì´í„°ì—ì„œ ê³„ì‚°ëœ ì‹¤ì œ RTG ì‚¬ìš© |\n| **ì¶”ë¡  ì‹œ** | ì›í•˜ëŠ” ëª©í‘œë¥¼ RTGë¡œ ì„¤ì • |\n| **ì—…ë°ì´íŠ¸** | ìƒˆ RTG = ì´ì „ RTG - ë°›ì€ ë³´ìƒ |\n\n### ğŸ“¦ ì‹œí€€ìŠ¤ êµ¬ì„±\n| ê°œë… | ì„¤ëª… |\n|:---:|:---|\n| **êµ¬ì¡°** | [Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚, aâ‚, ...] |\n| **RTG ìœ„ì¹˜** | 0::3 (0, 3, 6, ...) |\n| **State ìœ„ì¹˜** | 1::3 (1, 4, 7, ...) â† **Action ì˜ˆì¸¡ ìœ„ì¹˜** |\n| **Action ìœ„ì¹˜** | 2::3 (2, 5, 8, ...) |\n\n### ğŸ”„ ê¸°ì¡´ RLê³¼ì˜ ì°¨ì´\n| ê¸°ì¡´ RL | Decision Transformer |\n|:---:|:---:|\n| Value Function ì¶”ì • | ì‹œí€€ìŠ¤ ì˜ˆì¸¡ |\n| Bellman Equation | Supervised Learning |\n| State â†’ Action | (RTG, State) â†’ Action |\n\n---\n\n## â¡ï¸ ë‹¤ìŒ ë‹¨ê³„\n\n**Phase 3: Gym í™˜ê²½ ì‹¤ìŠµ** â†’ `phase3_gym_practice.ipynb`\n\n- ì‹¤ì œ Gym í™˜ê²½ì—ì„œ DT ì‹¤í–‰\n- ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬\n- í•™ìŠµ ë° í‰ê°€\n\n---\n\n## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸\n\në‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°€ê¸° ì „ì— í™•ì¸í•˜ì„¸ìš”:\n\n- [ ] RTGê°€ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤\n- [ ] RTGë¥¼ ì§ì ‘ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤\n- [ ] í•™ìŠµ vs ì¶”ë¡ ì—ì„œ RTGì˜ ì—­í•  ì°¨ì´ë¥¼ ì´í•´í–ˆë‹¤\n- [ ] (RTG, State, Action) ì‹œí€€ìŠ¤ êµ¬ì¡°ë¥¼ ì´í•´í–ˆë‹¤\n- [ ] ì™œ State ìœ„ì¹˜ì—ì„œ Actionì„ ì˜ˆì¸¡í•˜ëŠ”ì§€ ì´í•´í–ˆë‹¤\n- [ ] DTì™€ ê¸°ì¡´ RLì˜ í•µì‹¬ ì°¨ì´ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}