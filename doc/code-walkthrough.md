# Decision Transformer ì½”ë“œ ìƒì„¸ ë¶„ì„

ì´ ë¬¸ì„œëŠ” Decision Transformer (Atari)ì˜ ì£¼ìš” ì½”ë“œ êµ¬í˜„ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

## ëª©ì°¨

1. [ë©”ì¸ ì‹¤í–‰ íë¦„](#1-ë©”ì¸-ì‹¤í–‰-íë¦„)
2. [ë°ì´í„°ì…‹ ìƒì„±](#2-ë°ì´í„°ì…‹-ìƒì„±)
3. [GPT ëª¨ë¸ êµ¬í˜„](#3-gpt-ëª¨ë¸-êµ¬í˜„)
4. [í•™ìŠµ ë£¨í”„](#4-í•™ìŠµ-ë£¨í”„)
5. [í‰ê°€ ë° ìƒ˜í”Œë§](#5-í‰ê°€-ë°-ìƒ˜í”Œë§)

---

## ì „ì²´ ì½”ë“œ ì‹¤í–‰ íë¦„ ê°œìš”

```mermaid
flowchart TB
    subgraph Entry["ğŸ“¥ ì§„ì…ì "]
        Run["run_dt_atari.py<br/>ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸"]
    end

    subgraph Data["ğŸ“Š ë°ì´í„° ì¤€ë¹„"]
        Create["create_dataset.py<br/>DQN ë²„í¼ â†’ RTG ë°ì´í„°ì…‹"]
        Dataset["StateActionReturnDataset<br/>ë°°ì¹˜ ìƒ˜í”Œë§"]
    end

    subgraph Model["ğŸ§  ëª¨ë¸"]
        GPT["model_atari.py<br/>GPT + CNN Encoder"]
        Config["GPTConfig<br/>í•˜ì´í¼íŒŒë¼ë¯¸í„°"]
    end

    subgraph Train["ğŸ¯ í•™ìŠµ"]
        Trainer["trainer_atari.py<br/>Train/Eval ë£¨í”„"]
        Optim["AdamW + LR Decay"]
    end

    subgraph Eval["ğŸ“ˆ í‰ê°€"]
        Sample["utils.py/sample<br/>Action ìƒ˜í”Œë§"]
        Env["Atari Environment<br/>ì‹¤ì œ ê²Œì„ ì‹¤í–‰"]
    end

    Run --> Create
    Create --> Dataset
    Dataset --> Config
    Config --> GPT
    GPT --> Trainer
    Trainer --> Optim
    Optim --> Trainer
    Trainer --> Sample
    Sample --> Env

    style Run fill:#e3f2fd
    style GPT fill:#ff6b6b
    style Trainer fill:#4ecdc4
    style Sample fill:#ffd93d
```

---

## 1. ë©”ì¸ ì‹¤í–‰ íë¦„

**íŒŒì¼:** `atari/run_dt_atari.py`

### 1.1 ì„¤ì • íŒŒë¼ë¯¸í„°

```python
parser = argparse.ArgumentParser()
parser.add_argument('--seed', type=int, default=123)
parser.add_argument('--context_length', type=int, default=30)  # K: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°
parser.add_argument('--epochs', type=int, default=5)
parser.add_argument('--model_type', type=str, default='reward_conditioned')  # 'naive' ë˜ëŠ” 'reward_conditioned'
parser.add_argument('--num_steps', type=int, default=500000)  # ë¡œë“œí•  ì „ì²´ ìŠ¤í… ìˆ˜
parser.add_argument('--num_buffers', type=int, default=50)    # ì‚¬ìš©í•  ë²„í¼ ìˆ˜
parser.add_argument('--game', type=str, default='Breakout')
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--trajectories_per_buffer', type=int, default=10)
parser.add_argument('--data_dir_prefix', type=str, default='./dqn_replay/')
```

**í•µì‹¬ íŒŒë¼ë¯¸í„°:**
- `context_length`: ëª¨ë¸ì´ ë³¼ ìˆ˜ ìˆëŠ” íƒ€ì„ìŠ¤í… ìˆ˜ (ê¸°ë³¸ 30)
- `block_size`: `context_length * 3` - (R, s, a) íŠ¸ë¦¬í”Œì´ë¯€ë¡œ ì‹¤ì œ í† í° ìˆ˜ëŠ” 3ë°°
- `model_type`:
  - `'reward_conditioned'`: Decision Transformer (RTG ì¡°ê±´í™”)
  - `'naive'`: Behavior Cloning (RTG ì—†ìŒ)

### 1.2 ë°ì´í„°ì…‹ í´ë˜ìŠ¤

```python
class StateActionReturnDataset(Dataset):
    def __init__(self, data, block_size, actions, done_idxs, rtgs, timesteps):
        self.block_size = block_size  # context_length * 3
        self.vocab_size = max(actions) + 1  # í–‰ë™ ê³µê°„ í¬ê¸°
        self.data = data           # ê´€ì¸¡ (ì´ë¯¸ì§€)
        self.actions = actions     # í–‰ë™
        self.done_idxs = done_idxs # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¸ë±ìŠ¤
        self.rtgs = rtgs           # Return-to-go
        self.timesteps = timesteps # íƒ€ì„ìŠ¤í…
```

**`__getitem__` ë©”ì„œë“œ - ë°°ì¹˜ ìƒ˜í”Œë§:**

```python
def __getitem__(self, idx):
    # block_sizeëŠ” í† í° ìˆ˜ (R, s, a í¬í•¨)ì´ë¯€ë¡œ, ì‹¤ì œ íƒ€ì„ìŠ¤í…ì€ 1/3
    block_size = self.block_size // 3  # ì˜ˆ: 90 // 3 = 30 íƒ€ì„ìŠ¤í…
    done_idx = idx + block_size

    # ì—í”¼ì†Œë“œ ê²½ê³„ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ì¡°ì •
    for i in self.done_idxs:
        if i > idx:  # í˜„ì¬ ì¸ë±ìŠ¤ ì´í›„ ì²« ë²ˆì§¸ done
            done_idx = min(int(i), done_idx)
            break

    # ì‹¤ì œ ì‹œì‘ ì¸ë±ìŠ¤ ì¬ì¡°ì •
    idx = done_idx - block_size

    # States: (block_size, 4*84*84) â†’ ì •ê·œí™”
    states = torch.tensor(np.array(self.data[idx:done_idx]), dtype=torch.float32)
    states = states.reshape(block_size, -1)
    states = states / 255.  # [0, 1]ë¡œ ì •ê·œí™”

    # Actions: (block_size, 1)
    actions = torch.tensor(self.actions[idx:done_idx], dtype=torch.long).unsqueeze(1)

    # RTGs: (block_size, 1)
    rtgs = torch.tensor(self.rtgs[idx:done_idx], dtype=torch.float32).unsqueeze(1)

    # Timesteps: (1, 1) - ì‹œí€€ìŠ¤ ì‹œì‘ timestepë§Œ
    timesteps = torch.tensor(self.timesteps[idx:idx+1], dtype=torch.int64).unsqueeze(1)

    return states, actions, rtgs, timesteps
```

**ì¤‘ìš” í¬ì¸íŠ¸:**
1. **ì—í”¼ì†Œë“œ ê²½ê³„ ì²˜ë¦¬**: ìƒ˜í”Œì´ ì—¬ëŸ¬ ì—í”¼ì†Œë“œë¥¼ ë„˜ì–´ê°€ì§€ ì•Šë„ë¡ ë³´ì¥
2. **ì´ë¯¸ì§€ ì •ê·œí™”**: [0, 255] â†’ [0, 1]
3. **Timestep ì²˜ë¦¬**: ì‹œí€€ìŠ¤ë‹¹ í•˜ë‚˜ì˜ timestepë§Œ ì‚¬ìš© (ì‹œì‘ timestep)

### 1.3 ëª¨ë¸ ë° íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”

```python
# ë°ì´í„° ë¡œë“œ
obss, actions, returns, done_idxs, rtgs, timesteps = create_dataset(
    args.num_buffers, args.num_steps, args.game,
    args.data_dir_prefix, args.trajectories_per_buffer
)

# ë°ì´í„°ì…‹ ìƒì„±
train_dataset = StateActionReturnDataset(
    obss,
    args.context_length * 3,  # block_size
    actions,
    done_idxs,
    rtgs,
    timesteps
)

# ëª¨ë¸ ì„¤ì •
mconf = GPTConfig(
    train_dataset.vocab_size,  # í–‰ë™ ê³µê°„ í¬ê¸°
    train_dataset.block_size,  # ì‹œí€€ìŠ¤ ê¸¸ì´
    n_layer=6,                 # Transformer layers
    n_head=8,                  # Attention heads
    n_embd=128,                # Embedding dimension
    model_type=args.model_type,
    max_timestep=max(timesteps)
)
model = GPT(mconf)

# íŠ¸ë ˆì´ë„ˆ ì„¤ì •
tconf = TrainerConfig(
    max_epochs=args.epochs,
    batch_size=args.batch_size,
    learning_rate=6e-4,
    lr_decay=True,
    warmup_tokens=512*20,
    final_tokens=2*len(train_dataset)*args.context_length*3,
    num_workers=4,
    seed=args.seed,
    model_type=args.model_type,
    game=args.game,
    max_timestep=max(timesteps)
)
trainer = Trainer(model, train_dataset, None, tconf)

# í•™ìŠµ ì‹œì‘
trainer.train()
```

---

## 2. ë°ì´í„°ì…‹ ìƒì„±

**íŒŒì¼:** `atari/create_dataset.py`

### 2.1 DQN Replay ë²„í¼ ë¡œë”©

```python
def create_dataset(num_buffers, num_steps, game, data_dir_prefix, trajectories_per_buffer):
    # ë°ì´í„° ì €ì¥ì†Œ
    obss = []           # ê´€ì¸¡ (4Ã—84Ã—84 í”„ë ˆì„ ìŠ¤íƒ)
    actions = []        # í–‰ë™
    returns = [0]       # ì—í”¼ì†Œë“œë³„ ì´ return
    done_idxs = []      # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¸ë±ìŠ¤
    stepwise_returns = []  # ê° ìŠ¤í…ì˜ ì¦‰ê° ë³´ìƒ

    transitions_per_buffer = np.zeros(50, dtype=int)
    num_trajectories = 0

    # num_stepsì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°ì´í„° ë¡œë“œ
    while len(obss) < num_steps:
        # ë§ˆì§€ë§‰ num_buffersê°œ ë²„í¼ ì¤‘ ëœë¤ ì„ íƒ (ë” ë‚˜ì€ ì •ì±…)
        buffer_num = np.random.choice(np.arange(50 - num_buffers, 50), 1)[0]
        i = transitions_per_buffer[buffer_num]

        # FixedReplayBuffer ë¡œë“œ
        frb = FixedReplayBuffer(
            data_dir=data_dir_prefix + game + '/1/replay_logs',
            replay_suffix=buffer_num,
            observation_shape=(84, 84),
            stack_size=4,          # 4 í”„ë ˆì„ ìŠ¤íƒ
            update_horizon=1,
            gamma=0.99,
            observation_dtype=np.uint8,
            batch_size=32,
            replay_capacity=100000
        )

        if frb._loaded_buffers:
            done = False
            curr_num_transitions = len(obss)
            trajectories_to_load = trajectories_per_buffer

            # ê¶¤ì  ìƒ˜í”Œë§ ë£¨í”„
            while not done:
                # ì „ì´ ìƒ˜í”Œë§
                states, ac, ret, next_states, next_action, next_reward, terminal, indices = \
                    frb.sample_transition_batch(batch_size=1, indices=[i])

                # (1, 84, 84, 4) â†’ (4, 84, 84)
                states = states.transpose((0, 3, 1, 2))[0]

                obss += [states]
                actions += [ac[0]]
                stepwise_returns += [ret[0]]

                # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì²˜ë¦¬
                if terminal[0]:
                    done_idxs += [len(obss)]
                    returns += [0]
                    if trajectories_to_load == 0:
                        done = True
                    else:
                        trajectories_to_load -= 1

                returns[-1] += ret[0]  # ëˆ„ì  return
                i += 1

                # ë²„í¼ ìš©ëŸ‰ ì´ˆê³¼ ì²´í¬
                if i >= 100000:
                    obss = obss[:curr_num_transitions]
                    actions = actions[:curr_num_transitions]
                    stepwise_returns = stepwise_returns[:curr_num_transitions]
                    returns[-1] = 0
                    i = transitions_per_buffer[buffer_num]
                    done = True

            num_trajectories += (trajectories_per_buffer - trajectories_to_load)
            transitions_per_buffer[buffer_num] = i
```

### 2.2 Return-to-Go (RTG) ê³„ì‚° - í•µì‹¬!

```mermaid
flowchart LR
    subgraph Input["ì…ë ¥: stepwise_returns"]
        R0["r[0]=1"]
        R1["r[1]=2"]
        R2["r[2]=3"]
        R3["r[3]=4"]
    end

    subgraph Process["ì—­ë°©í–¥ ëˆ„ì í•© ê³„ì‚°"]
        Direction["ì—­ìˆœ ìˆœíšŒ<br/>for j in range(i-1, start_index-1, -1)"]
        Cumsum["sum(rtg_j)"]
    end

    subgraph Output["ì¶œë ¥: rtg ë°°ì—´"]
        RTG0["rtg[0]=10<br/>1+2+3+4"]
        RTG1["rtg[1]=9<br/>2+3+4"]
        RTG2["rtg[2]=7<br/>3+4"]
        RTG3["rtg[3]=4<br/>4"]
    end

    Input --> Direction --> Cumsum --> Output

    style RTG0 fill:#ff6b6b
    style RTG1 fill:#ee5a6f
    style RTG2 fill:#c44569
    style RTG3 fill:#a73e5c
```

```python
# RTG ê³„ì‚°: ê° íƒ€ì„ìŠ¤í…ì—ì„œ ì—í”¼ì†Œë“œ ëê¹Œì§€ì˜ ëˆ„ì  ë³´ìƒ
start_index = 0
rtg = np.zeros_like(stepwise_returns)

for i in done_idxs:
    i = int(i)
    curr_traj_returns = stepwise_returns[start_index:i]

    # ì—­ë°©í–¥ ìˆœíšŒ: ëì—ì„œ ì‹œì‘ìœ¼ë¡œ
    for j in range(i-1, start_index-1, -1):
        # jë¶€í„° ì—í”¼ì†Œë“œ ëê¹Œì§€ì˜ ë³´ìƒ í•©ì‚°
        rtg_j = curr_traj_returns[j-start_index:i-start_index]
        rtg[j] = sum(rtg_j)

    start_index = i

print('max rtg is %d' % max(rtg))
```

**RTG ê³„ì‚° ì˜ˆì‹œ:**

```
íƒ€ì„ìŠ¤í…:  0    1    2    3    (done)
ë³´ìƒ:      1    2    3    4
---------------------------------
RTG[3] = 4                      (ë§ˆì§€ë§‰)
RTG[2] = 3 + 4 = 7
RTG[1] = 2 + 3 + 4 = 9
RTG[0] = 1 + 2 + 3 + 4 = 10    (ì²˜ìŒ)
```

**ì˜ë¯¸:** RTG[t]ëŠ” "íƒ€ì„ìŠ¤í… të¶€í„° ì—í”¼ì†Œë“œ ëê¹Œì§€ ì–»ì„ ìˆ˜ ìˆëŠ” ì´ ë³´ìƒ"

### 2.3 Timestep ìƒì„±

```python
# ê° ì—í”¼ì†Œë“œë§ˆë‹¤ 0ë¶€í„° ë‹¤ì‹œ ì‹œì‘í•˜ëŠ” timestep
start_index = 0
timesteps = np.zeros(len(actions)+1, dtype=int)

for i in done_idxs:
    i = int(i)
    # í•´ë‹¹ ì—í”¼ì†Œë“œì˜ timestep: 0, 1, 2, ..., length-1
    timesteps[start_index:i+1] = np.arange(i+1 - start_index)
    start_index = i+1

print('max timestep is %d' % max(timesteps))

return obss, actions, returns, done_idxs, rtg, timesteps
```

---

## 3. GPT ëª¨ë¸ êµ¬í˜„

**íŒŒì¼:** `atari/mingpt/model_atari.py`

### 3.1 GPT Config

```python
class GPTConfig:
    embd_pdrop = 0.1   # Embedding dropout
    resid_pdrop = 0.1  # Residual dropout
    attn_pdrop = 0.1   # Attention dropout

    def __init__(self, vocab_size, block_size, **kwargs):
        self.vocab_size = vocab_size  # í–‰ë™ ê³µê°„ í¬ê¸°
        self.block_size = block_size  # ì‹œí€€ìŠ¤ ê¸¸ì´ (context_length * 3)
        for k,v in kwargs.items():
            setattr(self, k, v)
```

### 3.2 Causal Self-Attention

```python
class CausalSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0

        # Q, K, V projections
        self.key = nn.Linear(config.n_embd, config.n_embd)
        self.query = nn.Linear(config.n_embd, config.n_embd)
        self.value = nn.Linear(config.n_embd, config.n_embd)

        # Dropout
        self.attn_drop = nn.Dropout(config.attn_pdrop)
        self.resid_drop = nn.Dropout(config.resid_pdrop)

        # Output projection
        self.proj = nn.Linear(config.n_embd, config.n_embd)

        # Causal mask (í•˜ì‚¼ê° í–‰ë ¬)
        self.register_buffer(
            "mask",
            torch.tril(torch.ones(config.block_size + 1, config.block_size + 1))
                 .view(1, 1, config.block_size + 1, config.block_size + 1)
        )
        self.n_head = config.n_head

    def forward(self, x, layer_past=None):
        B, T, C = x.size()  # Batch, Time, Channels

        # Multi-head attention
        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)

        # Attention scores
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))

        # Causal masking: ë¯¸ë˜ í† í°ì€ ë³¼ ìˆ˜ ì—†ìŒ
        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))
        att = F.softmax(att, dim=-1)
        att = self.attn_drop(att)

        # Attention output
        y = att @ v
        y = y.transpose(1, 2).contiguous().view(B, T, C)

        # Output projection
        y = self.resid_drop(self.proj(y))
        return y
```

### 3.3 Transformer Block

```python
class Block(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln1 = nn.LayerNorm(config.n_embd)
        self.ln2 = nn.LayerNorm(config.n_embd)
        self.attn = CausalSelfAttention(config)

        # MLP: 4ë°° í™•ì¥ í›„ ì¶•ì†Œ
        self.mlp = nn.Sequential(
            nn.Linear(config.n_embd, 4 * config.n_embd),
            GELU(),
            nn.Linear(4 * config.n_embd, config.n_embd),
            nn.Dropout(config.resid_pdrop),
        )

    def forward(self, x):
        # Pre-LayerNorm + Residual
        x = x + self.attn(self.ln1(x))
        x = x + self.mlp(self.ln2(x))
        return x
```

### 3.4 GPT ë©”ì¸ í´ë˜ìŠ¤

```python
class GPT(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.model_type = config.model_type

        # ê¸°ë³¸ GPT êµ¬ì¡°
        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)
        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size + 1, config.n_embd))
        self.global_pos_emb = nn.Parameter(torch.zeros(1, config.max_timestep+1, config.n_embd))
        self.drop = nn.Dropout(config.embd_pdrop)

        # Transformer blocks
        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])
        self.ln_f = nn.LayerNorm(config.n_embd)
        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)

        self.block_size = config.block_size
        self.apply(self._init_weights)

        # Decision Transformer ì „ìš© ì¸ì½”ë”
        # State encoder: DQN ìŠ¤íƒ€ì¼ CNN
        self.state_encoder = nn.Sequential(
            nn.Conv2d(4, 32, 8, stride=4, padding=0), nn.ReLU(),   # (4, 84, 84) â†’ (32, 20, 20)
            nn.Conv2d(32, 64, 4, stride=2, padding=0), nn.ReLU(),  # (32, 20, 20) â†’ (64, 9, 9)
            nn.Conv2d(64, 64, 3, stride=1, padding=0), nn.ReLU(),  # (64, 9, 9) â†’ (64, 7, 7)
            nn.Flatten(),                                           # 64*7*7 = 3136
            nn.Linear(3136, config.n_embd),                        # 3136 â†’ 128
            nn.Tanh()
        )

        # RTG encoder
        self.ret_emb = nn.Sequential(
            nn.Linear(1, config.n_embd),
            nn.Tanh()
        )

        # Action encoder
        self.action_embeddings = nn.Sequential(
            nn.Embedding(config.vocab_size, config.n_embd),
            nn.Tanh()
        )
        nn.init.normal_(self.action_embeddings[0].weight, mean=0.0, std=0.02)
```

### 3.5 Forward Pass - í•µì‹¬!

```mermaid
flowchart TD
    subgraph Input["ğŸ“¥ ì…ë ¥"]
        S["states<br/>(batch, block_size, 4Ã—84Ã—84)"]
        A["actions<br/>(batch, block_size, 1)"]
        R["rtgs<br/>(batch, block_size, 1)"]
        T["timesteps<br/>(batch, 1, 1)"]
    end

    subgraph Encode["1ï¸âƒ£ ì„ë² ë”©"]
        S --> SE["state_encoder<br/>CNN â†’ 128dim"]
        R --> RE["ret_emb<br/>Linear â†’ 128dim"]
        A --> AE["action_embeddings<br/>Embedding â†’ 128dim"]
    end

    subgraph Interleave["2ï¸âƒ£ ì‹œí€€ìŠ¤ êµ¬ì„±"]
        SE --> Stack["token_embeddings"]
        RE --> Stack
        AE --> Stack
        Stack -->|"reward_conditioned<br/>[R,s,a,R,s,a,...]"| Seq1
        Stack -->|"naive<br/>[s,a,s,a,...]"| Seq2
    end

    subgraph PosEmbed["3ï¸âƒ£ ìœ„ì¹˜ ì„ë² ë”©"]
        Seq1 --> Add["+ position_embeddings"]
        Seq2 --> Add
        T --> Global["global_pos_emb"]
        Global --> Add
        Add --> Dropout
    end

    subgraph Transformer["4ï¸âƒ£ Transformer"]
        Dropout --> Blocks["6Ã— Block<br/>Attention + MLP"]
        Blocks --> LN["LayerNorm"]
        LN --> Head["Linear Head<br/>â†’ vocab_size"]
    end

    subgraph Extract["5ï¸âƒ£ ì˜ˆì¸¡ ì¶”ì¶œ"]
        Head --> Slice1["reward_conditioned<br/>logits[:, 1::3, :]"]
        Head --> Slice2["naive<br/>logits[:, ::2, :]"]
    end

    style Encode fill:#e3f2fd
    style Interleave fill:#fff3e0
    style Transformer fill:#c8e6c9
    style Extract fill:#ffccbc
```

```python
def forward(self, states, actions, targets=None, rtgs=None, timesteps=None):
    """
    Args:
        states: (batch, block_size, 4*84*84)
        actions: (batch, block_size, 1)
        targets: (batch, block_size, 1) - í•™ìŠµ ì‹œ action labels
        rtgs: (batch, block_size, 1)
        timesteps: (batch, 1, 1)
    """

    # Step 1: State ì„ë² ë”©
    # (batch, block_size, 4*84*84) â†’ (batch*block_size, 4, 84, 84)
    state_embeddings = self.state_encoder(
        states.reshape(-1, 4, 84, 84).type(torch.float32).contiguous()
    )
    # (batch*block_size, n_embd) â†’ (batch, block_size, n_embd)
    state_embeddings = state_embeddings.reshape(
        states.shape[0], states.shape[1], self.config.n_embd
    )

    # Step 2: ì‹œí€€ìŠ¤ êµ¬ì„±
    if actions is not None and self.model_type == 'reward_conditioned':
        # RTGì™€ Action ì„ë² ë”©
        rtg_embeddings = self.ret_emb(rtgs.type(torch.float32))
        action_embeddings = self.action_embeddings(actions.type(torch.long).squeeze(-1))

        # (R, s, a) íŠ¸ë¦¬í”Œ ì‹œí€€ìŠ¤ ìƒì„±
        token_embeddings = torch.zeros(
            (states.shape[0], states.shape[1]*3 - int(targets is None), self.config.n_embd),
            dtype=torch.float32,
            device=state_embeddings.device
        )
        token_embeddings[:,::3,:] = rtg_embeddings      # ìœ„ì¹˜ 0, 3, 6, ... (RTG)
        token_embeddings[:,1::3,:] = state_embeddings   # ìœ„ì¹˜ 1, 4, 7, ... (State)
        token_embeddings[:,2::3,:] = action_embeddings[:,-states.shape[1] + int(targets is None):,:]  # ìœ„ì¹˜ 2, 5, 8, ... (Action)

    elif actions is None and self.model_type == 'reward_conditioned':
        # ì²« íƒ€ì„ìŠ¤í… (action ì—†ìŒ)
        rtg_embeddings = self.ret_emb(rtgs.type(torch.float32))
        token_embeddings = torch.zeros(
            (states.shape[0], states.shape[1]*2, self.config.n_embd),
            dtype=torch.float32,
            device=state_embeddings.device
        )
        token_embeddings[:,::2,:] = rtg_embeddings   # [Râ‚€, sâ‚€]
        token_embeddings[:,1::2,:] = state_embeddings

    elif actions is not None and self.model_type == 'naive':
        # Behavior Cloning: (s, a) ì‹œí€€ìŠ¤ë§Œ
        action_embeddings = self.action_embeddings(actions.type(torch.long).squeeze(-1))
        token_embeddings = torch.zeros(
            (states.shape[0], states.shape[1]*2 - int(targets is None), self.config.n_embd),
            dtype=torch.float32,
            device=state_embeddings.device
        )
        token_embeddings[:,::2,:] = state_embeddings
        token_embeddings[:,1::2,:] = action_embeddings[:,-states.shape[1] + int(targets is None):,:]

    elif actions is None and self.model_type == 'naive':
        # ì²« íƒ€ì„ìŠ¤í…
        token_embeddings = state_embeddings

    else:
        raise NotImplementedError()

    # Step 3: ìœ„ì¹˜ ì„ë² ë”©
    batch_size = states.shape[0]
    all_global_pos_emb = torch.repeat_interleave(self.global_pos_emb, batch_size, dim=0)

    # Global (absolute) + Relative position embeddings
    position_embeddings = torch.gather(
        all_global_pos_emb, 1,
        torch.repeat_interleave(timesteps, self.config.n_embd, dim=-1)
    ) + self.pos_emb[:, :token_embeddings.shape[1], :]

    # Step 4: Transformer ì²˜ë¦¬
    x = self.drop(token_embeddings + position_embeddings)
    x = self.blocks(x)  # 6ê°œ transformer blocks
    x = self.ln_f(x)
    logits = self.head(x)  # (batch, seq_len, vocab_size)

    # Step 5: Action ì˜ˆì¸¡ ì¶”ì¶œ
    if actions is not None and self.model_type == 'reward_conditioned':
        # State ìœ„ì¹˜ (1::3)ì—ì„œë§Œ ì˜ˆì¸¡ ì¶”ì¶œ
        # [?, â†’aâ‚€, ?, ?, â†’aâ‚, ?, ?, â†’aâ‚‚, ...]
        logits = logits[:, 1::3, :]
    elif actions is None and self.model_type == 'reward_conditioned':
        logits = logits[:, 1:, :]
    elif actions is not None and self.model_type == 'naive':
        # State ìœ„ì¹˜ (0::2)ì—ì„œ ì˜ˆì¸¡ ì¶”ì¶œ
        logits = logits[:, ::2, :]
    elif actions is None and self.model_type == 'naive':
        logits = logits
    else:
        raise NotImplementedError()

    # Step 6: ì†ì‹¤ ê³„ì‚°
    loss = None
    if targets is not None:
        loss = F.cross_entropy(
            logits.reshape(-1, logits.size(-1)),
            targets.reshape(-1)
        )

    return logits, loss
```

**í•µì‹¬ í¬ì¸íŠ¸:**

1. **ì‹œí€€ìŠ¤ êµ¬ì„±:**
   - Reward-conditioned: `[Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚, aâ‚, ...]`
   - Naive: `[sâ‚€, aâ‚€, sâ‚, aâ‚, ...]`

2. **ì˜ˆì¸¡ ìœ„ì¹˜:**
   - State í† í° ìœ„ì¹˜ì—ì„œ action ì˜ˆì¸¡
   - Causal maskingìœ¼ë¡œ ì¸í•´ sâ‚€ëŠ” Râ‚€ë§Œ ë³´ê³ , sâ‚ì€ Râ‚€, sâ‚€, aâ‚€, Râ‚ê¹Œì§€ ë³¼ ìˆ˜ ìˆìŒ

3. **ìœ„ì¹˜ ì¸ì½”ë”©:**
   - Global: ì—í”¼ì†Œë“œ ë‚´ ì ˆëŒ€ timestep
   - Relative: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ë‚´ ìƒëŒ€ ìœ„ì¹˜

---

## 4. í•™ìŠµ ë£¨í”„

**íŒŒì¼:** `atari/mingpt/trainer_atari.py`

### 4.1 í•™ìŠµ Epoch

```python
def run_epoch(split, epoch_num=0):
    is_train = split == 'train'
    model.train(is_train)
    data = self.train_dataset if is_train else self.test_dataset
    loader = DataLoader(
        data,
        shuffle=True,
        pin_memory=True,
        batch_size=config.batch_size,
        num_workers=config.num_workers
    )

    losses = []
    pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)

    for it, (x, y, r, t) in pbar:
        # ë°ì´í„° GPUë¡œ ì´ë™
        x = x.to(self.device)  # states
        y = y.to(self.device)  # actions
        r = r.to(self.device)  # rtgs
        t = t.to(self.device)  # timesteps

        # Forward pass
        with torch.set_grad_enabled(is_train):
            logits, loss = model(x, y, y, r, t)  # targets=y (action labels)
            loss = loss.mean()
            losses.append(loss.item())

        if is_train:
            # ì—­ì „íŒŒ
            model.zero_grad()
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)

            # Optimizer step
            optimizer.step()

            # Learning rate decay
            if config.lr_decay:
                self.tokens += (y >= 0).sum()  # ì²˜ë¦¬ëœ í† í° ìˆ˜

                if self.tokens < config.warmup_tokens:
                    # Linear warmup
                    lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))
                else:
                    # Cosine decay
                    progress = float(self.tokens - config.warmup_tokens) / \
                               float(max(1, config.final_tokens - config.warmup_tokens))
                    lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))

                lr = config.learning_rate * lr_mult
                for param_group in optimizer.param_groups:
                    param_group['lr'] = lr
            else:
                lr = config.learning_rate

            # ì§„í–‰ ìƒí™© ì¶œë ¥
            pbar.set_description(
                f"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}"
            )
```

### 4.2 ë©”ì¸ í•™ìŠµ ë£¨í”„

```python
def train(self):
    model, config = self.model, self.config
    raw_model = model.module if hasattr(self.model, "module") else model
    optimizer = raw_model.configure_optimizers(config)

    best_return = -float('inf')
    self.tokens = 0  # LR decayìš© í† í° ì¹´ìš´í„°

    for epoch in range(config.max_epochs):
        # í•™ìŠµ
        run_epoch('train', epoch_num=epoch)

        # í‰ê°€: ëª©í‘œ RTGë¡œ ì¡°ê±´í™”
        if self.config.model_type == 'naive':
            eval_return = self.get_returns(0)  # BCëŠ” RTG ë¬´ì‹œ
        elif self.config.model_type == 'reward_conditioned':
            # ê²Œì„ë³„ ëª©í‘œ return
            if self.config.game == 'Breakout':
                eval_return = self.get_returns(90)
            elif self.config.game == 'Seaquest':
                eval_return = self.get_returns(1150)
            elif self.config.game == 'Qbert':
                eval_return = self.get_returns(14000)
            elif self.config.game == 'Pong':
                eval_return = self.get_returns(20)
            else:
                raise NotImplementedError()
```

---

## 5. í‰ê°€ ë° ìƒ˜í”Œë§

### 5.1 í‰ê°€ í•¨ìˆ˜ - get_returns

```mermaid
sequenceDiagram
    participant Main as ğŸ“± Main
    participant GetReturns as get_returns()
    participant Env as ğŸ® Atari Env
    participant Sample as sample()
    participant RTG as rtgs ë°°ì—´

    Main->>GetReturns: target=90

    GetReturns->>Env: Env(args).eval()
    Env-->>GetReturns: env

    GetReturns->>RTG: rtgs = [90]

    loop 10 ì—í”¼ì†Œë“œ
        GetReturns->>Env: env.reset()
        Env-->>GetReturns: state

        GetReturns->>Sample: sample(model, state, rtgs=[90])
        Sample-->>GetReturns: action

        loop ìŠ¤í… ì§„í–‰
            GetReturns->>Env: step(action)
            Env-->>GetReturns: state, reward, done

            alt done=False
                GetReturns->>RTG: rtgs += [rtgs[-1] - reward]
                Note over RTG: rtg = 90 - 5 = 85
                GetReturns->>Sample: sample(model, all_states, actions, rtgs)
                Sample-->>GetReturns: next_action
            else done=True
                GetReturns->>GetReturns: T_rewards.append(sum)
            end
        end
    end

    GetReturns->>Main: eval_return = mean(T_rewards)
    Note over Main: target: 90, eval: 85
```

```python
def get_returns(self, ret):
    """
    Args:
        ret: ëª©í‘œ return (ì˜ˆ: Breakoutì˜ ê²½ìš° 90)
    Returns:
        eval_return: 10 ì—í”¼ì†Œë“œì˜ í‰ê·  return
    """
    self.model.train(False)
    args = Args(self.config.game.lower(), self.config.seed)
    env = Env(args)
    env.eval()

    T_rewards = []

    for i in range(10):  # 10 ì—í”¼ì†Œë“œ í‰ê°€
        state = env.reset()
        state = state.type(torch.float32).to(self.device).unsqueeze(0).unsqueeze(0)
        rtgs = [ret]  # ì´ˆê¸° ëª©í‘œ RTG

        # ì²« ë²ˆì§¸ action ìƒ˜í”Œë§
        sampled_action = sample(
            self.model.module,
            state,
            1,
            temperature=1.0,
            sample=True,
            actions=None,
            rtgs=torch.tensor(rtgs, dtype=torch.long).to(self.device).unsqueeze(0).unsqueeze(-1),
            timesteps=torch.zeros((1, 1, 1), dtype=torch.int64).to(self.device)
        )

        j = 0
        all_states = state
        actions = []
        reward_sum = 0
        done = False

        # ì—í”¼ì†Œë“œ ë¡¤ì•„ì›ƒ
        while True:
            if done:
                state, reward_sum, done = env.reset(), 0, False

            # Action ì‹¤í–‰
            action = sampled_action.cpu().numpy()[0,-1]
            actions += [sampled_action]
            state, reward, done = env.step(action)
            reward_sum += reward
            j += 1

            if done:
                T_rewards.append(reward_sum)
                break

            # State history ì—…ë°ì´íŠ¸
            state = state.unsqueeze(0).unsqueeze(0).to(self.device)
            all_states = torch.cat([all_states, state], dim=0)

            # RTG ì—…ë°ì´íŠ¸ - í•µì‹¬!
            rtgs += [rtgs[-1] - reward]

            # ë‹¤ìŒ action ìƒ˜í”Œë§
            sampled_action = sample(
                self.model.module,
                all_states.unsqueeze(0),
                1,
                temperature=1.0,
                sample=True,
                actions=torch.tensor(actions, dtype=torch.long).to(self.device).unsqueeze(1).unsqueeze(0),
                rtgs=torch.tensor(rtgs, dtype=torch.long).to(self.device).unsqueeze(0).unsqueeze(-1),
                timesteps=(min(j, self.config.max_timestep) * torch.ones((1, 1, 1), dtype=torch.int64).to(self.device))
            )

    env.close()
    eval_return = sum(T_rewards) / 10.
    print("target return: %d, eval return: %d" % (ret, eval_return))
    self.model.train(True)
    return eval_return
```

**í•µì‹¬: RTG ë™ì  ì—…ë°ì´íŠ¸**

```python
rtgs += [rtgs[-1] - reward]
```

**ì˜ˆì‹œ:**
```
ì´ˆê¸°: rtg = 90 (ëª©í‘œ)
ìŠ¤í… 1: reward = 5 â†’ rtg = 90 - 5 = 85
ìŠ¤í… 2: reward = 10 â†’ rtg = 85 - 10 = 75
ìŠ¤í… 3: reward = 3 â†’ rtg = 75 - 3 = 72
...
```

ì´ë ‡ê²Œ RTGëŠ” "ì•„ì§ ì–»ì–´ì•¼ í•  ë³´ìƒ"ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤!

### 5.2 ìƒ˜í”Œë§ í•¨ìˆ˜

**íŒŒì¼:** `atari/mingpt/utils.py`

```python
@torch.no_grad()
def sample(model, x, steps, temperature=1.0, sample=False, top_k=None,
           actions=None, rtgs=None, timesteps=None):
    """
    Args:
        model: GPT ëª¨ë¸
        x: states (batch, seq_len, ...)
        steps: ìƒ˜í”Œë§í•  ìŠ¤í… ìˆ˜ (ì¼ë°˜ì ìœ¼ë¡œ 1)
        temperature: ì˜¨ë„ íŒŒë¼ë¯¸í„° (ë†’ì„ìˆ˜ë¡ íƒí—˜ì )
        sample: Trueë©´ í™•ë¥ ì , Falseë©´ greedy
        top_k: Top-k ìƒ˜í”Œë§
        actions: ì´ì „ actions
        rtgs: Return-to-go
        timesteps: í˜„ì¬ timestep
    Returns:
        sampled_action: ìƒ˜í”Œë§ëœ action
    """
    block_size = model.get_block_size()
    model.eval()

    for k in range(steps):
        # Context window í¬ê¸° ì œí•œ
        # block_sizeëŠ” í† í° ìˆ˜ì´ë¯€ë¡œ ì‹¤ì œ íƒ€ì„ìŠ¤í…ì€ 1/3
        x_cond = x if x.size(1) <= block_size//3 else x[:, -block_size//3:]

        if actions is not None:
            actions = actions if actions.size(1) <= block_size//3 else actions[:, -block_size//3:]

        rtgs = rtgs if rtgs.size(1) <= block_size//3 else rtgs[:, -block_size//3:]

        # Forward pass
        logits, _ = model(x_cond, actions=actions, targets=None, rtgs=rtgs, timesteps=timesteps)

        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ logitsë§Œ ì‚¬ìš©
        logits = logits[:, -1, :] / temperature

        # Top-k ìƒ˜í”Œë§ (ì˜µì…˜)
        if top_k is not None:
            logits = top_k_logits(logits, top_k)

        # Softmaxë¡œ í™•ë¥  ë¶„í¬ ë³€í™˜
        probs = F.softmax(logits, dim=-1)

        # ìƒ˜í”Œë§ vs Greedy
        if sample:
            # í™•ë¥ ì  ìƒ˜í”Œë§
            ix = torch.multinomial(probs, num_samples=1)
        else:
            # Greedy (argmax)
            _, ix = torch.topk(probs, k=1, dim=-1)

        x = ix  # ìƒ˜í”Œë§ëœ action

    return x
```

**Top-k í•„í„°ë§:**

```python
def top_k_logits(logits, k):
    """ìƒìœ„ kê°œë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ logitsë¥¼ -infë¡œ ì„¤ì •"""
    v, ix = torch.topk(logits, k)
    out = logits.clone()
    out[out < v[:, [-1]]] = -float('Inf')
    return out
```

---

## í•µì‹¬ ê°œë… ì •ë¦¬

### 1. Decision Transformerì˜ í˜ì‹ 

**ê¸°ì¡´ RL:**
```
Q-learning: Q(s,a) = r + Î³ max Q(s',a')
Policy Gradient: âˆ‡J(Î¸) = E[âˆ‡ log Ï€(a|s) A(s,a)]
```

**Decision Transformer:**
```
Supervised Learning: Ï€(a|s, R) = GPT(R, sâ‚€, aâ‚€, ..., R, s)
ë‹¨ìˆœíˆ (R, s, a) ì‹œí€€ìŠ¤ë¥¼ ëª¨ë¸ë§!
```

### 2. RTGì˜ ì—­í• 

**í•™ìŠµ ì‹œ:** RTGëŠ” ë°ì´í„°ì—ì„œ ê³„ì‚°ëœ ì‹¤ì œ ë¯¸ë˜ ëˆ„ì  ë³´ìƒ
```python
rtg[t] = sum(rewards[t:end])
```

**ì¶”ë¡  ì‹œ:** RTGëŠ” ì›í•˜ëŠ” ëª©í‘œë¥¼ ì§€ì •í•˜ê³  ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸
```python
rtg = target_return  # ì˜ˆ: 90
while not done:
    action = model(state, rtg)
    state, reward, done = env.step(action)
    rtg = rtg - reward  # ì—…ë°ì´íŠ¸!
```

### 3. ì™œ State ìœ„ì¹˜ì—ì„œ ì˜ˆì¸¡?

Causal masking ë•ë¶„:
```
ì…ë ¥:  [Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚, aâ‚, ...]
ì˜ˆì¸¡:  [  ,   , aâ‚€,   ,   , aâ‚, ...]
```

- sâ‚€ ìœ„ì¹˜: Râ‚€, sâ‚€ë§Œ ë³¼ ìˆ˜ ìˆìŒ â†’ aâ‚€ ì˜ˆì¸¡
- sâ‚ ìœ„ì¹˜: Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚ê¹Œì§€ ë³¼ ìˆ˜ ìˆìŒ â†’ aâ‚ ì˜ˆì¸¡

### 4. ëª¨ë¸ íƒ€ì… ë¹„êµ

| | Reward-Conditioned | Naive |
|---|---|---|
| **ì‹œí€€ìŠ¤** | [R, s, a] | [s, a] |
| **ì¡°ê±´í™”** | RTGë¡œ ëª©í‘œ ì§€ì • | ì—†ìŒ |
| **ìœ ì—°ì„±** | ë‹¤ì–‘í•œ ì„±ëŠ¥ ìˆ˜ì¤€ | ê³ ì •ëœ ì •ì±… |
| **ì‚¬ìš© ì‚¬ë¡€** | ì›í•˜ëŠ” return ë‹¬ì„± | ë‹¨ìˆœ ëª¨ë°© í•™ìŠµ |

---

## ì‹¤í–‰ ì˜ˆì‹œ

### í•™ìŠµ

```bash
cd atari
python run_dt_atari.py \
  --seed 123 \
  --context_length 30 \
  --epochs 5 \
  --model_type 'reward_conditioned' \
  --num_steps 500000 \
  --num_buffers 50 \
  --game 'Breakout' \
  --batch_size 128 \
  --data_dir_prefix ./dqn_replay
```

### ì—¬ëŸ¬ ì‹¤í—˜ ì‹¤í–‰

```bash
cd atari
bash run.sh
```

---

ì´ ë¬¸ì„œëŠ” Decision Transformer (Atari)ì˜ ì „ì²´ ì½”ë“œ êµ¬í˜„ì„ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤. ê° í•¨ìˆ˜ì˜ ì—­í• ê³¼ ë°ì´í„° íë¦„ì„ ì´í•´í•˜ë©´ Decision Transformerì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ ì™„ì „íˆ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
