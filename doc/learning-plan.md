# Decision Transformer ì´ˆë³´ì í•™ìŠµ ê³„íš

ì´ ë¬¸ì„œëŠ” Decision Transformerë¥¼ ì²˜ìŒ ì ‘í•˜ëŠ” ë¶„ë“¤ì„ ìœ„í•œ **ë‹¨ê³„ë³„ í•™ìŠµ ê³„íš**ì…ë‹ˆë‹¤.
ê° ë‹¨ê³„ë§ˆë‹¤ ì´í•´í•´ì•¼ í•  ê°œë…, ì½ì–´ì•¼ í•  ì½”ë“œ, ì§ì ‘ í•´ë³¼ ì‹¤ìŠµì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

---

## í•™ìŠµ ë¡œë“œë§µ ê°œìš”

```mermaid
flowchart TB
    subgraph Phase1["Phase 1: ë°°ê²½ ì§€ì‹"]
        direction LR
        P1RL["RL ê¸°ì´ˆ<br/>1-2ì¼"]
        P1TF["Transformer ê¸°ì´ˆ<br/>1-2ì¼"]
        P1TS["Timestep ê°œë…<br/>í•µì‹¬!"]
    end

    subgraph Phase2["Phase 2: í•µì‹¬ ê°œë…"]
        direction LR
        P2Paper["ë…¼ë¬¸ ì½ê¸°"]
        P2RTG["RTG ê°œë…"]
        P2Seq["ì‹œí€€ìŠ¤ êµ¬ì„±"]
    end

    subgraph Phase3["Phase 3: Gym ì‹¤ìŠµ"]
        direction LR
        P3Setup["í™˜ê²½ ì„¤ì •"]
        P3Data["ë°ì´í„° íƒìƒ‰"]
        P3Model["ëª¨ë¸ ì½”ë“œ"]
        P3Train["í•™ìŠµ ì‹¤í–‰"]
    end

    subgraph Phase4["Phase 4: Atari ì‹¤ìŠµ"]
        direction LR
        P4Setup["í™˜ê²½ ì„¤ì •"]
        P4CNN["CNN ì´í•´"]
        P4Train["í•™ìŠµ ì‹¤í–‰"]
        P4Compare["DT vs BC"]
    end

    subgraph Phase5["Phase 5: ì‹¬í™”"]
        direction LR
        P5Exp["í•˜ì´í¼íŒŒë¼ë¯¸í„°<br/>ì‹¤í—˜"]
        P5Read["ë…¼ë¬¸ ì‹¬í™”"]
        P5Custom["ì»¤ìŠ¤í…€ í™˜ê²½"]
    end

    Phase1 --> Phase2 --> Phase3 --> Phase4 --> Phase5

    style Phase1 fill:#e3f2fd
    style Phase2 fill:#fff3e0
    style Phase3 fill:#e8f5e9
    style Phase4 fill:#ffebee
    style Phase5 fill:#f3e5f5
    style P1TS fill:#ffccbc
    style P2RTG fill:#ffccbc
```

---

## Phase 1: ë°°ê²½ ì§€ì‹

### 1.1 ê°•í™”í•™ìŠµ ê¸°ì´ˆ

**ëª©í‘œ**: RLì˜ ê¸°ë³¸ ìš©ì–´ì™€ ê°œë… ì´í•´

**í•™ìŠµí•  ê°œë…**:
- [ ] State, Action, Rewardì˜ ì •ì˜
- [ ] Episodeì™€ Trajectory
- [ ] Policy (ì •ì±…): Ï€(a|s)
- [ ] Return: ëˆ„ì  ë³´ìƒì˜ í•©
- [ ] Discount factor (Î³)

**ì¶”ì²œ ìë£Œ**:
- Sutton & Barto "Reinforcement Learning" 1-3ì¥
- OpenAI Spinning Up: https://spinningup.openai.com/

**ì‹¤ìŠµ ì²´í¬í¬ì¸íŠ¸**:
```python
# ì´ ê°œë…ë“¤ì„ ì„¤ëª…í•  ìˆ˜ ìˆëŠ”ê°€?
# 1. "Returnì´ ë­”ê°€ìš”?" â†’ ì—í”¼ì†Œë“œ ëê¹Œì§€ì˜ ë³´ìƒ í•©
# 2. "Policyê°€ ë­”ê°€ìš”?" â†’ ìƒíƒœì—ì„œ í–‰ë™ì„ ì„ íƒí•˜ëŠ” ê·œì¹™
# 3. "Discount factorëŠ” ì™œ í•„ìš”í•œê°€ìš”?" â†’ ë¯¸ë˜ ë³´ìƒì˜ ê°€ì¹˜ë¥¼ í˜„ì¬ë³´ë‹¤ ë‚®ê²Œ í‰ê°€
```

### 1.2 Transformer ê¸°ì´ˆ

**ëª©í‘œ**: Transformer ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ ì´í•´

**í•™ìŠµí•  ê°œë…**:
- [ ] Self-Attention ë©”ì»¤ë‹ˆì¦˜
- [ ] Multi-Head Attention
- [ ] Positional Encoding
- [ ] Causal Masking (ì™œ ë¯¸ë˜ë¥¼ ë³¼ ìˆ˜ ì—†ëŠ”ê°€?)
- [ ] GPTì˜ Autoregressive ìƒì„±

**ì¶”ì²œ ìë£Œ**:
- "Attention Is All You Need" ë…¼ë¬¸
- Jay Alammar's Illustrated Transformer: https://jalammar.github.io/illustrated-transformer/

**ì‹¤ìŠµ ì²´í¬í¬ì¸íŠ¸**:
```python
# ì´ ì§ˆë¬¸ë“¤ì— ë‹µí•  ìˆ˜ ìˆëŠ”ê°€?
# 1. "Causal maskingì´ ë­”ê°€ìš”?" â†’ ë¯¸ë˜ í† í°ì„ ë³´ì§€ ëª»í•˜ê²Œ í•˜ëŠ” ë§ˆìŠ¤í¬
# 2. "ì™œ position embeddingì´ í•„ìš”í•œê°€ìš”?" â†’ TransformerëŠ” ìˆœì„œ ì •ë³´ê°€ ì—†ì–´ì„œ
# 3. "GPTëŠ” ì–´ë–»ê²Œ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ë‚˜ìš”?" â†’ ì´ì „ í† í°ë“¤ë§Œ ë³´ê³  ë‹¤ìŒì„ ì˜ˆì¸¡
```

### 1.3 Timestepì˜ ì˜ë¯¸ (ì¤‘ìš”!)

```mermaid
flowchart TB
    subgraph GPT["GPTì˜ Positional Encoding"]
        GToken["í† í° ì‹œí€€ìŠ¤"]
        GPos["ìœ„ì¹˜ ì„ë² ë”©"]
        GMean["ì‹œí€€ìŠ¤ ë‚´ ìˆœì„œ"]
    end

    subgraph DT["Decision Transformerì˜ Timestep"]
        DToken["(R, s, a) íŠ¸ë¦¬í”Œ"]
        DEpi["ì—í”¼ì†Œë“œ ë‚´ ì‹œê°„"]
        DMean["ì—í”¼ì†Œë“œ ë‚´ ì ˆëŒ€ ì‹œì "]
    end

    subgraph Example["DT ì‹œí€€ìŠ¤ ì˜ˆì‹œ"]
        ESeq["ì‹œí€€ìŠ¤: [Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚, aâ‚, Râ‚‚, sâ‚‚, aâ‚‚]"]
        ETStep["timestep:   0    0    0    1    1    1    2    2    2"]
        EGroup["           â””â”€ ê°™ì€ ì‹œì  â”€â”˜  â””â”€ ê°™ì€ ì‹œì  â”€â”˜"]
    end

    GToken --> GPos --> GMean
    DToken --> DEpi --> DMean
    Example

    style GPT fill:#e3f2fd
    style DT fill:#c8e6c9
    style Example fill:#fff3e0
    style ETStep fill:#ffccbc
```

Decision Transformerì—ì„œ **timestep**ì€ ë‘ ê°€ì§€ ë§¥ë½ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤:

#### Positional Encodingì—ì„œì˜ timestep

ì¼ë°˜ì ì¸ Transformerì—ì„œ timestepì€ **ì‹œí€€ìŠ¤ ë‚´ ìœ„ì¹˜(position)**ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤:
```
ì‹œí€€ìŠ¤: [í† í°0, í† í°1, í† í°2, í† í°3]
ìœ„ì¹˜:      0      1      2      3
```

#### Decision Transformerì—ì„œì˜ timestep (í•µì‹¬!)

DTì—ì„œ timestepì€ **ì—í”¼ì†Œë“œ ë‚´ ì‹œê°„ ë‹¨ê³„**ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤:
- **t=0**: ì—í”¼ì†Œë“œ ì‹œì‘ ì‹œì 
- **t=1**: ì²« ë²ˆì§¸ í–‰ë™ í›„
- **t=2**: ë‘ ë²ˆì§¸ í–‰ë™ í›„
- ...

**GPT vs Decision Transformer ë¹„êµ**:

| GPT | Decision Transformer |
|:---:|:---:|
| ì‹œí€€ìŠ¤ ë‚´ ìœ„ì¹˜ (0, 1, 2...) | ì—í”¼ì†Œë“œ ë‚´ ì‹œê°„ ë‹¨ê³„ |
| "The=0, cat=1, sat=2" | "t=0ì—ì„œì˜ (R, s, a)" |

**í•µì‹¬ ì°¨ì´ì **: DTì—ì„œëŠ” ê°™ì€ timestepì— **(R, s, a)** ì„¸ ê°œì˜ í† í°ì´ ëª¨ë‘ **ê°™ì€ timestep ì„ë² ë”©**ì„ ê³µìœ í•©ë‹ˆë‹¤.

```
ì‹œí€€ìŠ¤:    [Râ‚€,  sâ‚€,  aâ‚€,  Râ‚,  sâ‚,  aâ‚,  Râ‚‚,  sâ‚‚,  aâ‚‚]
timestep:   0    0    0    1    1    1    2    2    2
           â””â”€ ê°™ì€ ì‹œì  â”€â”˜  â””â”€ ê°™ì€ ì‹œì  â”€â”˜  â””â”€ ê°™ì€ ì‹œì  â”€â”˜
```

ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ "ì´ ìƒíƒœì™€ í–‰ë™ì´ ê°™ì€ ì‹œì ì— ì¼ì–´ë‚¬ë‹¤"ëŠ” ê²ƒì„ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì½”ë“œì—ì„œ í™•ì¸**:
```python
# gym/decision_transformer/models/decision_transformer.py
time_embeddings = self.embed_timestep(timesteps)

# ê°™ì€ timestep ì„ë² ë”©ì´ R, s, a ëª¨ë‘ì— ë”í•´ì§
state_embeddings = state_embeddings + time_embeddings
action_embeddings = action_embeddings + time_embeddings
returns_embeddings = returns_embeddings + time_embeddings
```

---

## Phase 2: Decision Transformer í•µì‹¬ ê°œë…

### 2.1 ë…¼ë¬¸ ì½ê¸°

**ëª©í‘œ**: Decision Transformerì˜ í•µì‹¬ ì•„ì´ë””ì–´ íŒŒì•…

**ì½ì„ ìë£Œ**:
- [ ] [Decision Transformer ë…¼ë¬¸](https://arxiv.org/abs/2106.01345) - Abstract, Introduction, Method ì„¹ì…˜

**í•µì‹¬ ì§ˆë¬¸**:
1. ê¸°ì¡´ RLê³¼ DTì˜ ì°¨ì´ì ì€?
2. Return-to-Go (RTG)ë€ ë¬´ì—‡ì¸ê°€?
3. ì™œ "ì‹œí€€ìŠ¤ ëª¨ë¸ë§"ìœ¼ë¡œ RLì„ í’€ ìˆ˜ ìˆëŠ”ê°€?

### 2.2 Return-to-Go ê°œë… ê¹Šì´ ì´í•´

**ëª©í‘œ**: RTGì˜ ê³„ì‚° ë°©ì‹ê³¼ ì—­í•  ì´í•´

**ì½ì„ ë¬¸ì„œ**: [architecture-flow.md](./architecture-flow.md) - "RTG ê³„ì‚° ì„¸ë¶€ ê³¼ì •" ì„¹ì…˜

```mermaid
flowchart LR
    subgraph Rewards["ë³´ìƒ ì‹œí€€ìŠ¤"]
        R0["t=0: reward=1"]
        R1["t=1: reward=2"]
        R2["t=2: reward=3"]
        R3["t=3: reward=4"]
    end

    subgraph RTGs["RTG ê³„ì‚° (í›„ë°© ëˆ„ì í•©)"]
        RT0["RTG[0] = 1+2+3+4 = 10"]
        RT1["RTG[1] = 2+3+4 = 9"]
        RT2["RTG[2] = 3+4 = 7"]
        RT3["RTG[3] = 4"]
    end

    R0 --> RT0
    R1 --> RT1
    R2 --> RT2
    R3 --> RT3

    RT0 -.->|"ë¯¸ë˜ ë³´ìƒì˜ í•©"| RT1
    RT1 -.-> RT2
    RT2 -.-> RT3

    style RT0 fill:#ff6b6b
    style RT1 fill:#ee5a6f
    style RT2 fill:#c44569
    style RT3 fill:#a73e5c
```

**í•µì‹¬ ê°œë…**:
```
ì‹œì :     t=0    t=1    t=2    t=3
ë³´ìƒ:      1      2      3      4
----------------------------------
RTG[0] = 1+2+3+4 = 10  (ì²˜ìŒë¶€í„° ëê¹Œì§€)
RTG[1] = 2+3+4 = 9     (t=1ë¶€í„° ëê¹Œì§€)
RTG[2] = 3+4 = 7       (t=2ë¶€í„° ëê¹Œì§€)
RTG[3] = 4             (ë§ˆì§€ë§‰)
```

**ì‹¤ìŠµ: RTG ì§ì ‘ ê³„ì‚°í•´ë³´ê¸°**
```python
# í„°ë¯¸ë„ì—ì„œ Python ì‹¤í–‰
python3

rewards = [1, 2, 3, 4, 5]

# RTG ê³„ì‚° (ì§ì ‘ êµ¬í˜„)
rtg = []
for i in range(len(rewards)):
    rtg.append(sum(rewards[i:]))

print("Rewards:", rewards)
print("RTG:", rtg)
# ì˜ˆìƒ ì¶œë ¥: RTG: [15, 14, 12, 9, 5]
```

### 2.3 ì‹œí€€ìŠ¤ êµ¬ì„± ì´í•´

**ëª©í‘œ**: (R, s, a) íŠ¸ë¦¬í”Œì˜ ì‹œí€€ìŠ¤ êµ¬ì„± ë°©ì‹ ì´í•´

```mermaid
flowchart LR
    subgraph Input["ì…ë ¥ ì‹œí€€ìŠ¤"]
        R0["Râ‚€"] --> S0["sâ‚€"] --> A0["aâ‚€"]
        A0 --> R1["Râ‚"] --> S1["sâ‚"] --> A1["aâ‚"]
        A1 --> R2["Râ‚‚"] --> S2["sâ‚‚"] --> A2["aâ‚‚"]
    end

    subgraph Output["ì¶œë ¥ (ì˜ˆì¸¡ ìœ„ì¹˜)"]
        O0["  "]
        O1["â†’aâ‚€"]
        O2["  "]
        O3["  "]
        O4["â†’aâ‚"]
        O5["  "]
        O6["  "]
        O7["â†’aâ‚‚"]
        O8["  "]
    end

    R0 -.-> O0
    S0 -.-> O1
    A0 -.-> O2
    R1 -.-> O3
    S1 -.-> O4
    A1 -.-> O5
    R2 -.-> O6
    S2 -.-> O7
    A2 -.-> O8

    style S0 fill:#c8e6c9
    style S1 fill:#c8e6c9
    style S2 fill:#c8e6c9
    style O1 fill:#4ecdc4
    style O4 fill:#4ecdc4
    style O7 fill:#4ecdc4
```

**í•µì‹¬ ê°œë…**:
```
Decision Transformer ì…ë ¥:
[Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚, aâ‚, Râ‚‚, sâ‚‚, aâ‚‚, ...]
 â””â”€ íŠ¸ë¦¬í”Œ 1 â”€â”˜  â””â”€ íŠ¸ë¦¬í”Œ 2 â”€â”˜  â””â”€ íŠ¸ë¦¬í”Œ 3 â”€â”˜

ì˜ˆì¸¡ ìœ„ì¹˜:
[  , â†’aâ‚€,   ,   , â†’aâ‚,   ,   , â†’aâ‚‚,   ]
    (sâ‚€ì—ì„œ)      (sâ‚ì—ì„œ)      (sâ‚‚ì—ì„œ)
```

**ì‹¤ìŠµ: ì‹œí€€ìŠ¤ êµ¬ì„± ì‹œê°í™”**
```python
# ì‹œí€€ìŠ¤ê°€ ì–´ë–»ê²Œ êµ¬ì„±ë˜ëŠ”ì§€ ì§ì ‘ í™•ì¸
rtgs = ["R0=10", "R1=9", "R2=7"]
states = ["s0", "s1", "s2"]
actions = ["a0", "a1", "a2"]

sequence = []
for r, s, a in zip(rtgs, states, actions):
    sequence.extend([r, s, a])

print("ì‹œí€€ìŠ¤:", sequence)
# ì¶œë ¥: ['R0=10', 's0', 'a0', 'R1=9', 's1', 'a1', 'R2=7', 's2', 'a2']

# ì˜ˆì¸¡ ìœ„ì¹˜ (state ìœ„ì¹˜ì—ì„œ action ì˜ˆì¸¡)
print("ì˜ˆì¸¡ ìœ„ì¹˜ (1::3):", sequence[1::3])
# ì¶œë ¥: ['s0', 's1', 's2'] - ì´ ìœ„ì¹˜ì—ì„œ a0, a1, a2 ì˜ˆì¸¡
```

---

## Phase 3: Gym í™˜ê²½ ì‹¤ìŠµ

```mermaid
flowchart TD
    subgraph Setup["í™˜ê²½ ì„¤ì •"]
        S1["conda env create"]
        S2["D4RL ì„¤ì¹˜"]
        S3["ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"]
    end

    subgraph Explore["ë°ì´í„° íƒìƒ‰"]
        E1["D4RL êµ¬ì¡° í™•ì¸"]
        E2["State/Action/Reward<br/>shape ì´í•´"]
    end

    subgraph Model["ëª¨ë¸ í•™ìŠµ"]
        M1["DecisionTransformer<br/>í´ë˜ìŠ¤ ë¶„ì„"]
        M2["forward í•¨ìˆ˜<br/>ì´í•´"]
        M3["get_action í•¨ìˆ˜<br/>ì´í•´"]
    end

    subgraph Train["í•™ìŠµ ì‹¤í–‰"]
        T1["experiment.py ì‹¤í–‰"]
        T2["í•™ìŠµ ê³¼ì • ê´€ì°°"]
        T3["í‰ê°€ ê²°ê³¼ í™•ì¸"]
    end

    Setup --> Explore --> Model --> Train

    style Setup fill:#e8f5e9
    style Explore fill:#c8e6c9
    style Model fill:#a5d6a7
    style Train fill:#81c784
```

Gym í™˜ê²½ì´ ë” ë‹¨ìˆœí•˜ë¯€ë¡œ ë¨¼ì € í•™ìŠµí•©ë‹ˆë‹¤.

### 3.1 í™˜ê²½ ì„¤ì •

```bash
# 1. Conda í™˜ê²½ ìƒì„±
cd gym
conda env create -f conda_env.yml
conda activate decision-transformer-gym

# 2. D4RL ì„¤ì¹˜ (ë°ì´í„°ì…‹ìš©)
pip install git+https://github.com/Farama-Foundation/d4rl@master#egg=d4rl

# 3. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
python data/download_d4rl_datasets.py
```

### 3.2 ë°ì´í„° êµ¬ì¡° íƒìƒ‰

**ëª©í‘œ**: D4RL ë°ì´í„°ì…‹ì˜ êµ¬ì¡° ì´í•´

**ì‹¤ìŠµ: ë°ì´í„°ì…‹ ì§ì ‘ ì—´ì–´ë³´ê¸°**
```python
# gym/ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰
cd gym
python3

import pickle

# ë°ì´í„°ì…‹ ë¡œë“œ
with open('data/hopper-medium-v2.pkl', 'rb') as f:
    trajectories = pickle.load(f)

# ë°ì´í„° êµ¬ì¡° í™•ì¸
print(f"ê¶¤ì  ìˆ˜: {len(trajectories)}")
print(f"ì²« ë²ˆì§¸ ê¶¤ì  í‚¤: {trajectories[0].keys()}")

traj = trajectories[0]
print(f"State shape: {traj['observations'].shape}")
print(f"Action shape: {traj['actions'].shape}")
print(f"Reward shape: {traj['rewards'].shape}")

# ëª‡ ê°œì˜ ê°’ ì¶œë ¥
print(f"\nì²« 3ê°œ state:\n{traj['observations'][:3]}")
print(f"\nì²« 3ê°œ action:\n{traj['actions'][:3]}")
print(f"\nì²« 3ê°œ reward: {traj['rewards'][:3]}")
```

### 3.3 ëª¨ë¸ ì½”ë“œ ì½ê¸°

**ëª©í‘œ**: DecisionTransformer í´ë˜ìŠ¤ ì´í•´

```mermaid
sequenceDiagram
    participant Code as ğŸ‘€ ë…ì
    participant File as decision_transformer.py
    participant Init as __init__
    participant Forward as forward()
    participant GetAction as get_action()

    Code->>File: íŒŒì¼ ì—´ê¸°

    Note over Code,File: 1ë‹¨ê³„: í´ë˜ìŠ¤ êµ¬ì¡° íŒŒì•…
    Code->>Init: lines 10-50
    Init-->>Code: ì„ë² ë”© ë ˆì´ì–´ í™•ì¸<br/>(embed_timestep, embed_return,<br/>embed_state, embed_action)

    Note over Code,File: 2ë‹¨ê³„: forward í•¨ìˆ˜ ë¶„ì„
    Code->>Forward: lines 52-99
    Forward-->>Code: 1. ê° ì…ë ¥ ì„ë² ë”©<br/>2. timestep ì„ë² ë”© ì¶”ê°€<br/>3. ì‹œí€€ìŠ¤ ì¸í„°ë¦¬ë¹™<br/>4. Transformer í†µê³¼<br/>5. action ì˜ˆì¸¡ ì¶”ì¶œ

    Note over Code,File: 3ë‹¨ê³„: get_action í•¨ìˆ˜ ë¶„ì„
    Code->>GetAction: lines 103-140
    GetAction-->>Code: ì¶”ë¡  ì‹œ action íšë“<br/>- íˆìŠ¤í† ë¦¬ ìë¥´ê¸°<br/>- íŒ¨ë”© ì²˜ë¦¬<br/>- forward í˜¸ì¶œ<br/>- ë§ˆì§€ë§‰ action ë°˜í™˜

    Note over Code: âœ… ëª¨ë¸ êµ¬ì¡° ì´í•´ ì™„ë£Œ!
```

**ì½ì„ íŒŒì¼**: `gym/decision_transformer/models/decision_transformer.py`

**ë‹¨ê³„ë³„ ì½”ë“œ ì½ê¸°**:

1. **í´ë˜ìŠ¤ êµ¬ì¡° íŒŒì•…** (lines 10-50)
```python
# ì–´ë–¤ ë ˆì´ì–´ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
# - embed_timestep: ì‹œê°„ ì„ë² ë”©
# - embed_return: RTG ì„ë² ë”©
# - embed_state: ìƒíƒœ ì„ë² ë”©
# - embed_action: í–‰ë™ ì„ë² ë”©
# - transformer: GPT2 ëª¨ë¸
# - predict_action: í–‰ë™ ì˜ˆì¸¡ í—¤ë“œ
```

2. **forward í•¨ìˆ˜ ë¶„ì„** (lines 52-99)
```python
# ì…ë ¥ì´ ì–´ë–»ê²Œ ì²˜ë¦¬ë˜ëŠ”ì§€ ì¶”ì 
# 1. ê° ì…ë ¥(state, action, return) ì„ë² ë”©
# 2. timestep ì„ë² ë”© ì¶”ê°€
# 3. ì‹œí€€ìŠ¤ ì¸í„°ë¦¬ë¹™ (stacked_inputs)
# 4. Transformer í†µê³¼
# 5. action ì˜ˆì¸¡ ì¶”ì¶œ
```

3. **get_action í•¨ìˆ˜ ë¶„ì„** (lines 103-140)
```python
# ì¶”ë¡  ì‹œ ì–´ë–»ê²Œ actionì„ ì–»ëŠ”ì§€ í™•ì¸
# - íˆìŠ¤í† ë¦¬ ìë¥´ê¸° (max_length)
# - íŒ¨ë”© ì²˜ë¦¬
# - forward í˜¸ì¶œ
# - ë§ˆì§€ë§‰ action ë°˜í™˜
```

**ì‹¤ìŠµ: ëª¨ë¸ êµ¬ì¡° ì¶œë ¥í•˜ê¸°**
```python
cd gym
python3

import torch
from decision_transformer.models.decision_transformer import DecisionTransformer

# ëª¨ë¸ ìƒì„±
model = DecisionTransformer(
    state_dim=11,      # hopperì˜ state ì°¨ì›
    act_dim=3,         # hopperì˜ action ì°¨ì›
    max_length=20,     # context length (K)
    max_ep_len=1000,   # ìµœëŒ€ ì—í”¼ì†Œë“œ ê¸¸ì´
    hidden_size=128,
    n_layer=3,
    n_head=1,
    n_inner=128*4,
    activation_function='relu',
    n_positions=1024,
    resid_pdrop=0.1,
    attn_pdrop=0.1,
)

print(model)

# íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
total_params = sum(p.numel() for p in model.parameters())
print(f"\nì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
```

### 3.4 í•™ìŠµ ì‹¤í–‰

**ëª©í‘œ**: ì‹¤ì œ í•™ìŠµì„ ëŒë ¤ë³´ê³  ê²°ê³¼ í™•ì¸

```bash
cd gym

# ì§§ì€ í•™ìŠµ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)
python experiment.py \
    --env hopper \
    --dataset medium \
    --model_type dt \
    --max_iters 5 \
    --num_steps_per_iter 100

# ì˜ˆìƒ ì¶œë ¥:
# iteration 1, eval return 1234.56 (target 1800), ...
```

**ì‹¤ìŠµ: í•™ìŠµ ê³¼ì • ì´í•´í•˜ê¸°**
```python
# experiment.py ì£¼ìš” ë¶€ë¶„ ë¶„ì„
# 1. get_batch() í•¨ìˆ˜ (line 118-164): ë°°ì¹˜ ìƒì„±
# 2. model.forward() í˜¸ì¶œ
# 3. MSE ì†ì‹¤ ê³„ì‚°
# 4. í‰ê°€ (evaluate_episode_rtg)
```

### 3.5 í‰ê°€ ê³¼ì • ì´í•´

**ëª©í‘œ**: RTG ì¡°ê±´ë¶€ ì¶”ë¡  ì´í•´

```mermaid
sequenceDiagram
    participant Code as ğŸ‘€ ë…ì
    participant Eval as evaluate_episode_rtg
    participant Model as get_action()
    participant Env as ğŸ¤¸ Hopper
    participant RTG as RTG ë³€ìˆ˜

    Note over Code: íŒŒì¼: evaluate_episodes.py

    Code->>Eval: í•¨ìˆ˜ ì½ê¸° ì‹œì‘

    Eval->>RTG: target_return = 1800<br/>(ì´ˆê¸° ëª©í‘œ)

    loop ì—í”¼ì†Œë“œ ì§„í–‰
        Eval->>Model: get_action(state, rtg=target_return)
        Model-->>Eval: action_pred

        Eval->>Env: step(action_pred)
        Env-->>Eval: state, reward, done

        Eval->>RTG: pred_return = target_return - reward/scale
        RTG-->>Eval: ìƒˆë¡œìš´ RTG

        Eval->>RTG: target_return = cat([old, new])
        Note over RTG: RTG ë°°ì—´ ì—…ë°ì´íŠ¸

        alt done=True
            Eval-->>Code: episode_return
        end
    end

    Note over Code: âœ… RTG ë™ì  ì—…ë°ì´íŠ¸ ì´í•´!
```

**ì½ì„ íŒŒì¼**: `gym/decision_transformer/evaluation/evaluate_episodes.py`

**í•µì‹¬ ì½”ë“œ ë¶„ì„**:
```python
# evaluate_episode_rtg í•¨ìˆ˜ì—ì„œ:
# 1. ì´ˆê¸° target_return ì„¤ì •
# 2. ë§¤ ìŠ¤í…ë§ˆë‹¤:
#    - model.get_action() í˜¸ì¶œ
#    - í™˜ê²½ì—ì„œ action ì‹¤í–‰
#    - target_return -= reward/scale  # RTG ì—…ë°ì´íŠ¸!
```

**ì‹¤ìŠµ: RTG ì—…ë°ì´íŠ¸ ì‹œë®¬ë ˆì´ì…˜**
```python
# ì¶”ë¡  ì‹œ RTGê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì‹œë®¬ë ˆì´ì…˜
target_return = 1800  # ëª©í‘œ
scale = 1000
rewards = [50, 100, 75, 200, 150]  # ê°€ìƒì˜ ë³´ìƒë“¤

print(f"ì´ˆê¸° RTG: {target_return}")
for i, reward in enumerate(rewards):
    target_return -= reward / scale
    print(f"Step {i+1}: reward={reward}, ìƒˆ RTG={target_return:.2f}")
```

---

## Phase 4: Atari í™˜ê²½ ì‹¤ìŠµ

```mermaid
flowchart TD
    subgraph Setup["í™˜ê²½ ì„¤ì •"]
        S1["conda env create"]
        S2["gsutil ì„¤ì¹˜"]
        S3["DQN ë²„í¼ ë‹¤ìš´ë¡œë“œ"]
    end

    subgraph Data["ë°ì´í„° ì´í•´"]
        D1["create_dataset.py<br/>ë¶„ì„"]
        D2["RTG ê³„ì‚° ê³¼ì •<br/>ì´í•´"]
    end

    subgraph Model["ëª¨ë¸ ì´í•´"]
        M1["CNN Encoder<br/>(4Ã—84Ã—84 â†’ 128)"]
        M2["GPT êµ¬ì¡°<br/>(6 layers, 8 heads)"]
        M3["reward_conditioned<br/>vs naive"]
    end

    subgraph Train["í•™ìŠµ ë° í‰ê°€"]
        T1["run_dt_atari.py ì‹¤í–‰"]
        T2["get_returns()ë¡œ<br/>ì‹¤ì œ ê²Œì„ í‰ê°€"]
        T3["RTG ì—…ë°ì´íŠ¸<br/>ê´€ì°°"]
    end

    Setup --> Data --> Model --> Train

    style Setup fill:#ffebee
    style Data fill:#ef9a9a
    style Model fill:#e57373
    style Train fill:#ef5350
```

### 4.1 í™˜ê²½ ì„¤ì •

```bash
# 1. Conda í™˜ê²½ ìƒì„±
cd atari
conda env create -f conda_env.yml
conda activate decision-transformer-atari

# 2. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (ìš©ëŸ‰ì´ í¼, í•˜ë‚˜ì˜ ê²Œì„ë§Œ ë¨¼ì €)
mkdir -p dqn_replay
gsutil -m cp -R gs://atari-replay-datasets/dqn/Breakout dqn_replay/

# gsutilì´ ì—†ë‹¤ë©´ Google Cloud SDK ì„¤ì¹˜ í•„ìš”
```

### 4.2 ë°ì´í„°ì…‹ ìƒì„± ê³¼ì • ì´í•´

**ëª©í‘œ**: DQN replay buffer â†’ RTG ë°ì´í„°ì…‹ ë³€í™˜ ì´í•´

```mermaid
sequenceDiagram
    participant Code as ğŸ‘€ ë…ì
    participant Create as create_dataset()
    participant Buffer as FixedReplayBuffer
    participant RTG as RTG ê³„ì‚°
    participant Output as ì¶œë ¥

    Note over Code: íŒŒì¼: create_dataset.py

    Code->>Create: í•¨ìˆ˜ í˜¸ì¶œ

    loop num_steps ë‹¬ì„±í•  ë•Œê¹Œì§€
        Create->>Buffer: ëœë¤ ë²„í¼ ì„ íƒ
        Buffer-->>Create: buffer_num

        Create->>Buffer: sample_transition_batch()
        Buffer-->>Create: (states, actions, rewards)

        alt terminal=True
            Create->>Create: done_idxsì— ê¸°ë¡
        end
    end

    Create->>RTG: ê° ì—í”¼ì†Œë“œë³„ RTG ê³„ì‚°

    Note over RTG: ì—­ìˆœ ìˆœíšŒ:
    Note over RTG: for j in range(i-1, start-1, -1):
    Note over RTG:     rtg[j] = sum(rewards[j:i])

    RTG-->>Create: rtg ë°°ì—´

    Create->>Output: (obss, actions, rtgs, timesteps)

    Note over Code: âœ… RTG ê³„ì‚° ê³¼ì • ì´í•´!
```

**ì½ì„ íŒŒì¼**: `atari/create_dataset.py`

**í•µì‹¬ ì½”ë“œ**:
```python
# RTG ê³„ì‚° ë¶€ë¶„ (lines 81-90)
for j in range(i-1, start_index-1, -1):  # ì—­ìˆœ ìˆœíšŒ!
    rtg_j = curr_traj_returns[j-start_index:i-start_index]
    rtg[j] = sum(rtg_j)
```

**ì‹¤ìŠµ: ë°ì´í„°ì…‹ ìƒì„± í…ŒìŠ¤íŠ¸**
```python
cd atari
python3

from create_dataset import create_dataset

# ì†ŒëŸ‰ì˜ ë°ì´í„°ë§Œ ë¡œë“œ (í…ŒìŠ¤íŠ¸ìš©)
obss, actions, returns, done_idxs, rtgs, timesteps = create_dataset(
    num_buffers=5,        # 5ê°œ ë²„í¼ë§Œ
    num_steps=1000,       # 1000 ìŠ¤í…ë§Œ
    game='Breakout',
    data_dir_prefix='./dqn_replay/',
    trajectories_per_buffer=2
)

print(f"ê´€ì¸¡ ìˆ˜: {len(obss)}")
print(f"ê´€ì¸¡ shape: {obss[0].shape}")  # (4, 84, 84)
print(f"í–‰ë™ ìˆ˜: {len(actions)}")
print(f"RTG ë²”ìœ„: {min(rtgs)} ~ {max(rtgs)}")
print(f"ì—í”¼ì†Œë“œ ìˆ˜: {len(done_idxs)}")
```

### 4.3 ëª¨ë¸ êµ¬ì¡° ì´í•´

**ëª©í‘œ**: Atariìš© GPT ëª¨ë¸ì˜ CNN encoder ì´í•´

**ì½ì„ íŒŒì¼**: `atari/mingpt/model_atari.py`

**í•µì‹¬ êµ¬ì¡°**:
```python
# State encoder (CNN) - lines 149-152
# 4Ã—84Ã—84 ì´ë¯¸ì§€ â†’ 128 ì°¨ì› ë²¡í„°

# Conv2d(4, 32, 8, stride=4)  â†’ 32Ã—20Ã—20
# Conv2d(32, 64, 4, stride=2) â†’ 64Ã—9Ã—9
# Conv2d(64, 64, 3, stride=1) â†’ 64Ã—7Ã—7
# Flatten â†’ 3136
# Linear(3136, 128) â†’ 128
```

**ì‹¤ìŠµ: CNN encoder ì´í•´í•˜ê¸°**
```python
cd atari
python3

import torch
import torch.nn as nn

# Atari CNN encoder êµ¬í˜„
encoder = nn.Sequential(
    nn.Conv2d(4, 32, 8, stride=4, padding=0), nn.ReLU(),
    nn.Conv2d(32, 64, 4, stride=2, padding=0), nn.ReLU(),
    nn.Conv2d(64, 64, 3, stride=1, padding=0), nn.ReLU(),
    nn.Flatten(),
    nn.Linear(3136, 128),
    nn.Tanh()
)

# ì…ë ¥ í…ŒìŠ¤íŠ¸
x = torch.randn(1, 4, 84, 84)  # ë°°ì¹˜ 1, 4í”„ë ˆì„, 84x84
out = encoder(x)
print(f"ì…ë ¥ shape: {x.shape}")
print(f"ì¶œë ¥ shape: {out.shape}")  # (1, 128)
```

### 4.4 í•™ìŠµ ì‹¤í–‰

```bash
cd atari

# ì§§ì€ í•™ìŠµ (GPU ê¶Œì¥)
python run_dt_atari.py \
    --seed 123 \
    --context_length 30 \
    --epochs 1 \
    --model_type 'reward_conditioned' \
    --num_steps 10000 \
    --num_buffers 5 \
    --game 'Breakout' \
    --batch_size 64 \
    --data_dir_prefix ./dqn_replay
```

### 4.5 Reward-Conditioned vs Naive ë¹„êµ

**ëª©í‘œ**: ë‘ ëª¨ë“œì˜ ì°¨ì´ ì´í•´

```mermaid
flowchart TB
    subgraph DT["Reward-Conditioned (Decision Transformer)"]
        DTSeq["ì‹œí€€ìŠ¤: [R, s, a, R, s, a, ...]"]
        DTCond["RTGë¡œ ëª©í‘œ ì§€ì •"]
        DTPred["State ìœ„ì¹˜(1::3)ì—ì„œ ì˜ˆì¸¡"]
        DTUse["ì›í•˜ëŠ” ì„±ëŠ¥ ë‹¬ì„±"]
    end

    subgraph BC["Naive (Behavior Cloning)"]
        BCSeq["ì‹œí€€ìŠ¤: [s, a, s, a, ...]"]
        BCCond["ì¡°ê±´í™” ì—†ìŒ"]
        BCPred["State ìœ„ì¹˜(0::2)ì—ì„œ ì˜ˆì¸¡"]
        BCUse["í‰ê· ì  í–‰ë™ ëª¨ë°©"]
    end

    style DT fill:#c8e6c9
    style DTCond fill:#4ecdc4
    style BC fill:#fff3e0
    style BCCond fill:#ffccbc
```

| í•­ëª© | Reward-Conditioned | Naive |
|-----|-------------------|-------|
| ì‹œí€€ìŠ¤ | [R, s, a, R, s, a, ...] | [s, a, s, a, ...] |
| ì¡°ê±´í™” | RTGë¡œ ëª©í‘œ ì§€ì • | ì—†ìŒ |
| ì¶”ë¡  ì‹œ | ì›í•˜ëŠ” RTG ì„¤ì • ê°€ëŠ¥ | í‰ê· ì  í–‰ë™ |
| ì˜ˆì¸¡ ìœ„ì¹˜ | 1::3 (state ìœ„ì¹˜) | 0::2 (state ìœ„ì¹˜) |

**ì‹¤ìŠµ: ë‘ ëª¨ë“œ ë¹„êµ ì‹¤í–‰**
```bash
# Reward-conditioned (DT)
python run_dt_atari.py --model_type reward_conditioned --epochs 1 --num_steps 5000

# Naive (BC)
python run_dt_atari.py --model_type naive --epochs 1 --num_steps 5000
```

---

## Phase 5: ì‹¬í™” í•™ìŠµ (ì„ íƒ)

### 5.1 ì½”ë“œ ìˆ˜ì • ì‹¤ìŠµ

**ì•„ì´ë””ì–´ 1**: Context length ë³€ê²½ íš¨ê³¼ ì‹¤í—˜
```bash
# K=10 vs K=30 vs K=50 ë¹„êµ
python run_dt_atari.py --context_length 10 ...
python run_dt_atari.py --context_length 30 ...
python run_dt_atari.py --context_length 50 ...
```

**ì•„ì´ë””ì–´ 2**: ë‹¤ë¥¸ ê²Œì„ ì‹¤í—˜
```bash
# Pong ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ í›„
gsutil -m cp -R gs://atari-replay-datasets/dqn/Pong dqn_replay/
python run_dt_atari.py --game Pong ...
```

### 5.2 ë…¼ë¬¸ ì‹¬í™” ì½ê¸°

- [ ] Experiments ì„¹ì…˜ ì „ì²´ ì½ê¸°
- [ ] Ablation studies ë¶„ì„
- [ ] ê´€ë ¨ ë…¼ë¬¸: Trajectory Transformer, Online Decision Transformer

### 5.3 ì»¤ìŠ¤í…€ í™˜ê²½ ì ìš©

ê°„ë‹¨í•œ í™˜ê²½ì— Decision Transformer ì ìš©í•´ë³´ê¸°:
1. CartPole í™˜ê²½
2. ìì²´ ë°ì´í„°ì…‹ ìƒì„±
3. í•™ìŠµ ë° í‰ê°€

---

## ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1 ì™„ë£Œ ì²´í¬
- [ ] RL ê¸°ë³¸ ìš©ì–´ ì„¤ëª… ê°€ëŠ¥
- [ ] Transformer attention ì´í•´
- [ ] Causal masking ì„¤ëª… ê°€ëŠ¥

### Phase 2 ì™„ë£Œ ì²´í¬
- [ ] RTG ì†ìœ¼ë¡œ ê³„ì‚° ê°€ëŠ¥
- [ ] DTì˜ ì‹œí€€ìŠ¤ êµ¬ì„± ì„¤ëª… ê°€ëŠ¥
- [ ] ë…¼ë¬¸ í•µì‹¬ ì•„ì´ë””ì–´ ìš”ì•½ ê°€ëŠ¥

### Phase 3 ì™„ë£Œ ì²´í¬
- [ ] Gym í™˜ê²½ ì„¤ì • ì™„ë£Œ
- [ ] ë°ì´í„°ì…‹ êµ¬ì¡° ì´í•´
- [ ] í•™ìŠµ 1íšŒ ì´ìƒ ì‹¤í–‰
- [ ] get_action í•¨ìˆ˜ ì´í•´

### Phase 4 ì™„ë£Œ ì²´í¬
- [ ] Atari í™˜ê²½ ì„¤ì • ì™„ë£Œ
- [ ] CNN encoder ì´í•´
- [ ] reward_conditioned vs naive ì°¨ì´ ì„¤ëª… ê°€ëŠ¥
- [ ] í•™ìŠµ 1íšŒ ì´ìƒ ì‹¤í–‰

### Phase 5 ì™„ë£Œ ì²´í¬
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‹¤í—˜ ìˆ˜í–‰
- [ ] ë‹¤ë¥¸ ê²Œì„/í™˜ê²½ ì‹œë„
- [ ] ì½”ë“œ ìˆ˜ì • ê²½í—˜

---

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ìì£¼ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜

**1. MuJoCo ë¼ì´ì„ ìŠ¤ ì˜¤ë¥˜**
```bash
# MuJoCo 2.1+ ëŠ” ë¬´ë£Œ
pip install mujoco
```

**2. gsutil ì—†ìŒ**
```bash
# Google Cloud SDK ì„¤ì¹˜
curl https://sdk.cloud.google.com | bash
gcloud init
```

**3. CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±**
```bash
# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¤„ì´ê¸°
--batch_size 32
```

**4. D4RL ì„¤ì¹˜ ì˜¤ë¥˜**
```bash
pip install git+https://github.com/Farama-Foundation/d4rl@master#egg=d4rl
```

---

## ë‹¤ìŒ ë‹¨ê³„

ì´ í•™ìŠµ ê³„íšì„ ì™„ë£Œí•œ í›„:

1. **Online Decision Transformer** ë…¼ë¬¸ ì½ê¸°
2. **Trajectory Transformer** ë¹„êµ ë¶„ì„
3. **ì‹¤ì œ ë¡œë´‡ í™˜ê²½** ì ìš© ì‹œë„
4. **ì»¤ìŠ¤í…€ ë³´ìƒ í•¨ìˆ˜** ì‹¤í—˜

---

## ê´€ë ¨ ë¬¸ì„œ

- [architecture-flow.md](./architecture-flow.md): Atari ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨
- [code-walkthrough.md](./code-walkthrough.md): ì½”ë“œ ìƒì„¸ ë¶„ì„
- [system-analysis.md](./system-analysis.md): ì „ì²´ ì‹œìŠ¤í…œ ë¶„ì„
