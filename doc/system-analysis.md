# Decision Transformer ì „ì²´ ì‹œìŠ¤í…œ ë¶„ì„

ì´ ë¬¸ì„œëŠ” Decision Transformerì˜ ì „ì²´ ì‹œìŠ¤í…œ(Atari + Gym í™˜ê²½)ì„ ë¶„ì„í•œ ì¢…í•© ë¬¸ì„œì…ë‹ˆë‹¤.

## ëª©ì°¨

1. [ê°œìš”](#1-ê°œìš”)
2. [í•µì‹¬ ëª¨ë¸ ì»´í¬ë„ŒíŠ¸](#2-í•µì‹¬-ëª¨ë¸-ì»´í¬ë„ŒíŠ¸)
3. [ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸](#3-ë°ì´í„°-ì²˜ë¦¬-íŒŒì´í”„ë¼ì¸)
4. [í•™ìŠµ ì‹œìŠ¤í…œ](#4-í•™ìŠµ-ì‹œìŠ¤í…œ)
5. [í‰ê°€ ì‹œìŠ¤í…œ](#5-í‰ê°€-ì‹œìŠ¤í…œ)
6. [ì „ì²´ ì‹¤í–‰ íë¦„](#6-ì „ì²´-ì‹¤í–‰-íë¦„)
7. [ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°](#7-ì£¼ìš”-í•˜ì´í¼íŒŒë¼ë¯¸í„°)
8. [í•µì‹¬ ì„¤ê³„ ê²°ì •](#8-í•µì‹¬-ì„¤ê³„-ê²°ì •)
9. [íŒŒì¼ êµ¬ì¡° ìš”ì•½](#9-íŒŒì¼-êµ¬ì¡°-ìš”ì•½)

---

## 1. ê°œìš”

```mermaid
flowchart TB
    subgraph Traditional["ğŸ”„ ê¸°ì¡´ RL"]
        TBellman["Bellman Equation"]
        TValue["Value Function"]
        TPolicy["Policy Optimization"]
        TBellman --> TValue
        TValue --> TPolicy
    end

    subgraph DT["ğŸ¤– Decision Transformer"]
        DSeq["Sequence Modeling"]
        DGPT["GPT Architecture"]
        DRTG["Return-to-Go Conditioning"]
        DSeq --> DGPT
        DGPT --> DRTG
    end

    Traditional -.->|"íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜"| DT

    style Traditional fill:#ffebee
    style DT fill:#e8f5e9
```

Decision TransformerëŠ” ê°•í™”í•™ìŠµ(RL)ì„ **ì‹œí€€ìŠ¤ ëª¨ë¸ë§ ë¬¸ì œ**ë¡œ ì¬êµ¬ì„±í•œ ì—°êµ¬ì…ë‹ˆë‹¤. ê¸°ì¡´ RLì˜ ë²¨ë§Œ ë°©ì •ì‹(Bellman equation) ê¸°ë°˜ ì ‘ê·¼ë²• ëŒ€ì‹ , GPT ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ (Return-to-go, State, Action) ì‹œí€€ìŠ¤ë¥¼ autoregressiveí•˜ê²Œ ëª¨ë¸ë§í•©ë‹ˆë‹¤.

**í•µì‹¬ ì•„ì´ë””ì–´**: ì›í•˜ëŠ” return(ëª©í‘œ ë³´ìƒ)ì„ ì¡°ê±´ìœ¼ë¡œ ì£¼ë©´, í•´ë‹¹ returnì„ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” actionì„ ì˜ˆì¸¡

---

## 1. ê°œìš”

Decision TransformerëŠ” ê°•í™”í•™ìŠµ(RL)ì„ **ì‹œí€€ìŠ¤ ëª¨ë¸ë§ ë¬¸ì œ**ë¡œ ì¬êµ¬ì„±í•œ ì—°êµ¬ì…ë‹ˆë‹¤. ê¸°ì¡´ RLì˜ ë²¨ë§Œ ë°©ì •ì‹(Bellman equation) ê¸°ë°˜ ì ‘ê·¼ë²• ëŒ€ì‹ , GPT ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ (Return-to-go, State, Action) ì‹œí€€ìŠ¤ë¥¼ autoregressiveí•˜ê²Œ ëª¨ë¸ë§í•©ë‹ˆë‹¤.

**í•µì‹¬ ì•„ì´ë””ì–´**: ì›í•˜ëŠ” return(ëª©í‘œ ë³´ìƒ)ì„ ì¡°ê±´ìœ¼ë¡œ ì£¼ë©´, í•´ë‹¹ returnì„ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” actionì„ ì˜ˆì¸¡

---

## 2. í•µì‹¬ ëª¨ë¸ ì»´í¬ë„ŒíŠ¸

```mermaid
flowchart TB
    subgraph Base["TrajectoryModel (ì¶”ìƒ ë² ì´ìŠ¤)"]
        direction TB
        BaseClass["TrajectoryModel"]
        BaseForward["forward(states, actions, ...)<br/>ì¶”ìƒ ë©”ì„œë“œ"]
    end

    subgraph DT["Decision Transformer"]
        direction TB
        DTClass["DecisionTransformer"]
        DTForward["forward()"]
        GetAction["get_action()<br/>ì¶”ë¡ ìš©"]
    end

    subgraph BC["Behavior Cloning"]
        direction TB
        BCClass["MLPBCModel"]
        BCForward["forward()"]
    end

    Base --> DT
    Base --> BC

    style Base fill:#e3f2fd
    style DT fill:#c8e6c9
    style BC fill:#fff3e0
    style GetAction fill:#ffccbc
```

### 2.1 Gym í™˜ê²½: DecisionTransformer

**íŒŒì¼**: [decision_transformer.py](../gym/decision_transformer/models/decision_transformer.py)

```
ì…ë ¥: (states, actions, rewards, returns_to_go, timesteps)
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“          â†“          â†“
   embed_state  embed_action  embed_return
   (Linear)     (Linear)      (Linear)
        â†“          â†“          â†“
        â””â”€â”€â”€â”€ + time_embeddings â”€â”€â”€â”€â”˜
                    â†“
         Interleave: [Râ‚, sâ‚, aâ‚, Râ‚‚, sâ‚‚, aâ‚‚, ...]
                    â†“
              LayerNorm + Dropout
                    â†“
           GPT2Model (Transformer)
                    â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“          â†“          â†“
   predict_state  predict_action  predict_return
                    â†“
              action_preds (Tanh)
```

**ì£¼ìš” íŠ¹ì§•**:
- **Timestep Embedding**: ìœ„ì¹˜ ì„ë² ë”© ëŒ€ì‹  timestepì„ ì§ì ‘ ì„ë² ë”© (line 40)
- **Sequence Interleaving**: (R, s, a) íŠ¸ë¦¬í”Œì„ ì¸í„°ë¦¬ë¹™í•˜ì—¬ 3Ã—seq_len ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ ìƒì„± (lines 73-78)
- **Action Prediction**: state í† í° ìœ„ì¹˜ì—ì„œ ë‹¤ìŒ action ì˜ˆì¸¡ (line 99)

```python
# ì‹œí€€ìŠ¤ êµ¬ì„± í•µì‹¬ ì½”ë“œ
stacked_inputs = torch.stack(
    (returns_embeddings, state_embeddings, action_embeddings), dim=1
).permute(0, 2, 1, 3).reshape(batch_size, 3*seq_length, self.hidden_size)
```

### 2.2 Atari í™˜ê²½: GPT (minGPT ê¸°ë°˜)

**íŒŒì¼**: [model_atari.py](../atari/mingpt/model_atari.py)

```
ì…ë ¥: (states, actions, targets, rtgs, timesteps)
                    â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                   â†“
   state_encoder        ret_emb + action_embeddings
   (Conv2d â†’ Linear)    (Linear + Embedding)
         â†“                   â†“
         â””â”€â”€â”€â”€â”€ Interleave â”€â”€â”˜
                    â†“
       position_embeddings (global + local)
                    â†“
           Transformer Blocks (6 layers)
                    â†“
              logits â†’ Cross Entropy Loss
```

**ì£¼ìš” íŠ¹ì§•**:
- **State Encoder**: 4Ã—84Ã—84 ì´ë¯¸ì§€ë¥¼ CNNìœ¼ë¡œ ì¸ì½”ë”© (lines 149-152)
- **Two Modes**: `reward_conditioned` (DT) vs `naive` (BC) (line 127)
- **Action Classification**: ì´ì‚°ì  action space â†’ Cross Entropy ì†ì‹¤ (line 279)

### 2.3 ëª¨ë¸ ê³„ì¸µ êµ¬ì¡°

```
TrajectoryModel (ì¶”ìƒ ë² ì´ìŠ¤)
    â”œâ”€â”€ DecisionTransformer
    â”‚       â””â”€â”€ GPT2Model (ì»¤ìŠ¤í…€, ìœ„ì¹˜ ì„ë² ë”© ì œê±°)
    â””â”€â”€ MLPBCModel (Behavior Cloning ë² ì´ìŠ¤ë¼ì¸)
```

**MLPBCModel** ([mlp_bc.py](../gym/decision_transformer/models/mlp_bc.py)): ë‹¨ìˆœ MLPë¡œ ìµœê·¼ Kê°œ stateì—ì„œ action ì˜ˆì¸¡

---

## 3. ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

```mermaid
flowchart TB
    subgraph AtariData["ğŸ® Atari ë°ì´í„°"]
        DQN["DQN Replay Buffers<br/>(50 checkpoints)"]
        Sample["ê¶¤ì  ìƒ˜í”Œë§<br/>FixedReplayBuffer"]
        RTG["RTG ê³„ì‚°<br/>í›„ë°© ëˆ„ì í•©"]
        AtariDS["StateActionReturnDataset"]
    end

    subgraph GymData["ğŸ¤¸ Gym ë°ì´í„°"]
        D4RL["D4RL Pickle<br/>trajectories"]
        Norm["State ì •ê·œí™”<br/>mean, std"]
        Batch["get_batch()<br/>ê¸¸ì´ ë¹„ë¡€ ìƒ˜í”Œë§"]
        GymDS["Context K ì„œë¸Œì‹œí€€ìŠ¤"]
    end

    DQN --> Sample --> RTG --> AtariDS
    D4RL --> Norm --> Batch --> GymDS

    style DQN fill:#fff3e0
    style D4RL fill:#e8f5e9
    style RTG fill:#ffccbc
    style Batch fill:#c8e6c9
```

### 3.1 Atari ë°ì´í„°ì…‹ ìƒì„±

**íŒŒì¼**: [create_dataset.py](../atari/create_dataset.py)

```
DQN Replay Buffers (50ê°œ)
         â†“
FixedReplayBuffer.sample_transition_batch()
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ obss: list of (4, 84, 84)  â”‚
â”‚ actions: [aâ‚, aâ‚‚, ...]     â”‚
â”‚ stepwise_returns: [râ‚, râ‚‚] â”‚
â”‚ done_idxs: ì—í”¼ì†Œë“œ ê²½ê³„    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
Returns-to-go ê³„ì‚° (í›„ë°© ëˆ„ì í•©)
         â†“
StateActionReturnDataset
```

**RTG ê³„ì‚°** (lines 81-90):
```python
for j in range(i-1, start_index-1, -1):  # ì—­ìˆœ ìˆœíšŒ
    rtg_j = curr_traj_returns[j-start_index:i-start_index]
    rtg[j] = sum(rtg_j)
```

### 3.2 Gym ë°°ì¹˜ ìƒì„±

**íŒŒì¼**: [experiment.py](../gym/experiment.py) (lines 118-164)

```
D4RL Pickle ë¡œë“œ (trajectories)
         â†“
State ì •ê·œí™” (mean, std ê³„ì‚°)
         â†“
get_batch() í•¨ìˆ˜:
  1. ê¶¤ì  ìƒ˜í”Œë§ (ê¸¸ì´ì— ë¹„ë¡€í•œ í™•ë¥ )
  2. ëœë¤ ì‹œì‘ì ì—ì„œ K ê¸¸ì´ ì„œë¸Œì‹œí€€ìŠ¤ ì¶”ì¶œ
  3. discount_cumsumìœ¼ë¡œ RTG ê³„ì‚°
  4. íŒ¨ë”© (ì•ìª½ì— zeros)
  5. ì •ê·œí™” ë° ìŠ¤ì¼€ì¼ë§ (RTG / scale)
         â†“
(states, actions, rewards, dones, rtg, timesteps, mask)
```

**discount_cumsum** (lines 18-23):
```python
def discount_cumsum(x, gamma):
    discount_cumsum[-1] = x[-1]
    for t in reversed(range(x.shape[0]-1)):
        discount_cumsum[t] = x[t] + gamma * discount_cumsum[t+1]
```

---

## 4. í•™ìŠµ ì‹œìŠ¤í…œ

```mermaid
flowchart TB
    subgraph GymTrainer["ğŸ¤¸ Gym Trainer"]
        direction TB
        Base["Trainer<br/>(ë² ì´ìŠ¤ í´ë˜ìŠ¤)"]
        SeqTr["SequenceTrainer<br/>(DTìš©)"]
        ActTr["ActTrainer<br/>(BCìš©)"]
    end

    subgraph Atarirainer["ğŸ® Atari Trainer"]
        direction TB
        Trainer["Trainer"]
        RunEpoch["run_epoch()<br/>DataLoader ê¸°ë°˜"]
        LRDecay["Cosine LR + Warmup"]
    end

    Base --> SeqTr
    Base --> ActTr
    Trainer --> RunEpoch --> LRDecay

    style SeqTr fill:#c8e6c9
    style ActTr fill:#fff3e0
    style LRDecay fill:#ffccbc
```

### 4.1 Gym Trainer ê³„ì¸µ
    â”œâ”€â”€ train_iteration(): ì—í­ ë‹¨ìœ„ í•™ìŠµ
    â”œâ”€â”€ train_step(): ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
    â””â”€â”€ eval_fns: í‰ê°€ í•¨ìˆ˜ ë¦¬ìŠ¤íŠ¸
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â†“         â†“
SequenceTrainer    ActTrainer
(DTìš©)             (BCìš©)
```

**SequenceTrainer** ([seq_trainer.py](../gym/decision_transformer/training/seq_trainer.py)):
```python
def train_step(self):
    # RTG ìŠ¬ë¼ì´ì‹±: rtg[:,:-1] (ë§ˆì§€ë§‰ í•˜ë‚˜ ì œì™¸)
    state_preds, action_preds, reward_preds = self.model.forward(
        states, actions, rewards, rtg[:,:-1], timesteps, attention_mask
    )

    # ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ë§Œ ì†ì‹¤ ê³„ì‚°
    action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]

    loss = MSE(action_preds, action_target)
    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)  # gradient clipping
```

**ActTrainer** ([act_trainer.py](../gym/decision_transformer/training/act_trainer.py)): BCìš©, ë§ˆì§€ë§‰ stateì—ì„œë§Œ action ì˜ˆì¸¡

### 4.2 Atari Trainer

**íŒŒì¼**: [trainer_atari.py](../atari/mingpt/trainer_atari.py)

- DataLoader ê¸°ë°˜ ë°°ì¹˜ í•™ìŠµ
- Cosine LR decay with warmup (lines 116-129)
- ì—í­ë§ˆë‹¤ ì‹¤ì œ ê²Œì„ í™˜ê²½ì—ì„œ í‰ê°€

```python
# Learning rate decay
if self.tokens < config.warmup_tokens:
    lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))  # warmup
else:
    progress = (self.tokens - warmup) / (final - warmup)
    lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))  # cosine
```

---

## 5. í‰ê°€ ì‹œìŠ¤í…œ

```mermaid
flowchart TD
    subgraph GymEval["ğŸ¤¸ Gym í‰ê°€"]
        RTGMode["evaluate_episode_rtg<br/>(DTìš©)"]
        StdMode["evaluate_episode<br/>(BCìš©)"]
        Update["RTG ì—…ë°ì´íŠ¸<br/>target_return -= reward/scale"]
    end

    subgraph AtariEval["ğŸ® Atari í‰ê°€"]
        GetReturns["get_returns(target)"]
        SampleFunc["sample() í•¨ìˆ˜"]
        EnvRun["ì‹¤ì œ ê²Œì„ ì‹¤í–‰<br/>atari_py"]
    end

    RTGMode --> Update
    GetReturns --> SampleFunc --> EnvRun

    style RTGMode fill:#c8e6c9
    style StdMode fill:#fff3e0
    style Update fill:#ffccbc
```

### 5.1 Gym í‰ê°€

**íŒŒì¼**: [evaluate_episodes.py](../gym/decision_transformer/evaluation/evaluate_episodes.py)

**ë‘ ê°€ì§€ ëª¨ë“œ**:

1. **evaluate_episode_rtg** (DTìš©): RTG ì¡°ê±´ë¶€ í‰ê°€
   - ì´ˆê¸° target_return ì„¤ì •
   - ë§¤ ìŠ¤í…ë§ˆë‹¤ `target_return -= reward/scale`ë¡œ ì—…ë°ì´íŠ¸

2. **evaluate_episode** (BCìš©): í‘œì¤€ í‰ê°€

```python
# RTG ì—…ë°ì´íŠ¸ (evaluate_episode_rtg)
pred_return = target_return[0,-1] - (reward/scale)
target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)
```

**get_action ì¶”ë¡  ê³¼ì •** (lines 103-140):
1. í˜„ì¬ê¹Œì§€ì˜ íˆìŠ¤í† ë¦¬ë¥¼ max_lengthë¡œ ìë¦„
2. ì•ìª½ì— zero padding
3. attention_mask ìƒì„± (ìœ íš¨ í† í°ë§Œ 1)
4. forward í›„ ë§ˆì§€ë§‰ action ë°˜í™˜

### 5.2 Atari í‰ê°€

**íŒŒì¼**: [trainer_atari.py](../atari/mingpt/trainer_atari.py) (lines 174-222)

- ì‹¤ì œ Atari ê²Œì„ í™˜ê²½ (atari_py) ì—ì„œ 10 ì—í”¼ì†Œë“œ í‰ê°€
- Target returnì„ ê²Œì„ë³„ë¡œ ì„¤ì • (Breakout: 90, Seaquest: 1150 ë“±)
- `sample()` í•¨ìˆ˜ë¡œ autoregressive action ìƒ˜í”Œë§

---

## 6. ì „ì²´ ì‹¤í–‰ íë¦„

```mermaid
flowchart TD
    subgraph GymFlow["ğŸ¤¸ Gym ì‹¤í–‰ íë¦„"]
        GStart["python experiment.py"]
        GEnv["í™˜ê²½ ì„¤ì •<br/>env_targets, scale"]
        GData["D4RL ë°ì´í„° ë¡œë“œ<br/>ì •ê·œí™”"]
        GModel["DecisionTransformer<br/>ì´ˆê¸°í™”"]
        GTrainer["SequenceTrainer<br/>ì´ˆê¸°í™”"]
        GLoop["Training Loop<br/>iter âˆˆ max_iters"]
        GBatch["get_batch()"]
        GForward["forward â†’ MSE loss"]
        GEval["evaluate_episode_rtg<br/>(target)"]
    end

    subgraph AtariFlow["ğŸ® Atari ì‹¤í–‰ íë¦„"]
        AStart["python run_dt_atari.py"]
        AData["create_dataset()<br/>DQN ë²„í¼ â†’ RTG"]
        ADS["StateActionReturnDataset"]
        AModel["GPT ëª¨ë¸<br/>(6 layers, 8 heads)"]
        ATrainer["Trainer.train()"]
        AEpoch["for epoch in epochs"]
        ATrain["run_epoch('train')"]
        AGetRet["get_returns(target)"]
    end

    GStart --> GEnv --> GData --> GModel --> GTrainer --> GLoop
    GLoop --> GBatch --> GForward --> GEval --> GLoop

    AStart --> AData --> ADS --> AModel --> ATrainer --> AEpoch
    AEpoch --> ATrain --> AGetRet --> AEpoch

    style GData fill:#e8f5e9
    style AData fill:#fff3e0
    style GEval fill:#c8e6c9
    style AGetRet fill:#ffccbc
```

### 6.1 Gym ì‹¤í–‰ íë¦„

```
python experiment.py --env hopper --dataset medium --model_type dt
         â”‚
         â†“
1. í™˜ê²½ ì„¤ì • (env_targets, scale, max_ep_len)
         â†“
2. D4RL ë°ì´í„°ì…‹ ë¡œë“œ ë° ì •ê·œí™”
         â†“
3. DecisionTransformer ëª¨ë¸ ì´ˆê¸°í™”
         â†“
4. SequenceTrainer ì´ˆê¸°í™”
         â†“
5. Training Loop:
   for iter in range(max_iters):
       trainer.train_iteration(num_steps_per_iter)
       â†’ get_batch() â†’ forward â†’ MSE loss â†’ backprop
       â†’ eval_episodes(target) â†’ episode return ì¸¡ì •
```

### 6.2 Atari ì‹¤í–‰ íë¦„

```
python run_dt_atari.py --game Breakout --model_type reward_conditioned
         â”‚
         â†“
1. create_dataset(): DQN replay buffers â†’ (obss, actions, rtgs, timesteps)
         â†“
2. StateActionReturnDataset ìƒì„±
         â†“
3. GPT ëª¨ë¸ ì´ˆê¸°í™” (6 layers, 8 heads, 128 dim)
         â†“
4. Trainer.train():
   for epoch in range(epochs):
       run_epoch('train')
       â†’ DataLoader â†’ forward â†’ Cross Entropy loss
       get_returns(target): ì‹¤ì œ ê²Œì„ì—ì„œ í‰ê°€
```

---

## 7. ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | Atari | Gym |
|---------|-------|-----|
| Context length | 30 | 20 (K) |
| Hidden size | 128 | 128 |
| Layers | 6 | 3 |
| Heads | 8 | 1 |
| Learning rate | 6e-4 | 1e-4 |
| Batch size | 128 | 64 |
| Warmup steps | 512*20 tokens | 10000 |
| Gradient clipping | 1.0 | 0.25 |
| Dropout | 0.1 | 0.1 |

### í™˜ê²½ë³„ ì„¤ì •

**Gym í™˜ê²½**:
| í™˜ê²½ | max_ep_len | env_targets | scale |
|------|-----------|-------------|-------|
| hopper | 1000 | [3600, 1800] | 1000 |
| halfcheetah | 1000 | [12000, 6000] | 1000 |
| walker2d | 1000 | [5000, 2500] | 1000 |
| reacher2d | 100 | [76, 40] | 10 |

**Atari í™˜ê²½**:
| ê²Œì„ | Target RTG |
|------|-----------|
| Breakout | 90 |
| Seaquest | 1150 |
| Qbert | 14000 |
| Pong | 20 |

---

## 8. í•µì‹¬ ì„¤ê³„ ê²°ì •

### 8.1 ì™œ RTGë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?

- **ëª©í‘œ ì§€í–¥ì  í•™ìŠµ**: ì›í•˜ëŠ” returnì„ ì§€ì •í•˜ë©´ í•´ë‹¹ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” policyë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í•™ìŠµ
- **Offline RLì— ì í•©**: ë°ì´í„°ì— ë‹¤ì–‘í•œ í’ˆì§ˆì˜ ê¶¤ì ì´ ì„ì—¬ ìˆì–´ë„, ë†’ì€ RTG ì¡°ê±´ìœ¼ë¡œ ì¢‹ì€ í–‰ë™ë§Œ ì„ íƒ ê°€ëŠ¥

### 8.2 ì™œ Positional Embeddingì„ ì œê±°í–ˆëŠ”ê°€?

- ì‹œí€€ìŠ¤ ë‚´ ìƒëŒ€ì  ìœ„ì¹˜ë³´ë‹¤ **ì ˆëŒ€ì  timestep**ì´ RLì—ì„œ ë” ì¤‘ìš”
- ì—í”¼ì†Œë“œ ë‚´ të²ˆì§¸ ìŠ¤í…ì´ë¼ëŠ” ì •ë³´ê°€ policyì— ì˜í–¥

### 8.3 ì™œ State ìœ„ì¹˜ì—ì„œ Actionì„ ì˜ˆì¸¡í•˜ëŠ”ê°€?

```
[Râ‚, sâ‚, aâ‚, Râ‚‚, sâ‚‚, aâ‚‚, ...]
      â†‘           â†‘
   ì—¬ê¸°ì„œ       ì—¬ê¸°ì„œ
   aâ‚ ì˜ˆì¸¡     aâ‚‚ ì˜ˆì¸¡
```

- Autoregressive íŠ¹ì„±ìƒ, sâ‚ê¹Œì§€ ë³´ê³  aâ‚ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ìì—°ìŠ¤ëŸ¬ì›€
- (R, s) â†’ a ì˜ ì¸ê³¼ ê´€ê³„ë¥¼ ëª¨ë¸ë§

### 8.4 Atari vs Gym ì°¨ì´ì 

| ì¸¡ë©´ | Atari | Gym |
|-----|-------|-----|
| **State í‘œí˜„** | 4Ã—84Ã—84 ì´ë¯¸ì§€ (flattened) | ì—°ì† ë²¡í„° (e.g., 17-dim) |
| **Encoder** | Conv2d â†’ Linear (3136 â†’ n_embd) | Linear (state_dim â†’ hidden_size) |
| **ëª¨ë¸** | Custom minGPT (6 layers, 8 heads) | Huggingface GPT-2 (configurable) |
| **ë°ì´í„°ì…‹** | DQN replay buffers (50 buffers) | D4RL offline RL data |
| **ì†ì‹¤ í•¨ìˆ˜** | Cross-entropy (action classification) | MSE (action regression) |
| **í‰ê°€** | ì•”ì‹œì  (loss monitoring) | ëª…ì‹œì  episode rollouts |
| **ì‹œí€€ìŠ¤ í˜•ì‹** | (R, s, a) linearly interleaved | (R, s, a) stacked then interleaved |
| **RTG ê³„ì‚°** | Simple cumsum of rewards | Discounted cumsum (Î³=1.0) |

---

## 9. íŒŒì¼ êµ¬ì¡° ìš”ì•½

```mermaid
flowchart TB
    subgraph Root["ğŸ“ decision-transformer/"]
        direction TB

        subgraph AtariDir["ğŸ® atari/"]
            direction TB
            AMain["run_dt_atari.py<br/>ë©”ì¸ ì§„ì…ì "]
            AData["create_dataset.py<br/>ë°ì´í„°ì…‹ ìƒì„±"]
            ABuffer["fixed_replay_buffer.py<br/>ë²„í¼ ë˜í¼"]
            subgraph AMingpt["mingpt/"]
                AModel["model_atari.py<br/>GPT + CNN"]
                ATrainer["trainer_atari.py<br/>í•™ìŠµ+í‰ê°€"]
                AUtils["utils.py<br/>ìƒ˜í”Œë§"]
            end
        end

        subgraph GymDir["ğŸ¤¸ gym/"]
            direction TB
            GExp["experiment.py<br/>ë©”ì¸ ì§„ì…ì "]
            subgraph GDT["decision_transformer/"]
                direction TB
                subgraph GModels["models/"]
                    GDTMain["decision_transformer.py<br/>í•µì‹¬ DT ëª¨ë¸"]
                    GGPT2["trajectory_gpt2.py<br/>ì»¤ìŠ¤í…€ GPT-2"]
                    GBase["model.py<br/>TrajectoryModel"]
                    GBC["mlp_bc.py<br/>BC ë² ì´ìŠ¤ë¼ì¸"]
                end
                subgraph GTrain["training/"]
                    GTrainer["trainer.py<br/>ë² ì´ìŠ¤"]
                    GSeq["seq_trainer.py<br/>DTìš©"]
                    GAct["act_trainer.py<br/>BCìš©"]
                end
                subgraph GEval["evaluation/"]
                    GEvalEp["evaluate_episodes.py<br/>RTG ì¡°ê±´ë¶€ í‰ê°€"]
                end
            end
        end

        subgraph DocDir["ğŸ“š doc/"]
            DArch["architecture-flow.md"]
            DCode["code-walkthrough.md"]
            DSys["system-analysis.md"]
            DPlan["learning-plan.md"]
        end
    end

    style AtariDir fill:#fff3e0
    style GymDir fill:#e8f5e9
    style DocDir fill:#e3f2fd
    style GDTMain fill:#c8e6c9
    style AModel fill:#ffccbc
```

```
decision-transformer/
â”œâ”€â”€ atari/
â”‚   â”œâ”€â”€ run_dt_atari.py          # ë©”ì¸ ì§„ì…ì 
â”‚   â”œâ”€â”€ create_dataset.py        # DQN replay â†’ ë°ì´í„°ì…‹
â”‚   â”œâ”€â”€ fixed_replay_buffer.py   # Dopamine ë²„í¼ ë˜í¼
â”‚   â””â”€â”€ mingpt/
â”‚       â”œâ”€â”€ model_atari.py       # GPT ëª¨ë¸ (CNN state encoder)
â”‚       â”œâ”€â”€ trainer_atari.py     # í•™ìŠµ + Atari í™˜ê²½ í‰ê°€
â”‚       â””â”€â”€ utils.py             # ìƒ˜í”Œë§ í•¨ìˆ˜
â”‚
â”œâ”€â”€ gym/
â”‚   â”œâ”€â”€ experiment.py            # ë©”ì¸ ì§„ì…ì  + get_batch()
â”‚   â””â”€â”€ decision_transformer/
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ decision_transformer.py  # í•µì‹¬ DT ëª¨ë¸
â”‚       â”‚   â”œâ”€â”€ trajectory_gpt2.py       # ì»¤ìŠ¤í…€ GPT-2
â”‚       â”‚   â”œâ”€â”€ model.py                 # TrajectoryModel ë² ì´ìŠ¤
â”‚       â”‚   â””â”€â”€ mlp_bc.py                # BC ë² ì´ìŠ¤ë¼ì¸
â”‚       â”œâ”€â”€ training/
â”‚       â”‚   â”œâ”€â”€ trainer.py               # ë² ì´ìŠ¤ Trainer
â”‚       â”‚   â”œâ”€â”€ seq_trainer.py           # DTìš© Trainer
â”‚       â”‚   â””â”€â”€ act_trainer.py           # BCìš© Trainer
â”‚       â””â”€â”€ evaluation/
â”‚           â””â”€â”€ evaluate_episodes.py     # RTG ì¡°ê±´ë¶€ í‰ê°€
â”‚
â””â”€â”€ doc/
    â”œâ”€â”€ architecture-flow.md     # Atari ì•„í‚¤í…ì²˜ flowchart
    â”œâ”€â”€ code-walkthrough.md      # Atari ì½”ë“œ ìƒì„¸ ë¶„ì„
    â””â”€â”€ system-analysis.md       # ì „ì²´ ì‹œìŠ¤í…œ ë¶„ì„ (ë³¸ ë¬¸ì„œ)
```

---

## ì°¸ê³  ìë£Œ

- **ë…¼ë¬¸**: [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345)
- **ê´€ë ¨ ë¬¸ì„œ**:
  - [architecture-flow.md](./architecture-flow.md): Atari í™˜ê²½ ì•„í‚¤í…ì²˜ flowchart
  - [code-walkthrough.md](./code-walkthrough.md): Atari ì½”ë“œ ìƒì„¸ ë¶„ì„
