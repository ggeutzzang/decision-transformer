# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Overview

Decision TransformerëŠ” ê°•í™”í•™ìŠµì„ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ ë¬¸ì œë¡œ ì¬êµ¬ì„±í•œ ì—°êµ¬ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. GPT ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ (return-to-go, state, action) ì‹œí€€ìŠ¤ë¥¼ ëª¨ë¸ë§í•˜ê³ , ì›í•˜ëŠ” returnì„ ì¡°ê±´ìœ¼ë¡œ í•˜ì—¬ í–‰ë™ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

```mermaid
flowchart LR
    subgraph Traditional["ğŸ”„ ê¸°ì¡´ RL"]
        direction TB
        T1["State s"] --> T2["Policy Ï€(s)"]
        T2 --> T3["Action a"]
        T3 --> T4["Reward r"]
        T4 --> T5["Bellman Update"]
        T5 -.-> T2
    end

    subgraph DT["ğŸ¤– Decision Transformer"]
        direction TB
        D1["Target Return RÌ‚"] --> D4
        D2["State s"] --> D4["Transformer"]
        D3["Past Actions"] --> D4
        D4 --> D5["Action a"]
    end

    Traditional -.->|"íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜"| DT

    style Traditional fill:#ffebee
    style DT fill:#e3f2fd
```

ì´ ì €ì¥ì†ŒëŠ” ë‘ ê°œì˜ ë…ë¦½ì ì¸ ì‹¤í—˜ í™˜ê²½ì„ í¬í•¨í•©ë‹ˆë‹¤:

```mermaid
flowchart TB
    subgraph Root["ğŸ“ decision-transformer"]
        direction LR
        subgraph Atari["ğŸ® atari/"]
            A1["DQN-replay ë°ì´í„°ì…‹"]
            A2["minGPT ê¸°ë°˜ êµ¬í˜„"]
            A3["ì´ë¯¸ì§€ ì…ë ¥ (84Ã—84Ã—4)"]
        end

        subgraph Gym["ğŸ¤¸ gym/"]
            G1["D4RL ë°ì´í„°ì…‹"]
            G2["HuggingFace GPT-2 ê¸°ë°˜"]
            G3["ì—°ì† ìƒíƒœ ë²¡í„° ì…ë ¥"]
        end
    end

    style Atari fill:#fff3e0
    style Gym fill:#e8f5e9
```

## Development Commands

### Atari í™˜ê²½

**í™˜ê²½ ì„¤ì •:**
```bash
cd atari
conda env create -f conda_env.yml
conda activate decision-transformer-atari
```

**ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ:**
```bash
mkdir dqn_replay
gsutil -m cp -R gs://atari-replay-datasets/dqn/[GAME_NAME] dqn_replay
# ì˜ˆ: gsutil -m cp -R gs://atari-replay-datasets/dqn/Breakout dqn_replay
```

**ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰:**
```bash
cd atari
python run_dt_atari.py --seed 123 --context_length 30 --epochs 5 \
  --model_type 'reward_conditioned' --num_steps 500000 --num_buffers 50 \
  --game 'Breakout' --batch_size 128 --data_dir_prefix ./dqn_replay
```

**ì¬í˜„ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰:**
```bash
cd atari
bash run.sh  # ì—¬ëŸ¬ ê²Œì„ê³¼ seedì— ëŒ€í•œ ì „ì²´ ì‹¤í—˜ ì‹¤í–‰
```

**ëª¨ë¸ íƒ€ì…:**
- `reward_conditioned`: Decision Transformer (DT)
- `naive`: Behavior Cloning (BC) ë² ì´ìŠ¤ë¼ì¸

### OpenAI Gym í™˜ê²½

**í™˜ê²½ ì„¤ì •:**
```bash
cd gym
conda env create -f conda_env.yml
conda activate decision-transformer-gym
```

**ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ:**
```bash
cd gym
# D4RL ì„¤ì¹˜ í•„ìš”: https://github.com/rail-berkeley/d4rl
python data/download_d4rl_datasets.py
```

**ì‹¤í—˜ ì‹¤í–‰:**
```bash
cd gym
python experiment.py --env hopper --dataset medium --model_type dt

# Weights & Biases ë¡œê¹… í™œì„±í™”
python experiment.py --env hopper --dataset medium --model_type dt -w True
```

**ì§€ì›ë˜ëŠ” í™˜ê²½:**
- `hopper`: Hopper-v3
- `halfcheetah`: HalfCheetah-v3
- `walker2d`: Walker2d-v3
- `reacher2d`: ì»¤ìŠ¤í…€ Reacher2D í™˜ê²½

**ì§€ì›ë˜ëŠ” ë°ì´í„°ì…‹:**
- `medium`, `medium-replay`, `medium-expert`, `expert` (D4RL ë°ì´í„°ì…‹ ì¢…ë¥˜)

**ëª¨ë¸ íƒ€ì…:**
- `dt`: Decision Transformer
- `bc`: Behavior Cloning

## Architecture Overview

### Core Sequence Modeling Approach

Decision TransformerëŠ” ê¸°ì¡´ RLì˜ ë²¨ë§Œ ë°©ì •ì‹ ëŒ€ì‹  autoregressive sequence modelingì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

```mermaid
flowchart LR
    subgraph Input["ğŸ“¥ ì…ë ¥ ì‹œí€€ìŠ¤"]
        R1["RÌ‚â‚"] --> S1["sâ‚"] --> A1["aâ‚"]
        R2["RÌ‚â‚‚"] --> S2["sâ‚‚"] --> A2["aâ‚‚"]
        R3["RÌ‚â‚ƒ"] --> S3["sâ‚ƒ"] --> A3["?"]
    end

    subgraph Process["ğŸ§  ì²˜ë¦¬"]
        Input --> TF["GPT-2<br/>Transformer"]
        TF --> CM["Causal Masking<br/>(ë¯¸ë˜ í† í° ì°¨ë‹¨)"]
    end

    subgraph Output["ğŸ“¤ ì¶œë ¥"]
        CM --> Pred["State ìœ„ì¹˜ì—ì„œ<br/>Action ì˜ˆì¸¡"]
        Pred --> A3_pred["aâ‚ƒ ì˜ˆì¸¡"]
    end

    style R1 fill:#ffcdd2
    style R2 fill:#ffcdd2
    style R3 fill:#ffcdd2
    style S1 fill:#c8e6c9
    style S2 fill:#c8e6c9
    style S3 fill:#c8e6c9
    style A1 fill:#bbdefb
    style A2 fill:#bbdefb
    style A3 fill:#fff9c4
```

**í•µì‹¬ ê°œë…:**
- ì…ë ¥: `(R_1, s_1, a_1, R_2, s_2, a_2, ...)` í˜•íƒœì˜ ì‹œí€€ìŠ¤
- Rì€ returns-to-go (ë¯¸ë˜ ëˆ„ì  ë³´ìƒ)
- GPT-2 ê¸°ë°˜ transformerê°€ stateì—ì„œ actionì„ ì˜ˆì¸¡
- ì¡°ê±´ë¶€ ìƒì„±: ì›í•˜ëŠ” returnì„ ì§€ì •í•˜ì—¬ í–‰ë™ ì •ì±…ì„ ìœ ë„

### Key Components

```mermaid
flowchart TB
    subgraph Atari["ğŸ® Atari êµ¬í˜„ (atari/)"]
        direction TB
        AM["mingpt/"]
        AM --> AM1["model_atari.py<br/>GPT ëª¨ë¸"]
        AM --> AM2["trainer_atari.py<br/>í•™ìŠµ ë£¨í”„"]

        AR["run_dt_atari.py<br/>ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸"]
        AD["create_dataset.py<br/>ë°ì´í„°ì…‹ ìƒì„±"]
        AB["fixed_replay_buffer.py<br/>ë²„í¼ ë¡œë”©"]
    end

    subgraph Gym["ğŸ¤¸ Gym êµ¬í˜„ (gym/)"]
        direction TB
        subgraph Models["models/"]
            GM1["decision_transformer.py<br/>ë©”ì¸ DT ëª¨ë¸"]
            GM2["trajectory_gpt2.py<br/>ì»¤ìŠ¤í…€ GPT-2"]
            GM3["mlp_bc.py<br/>BC ë² ì´ìŠ¤ë¼ì¸"]
        end

        subgraph Training["training/"]
            GT1["seq_trainer.py<br/>DT íŠ¸ë ˆì´ë„ˆ"]
            GT2["act_trainer.py<br/>BC íŠ¸ë ˆì´ë„ˆ"]
        end

        subgraph Eval["evaluation/"]
            GE1["evaluate_episodes.py<br/>ì—í”¼ì†Œë“œ í‰ê°€"]
        end

        GX["experiment.py<br/>ë©”ì¸ ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸"]
    end

    style Atari fill:#fff3e0
    style Gym fill:#e8f5e9
```

### Data Processing

```mermaid
flowchart TB
    subgraph AtariData["ğŸ® Atari ë°ì´í„° ì²˜ë¦¬"]
        AD1["DQN-replay ë²„í¼<br/>(50ê°œ/ê²Œì„)"] --> AD2["ê¶¤ì  ìƒ˜í”Œë§"]
        AD2 --> AD3["í”„ë ˆì„ ìŠ¤íƒ<br/>(4Ã—84Ã—84)"]
        AD3 --> AD4["(s, a, rtg) ì‹œí€€ìŠ¤"]
    end

    subgraph GymData["ğŸ¤¸ Gym ë°ì´í„° ì²˜ë¦¬"]
        GD1["D4RL ë°ì´í„°ì…‹"] --> GD2["Pickle ë³€í™˜<br/>(env-dataset-v2.pkl)"]
        GD2 --> GD3["State ì •ê·œí™”<br/>(í‰ê· /í‘œì¤€í¸ì°¨)"]
        GD3 --> GD4["RTG ê³„ì‚°<br/>(discount cumsum)"]
        GD4 --> GD5["Context K ì¶”ì¶œ<br/>(ê¸°ë³¸ K=20)"]
    end

    style AtariData fill:#fff3e0
    style GymData fill:#e8f5e9
```

### Model Details

**Decision Transformer ì•„í‚¤í…ì²˜:**

```mermaid
flowchart TB
    subgraph Inputs["ğŸ“¥ ì…ë ¥"]
        RTG["RTG<br/>(batch, K, 1)"]
        State["State<br/>(batch, K, state_dim)"]
        Action["Action<br/>(batch, K, act_dim)"]
        Time["Timestep<br/>(batch, K)"]
    end

    subgraph Embedding["1ï¸âƒ£ ì„ë² ë”©"]
        RTG --> |"Linear"| RE["RTG Emb"]
        State --> |"Linear"| SE["State Emb"]
        Action --> |"Linear"| AE["Action Emb"]
        Time --> |"Embedding"| TE["Time Emb"]

        RE --> |"+"| REF["R + T"]
        TE --> REF
        SE --> |"+"| SEF["S + T"]
        TE --> SEF
        AE --> |"+"| AEF["A + T"]
        TE --> AEF
    end

    subgraph Stack["2ï¸âƒ£ ì‹œí€€ìŠ¤ êµ¬ì„±"]
        REF --> Interleave
        SEF --> Interleave
        AEF --> Interleave
        Interleave["Interleave<br/>[R,s,a,R,s,a,...]"] --> LN["LayerNorm"]
    end

    subgraph TF["3ï¸âƒ£ Transformer"]
        LN --> GPT["GPT-2<br/>(Causal Attention)"]
        GPT --> Out["(batch, KÃ—3, hidden)"]
    end

    subgraph Heads["4ï¸âƒ£ ì˜ˆì¸¡ í—¤ë“œ"]
        Out --> |"[:, 1::3, :]"| PA["predict_action<br/>â­ ì£¼ìš” ëª©í‘œ"]
        Out --> |"[:, 2::3, :]"| PS["predict_state<br/>(ë¯¸ì‚¬ìš©)"]
        Out --> |"[:, 2::3, :]"| PR["predict_return<br/>(ë¯¸ì‚¬ìš©)"]
    end

    style Inputs fill:#e1f5fe
    style Embedding fill:#fff3e0
    style Stack fill:#f3e5f5
    style TF fill:#e8f5e9
    style Heads fill:#ffebee
    style PA fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px
```

**ì‹œí€€ìŠ¤ êµ¬ì„±** ([decision_transformer.py:73-78](gym/decision_transformer/models/decision_transformer.py#L73-L78)):
```python
# (R, s, a) íŠ¸ë¦¬í”Œì„ ìŠ¤íƒí•˜ì—¬ ì‹œí€€ìŠ¤ ìƒì„±
# ìµœì¢… í˜•íƒœ: [batch, seq_len*3, hidden_dim]
# R_1, s_1, a_1, R_2, s_2, a_2, ...
```

**ì˜ˆì¸¡ í—¤ë“œ** ([decision_transformer.py:97-99](gym/decision_transformer/models/decision_transformer.py#L97-L99)):
- `predict_action`: state í† í°ì—ì„œ ë‹¤ìŒ action ì˜ˆì¸¡ (ì£¼ìš” ëª©í‘œ)
- `predict_state`: action í† í°ì—ì„œ ë‹¤ìŒ state ì˜ˆì¸¡ (ë…¼ë¬¸ì—ì„œ ë¯¸ì‚¬ìš©)
- `predict_return`: action í† í°ì—ì„œ ë‹¤ìŒ return ì˜ˆì¸¡ (ë…¼ë¬¸ì—ì„œ ë¯¸ì‚¬ìš©)

**ì¶”ë¡  ì‹œ** ([decision_transformer.py:103-140](gym/decision_transformer/models/decision_transformer.py#L103-L140)):
- `get_action()`: í˜„ì¬ê¹Œì§€ì˜ ê¶¤ì ê³¼ ì›í•˜ëŠ” rtgë¥¼ ë°›ì•„ ë‹¤ìŒ action ë°˜í™˜
- Max lengthë¡œ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì œí•œ, íŒ¨ë”© ì²˜ë¦¬

### í•™ìŠµ vs ì¶”ë¡  íë¦„

```mermaid
flowchart TB
    subgraph Training["ğŸ“š í•™ìŠµ (Offline)"]
        T1["ê³¼ê±° ë°ì´í„°ì…‹<br/>(trajectories)"] --> T2["RTG ê³„ì‚°<br/>(ì‹¤ì œ ê°’)"]
        T2 --> T3["(R, s, a) ì‹œí€€ìŠ¤ êµ¬ì„±"]
        T3 --> T4["Transformer Forward"]
        T4 --> T5["Action ì˜ˆì¸¡"]
        T5 --> T6["MSE Loss<br/>(ì˜ˆì¸¡ vs ì‹¤ì œ)"]
        T6 --> T7["Backprop"]
    end

    subgraph Inference["ğŸ¯ ì¶”ë¡  (Online)"]
        I1["ëª©í‘œ Return ì„¤ì •<br/>(ì‚¬ìš©ì ì§€ì •)"] --> I2["ì´ˆê¸° RTG = ëª©í‘œ"]
        I2 --> I3["í˜„ì¬ State ê´€ì¸¡"]
        I3 --> I4["Transformerë¡œ<br/>Action ì˜ˆì¸¡"]
        I4 --> I5["í™˜ê²½ì—ì„œ ì‹¤í–‰"]
        I5 --> I6["Reward íšë“"]
        I6 --> I7["RTG ì—…ë°ì´íŠ¸<br/>(RTG -= reward)"]
        I7 --> I3
    end

    Training --> |"í•™ìŠµëœ ëª¨ë¸"| Inference

    style Training fill:#e3f2fd
    style Inference fill:#fff8e1
```

## Important Implementation Notes

```mermaid
flowchart LR
    subgraph Notes["âš ï¸ ì£¼ì˜ì‚¬í•­"]
        N1["PYTHONPATH<br/>ê° ë””ë ‰í† ë¦¬ ì¶”ê°€ í•„ìš”"]
        N2["ì‹¤í–‰ ìœ„ì¹˜<br/>cd atari ë˜ëŠ” cd gym"]
        N3["Context Length<br/>Atari: 30 / Gym: 20"]
        N4["í•˜ì´í¼íŒŒë¼ë¯¸í„°<br/>í™˜ê²½ë³„ë¡œ ë‹¤ë¦„"]
    end

    style Notes fill:#fff3e0
```

- **PYTHONPATH ì„¤ì •**: ê° ë””ë ‰í† ë¦¬(`atari/`, `gym/`)ë¥¼ PYTHONPATHì— ì¶”ê°€í•´ì•¼ í•  ìˆ˜ ìˆìŒ
- **ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ìœ„ì¹˜**: í•­ìƒ í•´ë‹¹ í•˜ìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰ (`cd atari` ë˜ëŠ” `cd gym`)
- **ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸**: AtariëŠ” ìë™ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥, Gymì€ wandb ì˜µì…˜ ì‚¬ìš© ì‹œ ë¡œê¹…
- **Context length**: AtariëŠ” `context_length` (ê¸°ë³¸ 30), Gymì€ `K` (ê¸°ë³¸ 20) íŒŒë¼ë¯¸í„°ë¡œ ì œì–´
- **í•˜ì´í¼íŒŒë¼ë¯¸í„°**: ê° ê²Œì„/í™˜ê²½ë§ˆë‹¤ ìµœì  ì„¤ì •ì´ ë‹¤ë¦„ - `run.sh` ë˜ëŠ” `experiment.py` ì°¸ì¡°

## Known Issues

- **off-by-one ë²„ê·¸ ìˆ˜ì •ë¨**: rtg ê³„ì‚° ê´€ë ¨ ë²„ê·¸ íŒ¨ì¹˜ ì ìš©ë¨ (ìµœê·¼ ì»¤ë°‹ ì°¸ì¡°)
- **MuJoCo ë¼ì´ì„ ìŠ¤**: Gym í™˜ê²½ì€ MuJoCo ì„¤ì¹˜ ë° ë¼ì´ì„ ìŠ¤ í•„ìš”
- **GPU ë©”ëª¨ë¦¬**: Atari í•™ìŠµ ì‹œ ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ í•„ìš”í•  ìˆ˜ ìˆìŒ
