# CLAUDE.md

ì´ íŒŒì¼ì€ Claude Code (claude.ai/code)ê°€ ì´ ì €ì¥ì†Œì˜ ì½”ë“œë¥¼ ì‘ì—…í•  ë•Œ ì°¸ì¡°í•˜ëŠ” ê°€ì´ë“œì…ë‹ˆë‹¤.

## Quick Overview

Decision TransformerëŠ” ê°•í™”í•™ìŠµì„ **ì‹œí€€ìŠ¤ ëª¨ë¸ë§ ë¬¸ì œ**ë¡œ ì¬êµ¬ì„±í•œ ì—°êµ¬ì…ë‹ˆë‹¤. GPT ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ (return-to-go, state, action) ì‹œí€€ìŠ¤ë¥¼ ëª¨ë¸ë§í•˜ê³ , ì›í•˜ëŠ” returnì„ ì¡°ê±´ìœ¼ë¡œ í•˜ì—¬ í–‰ë™ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

```mermaid
flowchart LR
    subgraph DT["Decision Transformer"]
        R["Target Return RÌ‚"] --> S["State s"]
        S --> T["Transformer"]
        T --> A["Action a"]
    end

    style DT fill:#e3f2fd
```

## ğŸ“š ìƒì„¸ ë¬¸ì„œ

í”„ë¡œì íŠ¸ì˜ ìƒì„¸í•œ ê¸°ìˆ  ë¬¸ì„œëŠ” [`doc/`](./doc/) ë””ë ‰í† ë¦¬ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”:

| ë¬¸ì„œ | ì„¤ëª… |
|------|------|
| [`doc/README.md`](./doc/README.md) | ë¬¸ì„œ ê°€ì´ë“œ ë° í•™ìŠµ ê²½ë¡œ |
| [`doc/learning-plan.md`](./doc/learning-plan.md) | Phaseë³„ í•™ìŠµ ê³„íš (ì´ˆë³´ì ì¶”ì²œ) |
| [`doc/architecture-flow.md`](./doc/architecture-flow.md) | ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ |
| [`doc/system-analysis.md`](./doc/system-analysis.md) | ì „ì²´ ì‹œìŠ¤í…œ ë¶„ì„ |
| [`doc/code-walkthrough.md`](./doc/code-walkthrough.md) | ì½”ë“œ ìƒì„¸ ë¶„ì„ |

## Project Structure

```mermaid
flowchart TB
    subgraph Root["decision-transformer/"]
        direction LR

        subgraph Atari["ğŸ® atari/"]
            A1["DQN-replay ë°ì´í„°ì…‹"]
            A2["minGPT ê¸°ë°˜ êµ¬í˜„"]
            A3["ì´ë¯¸ì§€ ì…ë ¥ (84Ã—84Ã—4)"]
        end

        subgraph Gym["ğŸ¤¸ gym/"]
            G1["D4RL ë°ì´í„°ì…‹"]
            G2["HuggingFace GPT-2 ê¸°ë°˜"]
            G3["ì—°ì† ìƒíƒœ ë²¡í„° ì…ë ¥"]
        end
    end

    style Atari fill:#fff3e0
    style Gym fill:#e8f5e9
```

## Quick Start Commands

### Atari í™˜ê²½

```bash
cd atari
conda env create -f conda_env.yml
conda activate decision-transformer-atari

# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (gsutil í•„ìš”)
mkdir dqn_replay
gsutil -m cp -R gs://atari-replay-datasets/dqn/Breakout dqn_replay

# í•™ìŠµ ì‹¤í–‰
python run_dt_atari.py --game Breakout --model_type reward_conditioned
```

### Gym í™˜ê²½

```bash
cd gym
conda env create -f conda_env.yml
conda activate decision-transformer-gym

# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (D4RL ì„¤ì¹˜ í•„ìš”)
python data/download_d4rl_datasets.py

# í•™ìŠµ ì‹¤í–‰
python experiment.py --env hopper --dataset medium --model_type dt
```

## Important Implementation Notes

| í•­ëª© | Atari | Gym |
|------|-------|-----|
| **ì‹¤í–‰ ìœ„ì¹˜** | `cd atari` | `cd gym` |
| **Context Length** | 30 | 20 |
| **State í˜•íƒœ** | ì´ë¯¸ì§€ (4Ã—84Ã—84) | ì—°ì† ë²¡í„° |
| **ëª¨ë¸** | minGPT (6 layers, 8 heads) | HuggingFace GPT-2 |
| **ë°ì´í„°ì…‹** | DQN replay buffers | D4RL pickle |

## Core Concepts

### Return-to-Go (RTG)
- ê° íƒ€ì„ìŠ¤í…ì—ì„œ **ì—í”¼ì†Œë“œ ëê¹Œì§€ì˜ ëˆ„ì  ë³´ìƒ**
- í•™ìŠµ ì‹œ: ë°ì´í„°ì—ì„œ ê³„ì‚°ëœ ì‹¤ì œ ê°’
- ì¶”ë¡  ì‹œ: ëª©í‘œ returnì„ ì„¤ì •í•˜ê³  ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ (`rtg -= reward`)

### Sequence Structure
```
ì…ë ¥: [Râ‚€, sâ‚€, aâ‚€, Râ‚, sâ‚, aâ‚, Râ‚‚, sâ‚‚, aâ‚‚, ...]
ì˜ˆì¸¡: [  , â†’aâ‚€,   ,   , â†’aâ‚,   ,   , â†’aâ‚‚,   ]
       â””â”€ sâ‚€ì—ì„œ â”€â”˜   â””â”€ sâ‚ì—ì„œ â”€â”˜   â””â”€ sâ‚‚ì—ì„œ â”€â”˜
```

### Model Types
- **reward_conditioned** (Decision Transformer): RTGë¡œ ëª©í‘œ ì§€ì •
- **naive** (Behavior Cloning): RTG ì—†ì´ ë‹¨ìˆœ ëª¨ë°© í•™ìŠµ

## File Organization

```
decision-transformer/
â”œâ”€â”€ atari/                          # Atari í™˜ê²½
â”‚   â”œâ”€â”€ run_dt_atari.py            # ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ create_dataset.py          # ë°ì´í„°ì…‹ ìƒì„±
â”‚   â””â”€â”€ mingpt/                    # GPT ëª¨ë¸
â”‚       â”œâ”€â”€ model_atari.py         # CNN encoder + Transformer
â”‚       â””â”€â”€ trainer_atari.py       # í•™ìŠµ ë£¨í”„
â”‚
â”œâ”€â”€ gym/                           # Gym í™˜ê²½
â”‚   â”œâ”€â”€ experiment.py              # ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ decision_transformer/
â”‚       â”œâ”€â”€ models/                # DT ëª¨ë¸, GPT-2, BC
â”‚       â”œâ”€â”€ training/              # SequenceTrainer, ActTrainer
â”‚       â””â”€â”€ evaluation/            # RTG ì¡°ê±´ë¶€ í‰ê°€
â”‚
â”œâ”€â”€ doc/                           # ğŸ“š ìƒì„¸ ë¬¸ì„œ
â”‚   â”œâ”€â”€ README.md                  # ë¬¸ì„œ ê°€ì´ë“œ
â”‚   â”œâ”€â”€ learning-plan.md           # í•™ìŠµ ê³„íš
â”‚   â”œâ”€â”€ architecture-flow.md       # ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨
â”‚   â”œâ”€â”€ system-analysis.md         # ì‹œìŠ¤í…œ ë¶„ì„
â”‚   â””â”€â”€ code-walkthrough.md        # ì½”ë“œ ë¶„ì„
â”‚
â””â”€â”€ CLAUDE.md                      # ë³¸ íŒŒì¼
```

## Common Issues

- **PYTHONPATH**: ê° ë””ë ‰í† ë¦¬(`atari/`, `gym/`)ì—ì„œ ì‹¤í–‰í•˜ë©´ ìë™ìœ¼ë¡œ ì„¤ì •ë¨
- **MuJoCo ë¼ì´ì„ ìŠ¤**: Gym í™˜ê²½ì€ MuJoCo 2.1+ í•„ìš” (ë¬´ë£Œ)
- **GPU ë©”ëª¨ë¦¬**: Atari í•™ìŠµ ì‹œ ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ í•„ìš”í•  ìˆ˜ ìˆìŒ
- **off-by-one ë²„ê·¸**: rtg ê³„ì‚° ë²„ê·¸ëŠ” ìˆ˜ì •ë¨ (ìµœê·¼ ì»¤ë°‹ ì°¸ì¡°)

## References

- **ë…¼ë¬¸**: [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345)
- **ì½”ë“œ**: ì›ë³¸ êµ¬í˜„ì€ [https://github.com/kzl/decision-transformer](https://github.com/kzl/decision-transformer)
