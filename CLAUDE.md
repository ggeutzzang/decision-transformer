# CLAUDE.md

ì´ íŒŒì¼ì€ Claude Code (claude.ai/code)ê°€ ì´ ì €ì¥ì†Œì˜ ì½”ë“œë¥¼ ì‘ì—…í•  ë•Œ ì°¸ì¡°í•˜ëŠ” ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ë¬¸ì„œ ì‘ì„± ê°€ì´ë“œë¼ì¸

ì´ í”„ë¡œì íŠ¸ì˜ ë¬¸ì„œëŠ” **ì‹œê°í™”**ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ í•©ë‹ˆë‹¤. ì„¤ëª…ì„ ì¶”ê°€í•  ë•ŒëŠ” ë‹¤ìŒ ìˆœì„œë¥¼ ë”°ë¥´ì„¸ìš”:

1. **ë¨¼ì € ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ**: `sequenceDiagram`, `flowchart`, `graph` ë“±ìœ¼ë¡œ ì‹œê°í™”
2. **í•µì‹¬ë§Œ ìš”ì•½**: í…ìŠ¤íŠ¸ëŠ” ë‹¤ì´ì–´ê·¸ë¨ì˜ ë³´ì¡° ì—­í• ë¡œë§Œ ì‚¬ìš©
3. **ìƒì„¸ ë‚´ìš©ì€ `doc/`ë¡œ**: ê¸´ ì„¤ëª…ì€ `doc/`ì˜ ì ì ˆí•œ ë¬¸ì„œì— ìœ„ì„

### ê¶Œì¥ ë‹¤ì´ì–´ê·¸ë¨ ìœ í˜•

| ìš©ë„ | ì¶”ì²œ ë‹¤ì´ì–´ê·¸ë¨ | ì˜ˆì‹œ |
|------|----------------|------|
| **ì‹œê°„ ìˆœì„œ íë¦„** | `sequenceDiagram` | ì¶”ë¡  ê³¼ì •, í•™ìŠµ ë£¨í”„, í•¨ìˆ˜ í˜¸ì¶œ ìˆœì„œ |
| **ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸** | `flowchart TD` | ì „ì²˜ë¦¬, ëª¨ë¸ forward, í‰ê°€ ê³¼ì • |
| **ì•„í‚¤í…ì²˜ êµ¬ì¡°** | `flowchart TB` + `subgraph` | ëª¨ë¸ êµ¬ì¡°, ëª¨ë“ˆ ê´€ê³„ |
| **ìƒíƒœ ì „í™˜** | `stateDiagram-v2` | ì—í”¼ì†Œë“œ ì§„í–‰, RTG ì—…ë°ì´íŠ¸ |
| **ê°œë… ë¹„êµ** | `flowchart LR` (ë¶„ê¸°í˜•) | DT vs BC, Atari vs Gym |

---

## Quick Overview

Decision TransformerëŠ” ê°•í™”í•™ìŠµì„ **ì‹œí€€ìŠ¤ ëª¨ë¸ë§ ë¬¸ì œ**ë¡œ ì¬êµ¬ì„±í•œ ì—°êµ¬ì…ë‹ˆë‹¤.

```mermaid
flowchart LR
    subgraph Traditional["ğŸ”„ ê¸°ì¡´ RL"]
        Bellman["Bellman ë°©ì •ì‹"]
        Value["Value Function"]
    end

    subgraph DT["ğŸ¤– Decision Transformer"]
        Seq["Sequence Modeling"]
        GPT["GPT Architecture"]
    end

    Traditional -.->|"íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜"| DT

    style DT fill:#e3f2fd
```

**ğŸ“– ìƒì„¸ ì„¤ëª…**: [`doc/system-analysis.md`](./doc/system-analysis.md#1-ê°œìš”)

---

## ğŸ“š ìƒì„¸ ë¬¸ì„œ ë§µ

```mermaid
mindmap
    root((doc/))
        Overview["README.md<br/>ë¬¸ì„œ ê°€ì´ë“œ"]
        Learning["learning-plan.md<br/>Phaseë³„ í•™ìŠµ"]
        Architecture["architecture-flow.md<br/>ì•„í‚¤í…ì²˜"]
        System["system-analysis.md<br/>ì‹œìŠ¤í…œ ë¶„ì„"]
        Code["code-walkthrough.md<br/>ì½”ë“œ ë¶„ì„"]
```

| ë¬¸ì„œ | ìš©ë„ | ë§í¬ |
|------|------|------|
| **ë¬¸ì„œ ê°€ì´ë“œ** | `doc/` êµ¬ì¡° ë° í•™ìŠµ ê²½ë¡œ | [`doc/README.md`](./doc/README.md) |
| **í•™ìŠµ ê³„íš** | Phaseë³„ í•™ìŠµ ë¡œë“œë§µ | [`doc/learning-plan.md`](./doc/learning-plan.md) |
| **ì•„í‚¤í…ì²˜** | ì „ì²´ ì‹œìŠ¤í…œ ë‹¤ì´ì–´ê·¸ë¨ | [`doc/architecture-flow.md`](./doc/architecture-flow.md) |
| **ì‹œìŠ¤í…œ ë¶„ì„** | Atari + Gym ë¹„êµ | [`doc/system-analysis.md`](./doc/system-analysis.md) |
| **ì½”ë“œ ë¶„ì„** | êµ¬í˜„ ìƒì„¸ ì„¤ëª… | [`doc/code-walkthrough.md`](./doc/code-walkthrough.md) |

---

## Project Structure

```mermaid
flowchart TB
    subgraph Root["decision-transformer/"]
        direction LR

        subgraph Atari["ğŸ® atari/"]
            A1["DQN-replay ë°ì´í„°"]
            A2["minGPT êµ¬í˜„"]
            A3["ì´ë¯¸ì§€ ì…ë ¥"]
        end

        subgraph Gym["ğŸ¤¸ gym/"]
            G1["D4RL ë°ì´í„°"]
            G2["HF GPT-2"]
            G3["ì—°ì† state"]
        end

        subgraph Doc["ğŸ“š doc/"]
            D1["README.md"]
            D2["learning-plan.md"]
            D3["architecture-flow.md"]
            D4["system-analysis.md"]
            D5["code-walkthrough.md"]
        end
    end

    style Atari fill:#fff3e0
    style Gym fill:#e8f5e9
    style Doc fill:#e3f2fd
```

---

## Quick Start

### Atari í™˜ê²½

```mermaid
sequenceDiagram
    participant User as ğŸ‘¤ ì‚¬ìš©ì
    participant Shell as ğŸ–¥ï¸ í„°ë¯¸ë„
    participant Conda as Conda
    participant GCS as gsutil
    participant Script as run_dt_atari.py

    User->>Shell: cd atari
    User->>Conda: conda env create -f conda_env.yml
    Conda-->>User: í™˜ê²½ ìƒì„± ì™„ë£Œ

    User->>Shell: mkdir dqn_replay
    User->>GCS: gsutil -m cp -R gs://atari-replay-datasets/dqn/Breakout dqn_replay
    GCS-->>User: ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ

    User->>Script: python run_dt_atari.py --game Breakout
    Script-->>User: í•™ìŠµ ì‹œì‘
```

### Gym í™˜ê²½

```mermaid
sequenceDiagram
    participant User as ğŸ‘¤ ì‚¬ìš©ì
    participant Shell as ğŸ–¥ï¸ í„°ë¯¸ë„
    participant D4RL as D4RL
    participant Script as experiment.py

    User->>Shell: cd gym
    User->>Conda: conda env create -f conda_env.yml
    Conda-->>User: í™˜ê²½ ìƒì„± ì™„ë£Œ

    User->>D4RL: python data/download_d4rl_datasets.py
    D4RL-->>User: ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ

    User->>Script: python experiment.py --env hopper --dataset medium
    Script-->>User: í•™ìŠµ ì‹œì‘
```

---

## Core Concepts

### Return-to-Go (RTG)

```mermaid
flowchart LR
    subgraph RTG["RTG ì—…ë°ì´íŠ¸"]
        R1["RTG = 100"]
        R2["reward = 10"]
        R3["RTG = 100 - 10 = 90"]
    end

    R1 -->|"-"| R2 --> R3

    style R1 fill:#ffcdd2
    style R3 fill:#c8e6c9
```

**ğŸ“– ìƒì„¸**: [`doc/learning-plan.md`](./doc/learning-plan.md#22-return-to-go-ê°œë…-ê¹Šì´-ì´í•´)

### Sequence Structure

```mermaid
flowchart LR
    subgraph Input["ì…ë ¥ ì‹œí€€ìŠ¤"]
        R0["Râ‚€"] --> S0["sâ‚€"] --> A0["aâ‚€"]
        A0 --> R1["Râ‚"] --> S1["sâ‚"] --> A1["aâ‚"]
    end

    subgraph Output["ì˜ˆì¸¡"]
        P0["  "]
        P1["â†’aâ‚€"]
        P2["  "]
        P3["  "]
        P4["â†’aâ‚"]
    end

    S0 -.-> P1
    S1 -.-> P4

    style P1 fill:#4ecdc4
    style P4 fill:#4ecdc4
```

**ğŸ“– ìƒì„¸**: [`doc/architecture-flow.md`](./doc/architecture-flow.md#6-ëª¨ë¸ë³„-ì‹œí€€ìŠ¤-êµ¬ì„±-ë¹„êµ)

### Inference Flow

```mermaid
sequenceDiagram
    participant User as ğŸ¯ ëª©í‘œ
    participant Model as ğŸ§  DT
    participant Env as ğŸŒ í™˜ê²½

    User->>Model: target_return = 100
    Model->>Env: actionâ‚€
    Env-->>Model: reward = 10

    Note over Model: RTG = 100 - 10 = 90

    Model->>Env: actionâ‚
    Env-->>Model: reward = 20

    Note over Model: RTG = 90 - 20 = 70

    Model->>Env: actionâ‚‚
```

**ğŸ“– ìƒì„¸**: [`doc/architecture-flow.md`](./doc/architecture-flow.md#5-í‰ê°€-ì¶”ë¡ -ê³¼ì •)

---

## Environment Comparison

| í•­ëª© | Atari | Gym |
|------|-------|-----|
| **ë””ë ‰í† ë¦¬** | `atari/` | `gym/` |
| **ì‹¤í–‰ ìœ„ì¹˜** | `cd atari` | `cd gym` |
| **Context Length** | 30 | 20 |
| **State** | ì´ë¯¸ì§€ (4Ã—84Ã—84) | ì—°ì† ë²¡í„° |
| **Action** | ì´ì‚°ì  (ë¶„ë¥˜) | ì—°ì†ì  (íšŒê·€) |
| **ëª¨ë¸** | minGPT (6L, 8H) | HF GPT-2 (3L, 1H) |
| **ë°ì´í„°ì…‹** | DQN replay buffers | D4RL pickle |

**ğŸ“– ìƒì„¸**: [`doc/system-analysis.md`](./doc/system-analysis.md#84-atari-vs-gym-ì°¨ì´ì )

---

## Common Issues

```mermaid
flowchart TD
    Start[ë¬¸ì œ ë°œìƒ] --> Q1{PYTHONPATH?}
    Q1 -->|Yes| Q2{MuJoCo?}
    Q1 -->|No| A1["í•´ë‹¹ ë””ë ‰í† ë¦¬ì—ì„œ<br/>cd atari ë˜ëŠ” cd gym"]

    Q2 -->|ë¬¸ì œ| A2["pip install mujoco"]
    Q2 -->|OK| Q3{GPU ë©”ëª¨ë¦¬?}

    Q3 -->|ë¶€ì¡±| A3["batch_size ì¤„ì´ê¸°"]
    Q3 -->|ì¶©ë¶„| Q4{D4RL ì„¤ì¹˜?}

    Q4 -->|ë¬¸ì œ| A4["pip install git+https://github.com/Farama-Foundation/d4rl@master#egg=d4rl"]
    Q4 -->|OK| A5["ì´ìŠˆ íŠ¸ë˜ì»¤ í™•ì¸"]

    style A1 fill:#c8e6c9
    style A2 fill:#c8e6c9
    style A3 fill:#c8e6c9
    style A4 fill:#c8e6c9
```

---

## References

- **ë…¼ë¬¸**: [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345)
- **ì›ë³¸ ì½”ë“œ**: [https://github.com/kzl/decision-transformer](https://github.com/kzl/decision-transformer)
- **ìƒì„¸ ë¬¸ì„œ**: [`doc/`](./doc/) ë””ë ‰í† ë¦¬ ì°¸ì¡°
